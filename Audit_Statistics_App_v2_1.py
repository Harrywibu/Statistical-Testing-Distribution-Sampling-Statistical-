# === Audit Statistics ‚Äî v2.2 (refactor by Copilot) ===
from __future__ import annotations

# ---- Core & typing ----
import os, io, re, json, time, hashlib, contextlib, tempfile, warnings
from datetime import datetime
from typing import Optional, List
import numpy as np
import pandas as pd
import streamlit as st
from scipy import stats
warnings.filterwarnings("ignore")

# ---- Optional deps (soft import -> feature flags) ----
try:
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    HAS_PLOTLY = True
except Exception:
    HAS_PLOTLY = False

try:
    import plotly.io as pio
    HAS_KALEIDO = True
except Exception:
    HAS_KALEIDO = False

try:
    import docx
    from docx.shared import Inches
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False

try:
    import fitz  # PyMuPDF
    HAS_PDF = True
except Exception:
    HAS_PDF = False

try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    HAS_PYARROW = True
except Exception:
    HAS_PYARROW = False

try:
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.metrics import (r2_score, mean_squared_error, accuracy_score,
                                 roc_auc_score, roc_curve, confusion_matrix)
    HAS_SK = True
except Exception:
    HAS_SK = False

# ---- App config & compact theme ----
st.set_page_config(
    page_title="Audit Statistics",
    layout="wide",
    initial_sidebar_state="expanded"
)

CSS = r'''
<style>
/* Compact layout to reduce empty spaces */
.block-container {padding-top: 1.2rem; padding-bottom: 2rem;}
[data-testid="stSidebar"] .block-container {padding-top: .8rem;}
h2, h3 { margin-bottom: .25rem; }
div.stButton > button, .stDownloadButton > button { border-radius: 8px }

/* Tidy dataframe default height (n·∫øu l·ªói tokenizer, ƒë·ªÉ comment d√≤ng d∆∞·ªõi) */
/* iframe[title^="dataframe"] { height: 320px; } */
</style>
'''
st.markdown(CSS, unsafe_allow_html=True)

# ---- Small utils ----
def file_sha12(b: bytes) -> str:
    """12-char sha256 for file identity."""
    return hashlib.sha256(b).hexdigest()[:12]

def st_plotly(fig, **kwargs):
    """Plotly wrapper with stable keys & sane defaults."""
    SS = st.session_state
    if "_plt_seq" not in SS:
        SS["_plt_seq"] = 0
    SS["_plt_seq"] += 1
    kwargs.setdefault("use_container_width", True)
    kwargs.setdefault("config", {"displaylogo": False})
    kwargs.setdefault("key", f"plt_{SS['_plt_seq']}")
    return st.plotly_chart(fig, **kwargs)

def _downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """Memory-friendly numeric downcast."""
    for c in df.select_dtypes(include=["float64"]).columns:
        df[c] = pd.to_numeric(df[c], downcast="float")
    for c in df.select_dtypes(include=["int64"]).columns:
        df[c] = pd.to_numeric(df[c], downcast="integer")
    return df

def to_float(x) -> Optional[float]:
    """Safe numeric parse from str/obj -> float or None (for thresholds)."""
    from numbers import Real
    try:
        if isinstance(x, Real): return float(x)
        if x is None: return None
        return float(str(x).strip().replace(",", ""))
    except Exception:
        return None

def is_datetime_like(colname: str, s: pd.Series) -> bool:
    """Detect datetime either by dtype or name hint."""
    return pd.api.types.is_datetime64_any_dtype(s) or bool(re.search(r"(date|time)", str(colname), re.I))

# ---- Disk cache helpers (Parquet) ----
def _parquet_cache_path(sha: str, key: str) -> str:
    return os.path.join(tempfile.gettempdir(), f"astats_cache_{sha}_{key}.parquet")

@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=24)
def write_parquet_cache(df: pd.DataFrame, sha: str, key: str) -> str:
    if not HAS_PYARROW: return ""
    try:
        table = pa.Table.from_pandas(df)
        path = _parquet_cache_path(sha, key)
        pq.write_table(table, path)
        return path
    except Exception:
        return ""

def read_parquet_cache(sha: str, key: str) -> Optional[pd.DataFrame]:
    if not HAS_PYARROW: return None
    path = _parquet_cache_path(sha, key)
    if os.path.exists(path):
        try:
            return pq.read_table(path).to_pandas()
        except Exception:
            return None
    return None

# ---- Fast readers ----
@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=16)
def list_sheets_xlsx(file_bytes: bytes) -> List[str]:
    from openpyxl import load_workbook
    wb = load_workbook(io.BytesIO(file_bytes), read_only=True, data_only=True)
    try:
        return wb.sheetnames
    finally:
        wb.close()

@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=16)
def read_csv_fast(file_bytes: bytes, usecols=None) -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    try:
        df = pd.read_csv(bio, usecols=usecols, engine="pyarrow")
    except Exception:
        bio.seek(0)
        df = pd.read_csv(bio, usecols=usecols, low_memory=False, memory_map=True)
    return _downcast_numeric(df)

@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=16)
def read_xlsx_fast(file_bytes: bytes, sheet: str, usecols=None,
                   header_row: int = 1, skip_top: int = 0, dtype_map=None) -> pd.DataFrame:
    skiprows = list(range(header_row, header_row + skip_top)) if skip_top > 0 else None
    bio = io.BytesIO(file_bytes)
    df = pd.read_excel(
        bio, sheet_name=sheet, usecols=usecols,
        header=header_row - 1, skiprows=skiprows,
        dtype=dtype_map, engine="openpyxl"
    )
    return _downcast_numeric(df)

# ---- Cached stats ----
@st.cache_data(ttl=1800, show_spinner=False, max_entries=64)
def numeric_profile_stats(series: pd.Series):
    s = pd.to_numeric(series, errors="coerce").replace([np.inf, -np.inf], np.nan).dropna()
    desc = s.describe(percentiles=[0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99])
    skew = float(stats.skew(s)) if len(s) > 2 else np.nan
    kurt = float(stats.kurtosis(s, fisher=True)) if len(s) > 3 else np.nan
    try:
        p_norm = float(stats.normaltest(s)[1]) if len(s) > 7 else np.nan
    except Exception:
        p_norm = np.nan
    p95 = s.quantile(0.95) if len(s) > 1 else np.nan
    p99 = s.quantile(0.99) if len(s) > 1 else np.nan
    zero_ratio = float((s == 0).mean()) if len(s) > 0 else np.nan
    return desc.to_dict(), skew, kurt, p_norm, float(p95), float(p99), zero_ratio

@st.cache_data(ttl=1800, show_spinner=False, max_entries=64)
def cat_freq(series: pd.Series) -> pd.DataFrame:
    s = series.dropna().astype(str)
    vc = s.value_counts(dropna=True)
    out = pd.DataFrame({"category": vc.index, "count": vc.values})
    out["share"] = out["count"] / out["count"].sum()
    return out
# ==== SIDEBAR + INGEST ====

# -- Sidebar: Workflow & Controls --
st.sidebar.title("Workflow")

with st.sidebar.expander("0) Ingest", expanded=True):
    uploaded = st.file_uploader("Upload CSV/XLSX", type=["csv", "xlsx"], key="uploader")
    if uploaded is not None:
        fb = uploaded.read()
        SS["file_bytes"] = fb
        SS["sha12"] = file_sha12(fb)
        SS["uploaded_name"] = uploaded.name
        st.caption(f"SHA12: {SS['sha12']}")
        
with st.sidebar.expander("1) Display & Performance", expanded=True):
    SS["bins"] = st.slider("Histogram bins", 10, 200, SS["bins"], 5, help="S·ªë bins cho histogram; ·∫£nh h∆∞·ªüng ƒë·ªô m·ªãn ph√¢n ph·ªëi.")
    SS["log_scale"] = st.checkbox(
        "Log scale (X)",
        value=SS["log_scale"],
        help="Ch·ªâ √°p d·ª•ng khi m·ªçi gi√° tr·ªã > 0."
    )

    SS["kde_threshold"] = st.number_input(
        "KDE max n",
        min_value=1_000,
        max_value=300_000,
        value=SS["kde_threshold"],
        step=1_000,
        help="N·∫øu s·ªë ƒëi·ªÉm > ng∆∞·ª°ng n√†y th√¨ b·ªè KDE ƒë·ªÉ tƒÉng t·ªëc."
    )

    # GI·ªÆ nguy√™n bi·∫øn 'downsample' v√¨ ph√≠a d∆∞·ªõi ƒëang d√πng
    downsample = st.checkbox(
        "Downsample view 50k",
        value=True,
        help="Ch·ªâ hi·ªÉn th·ªã & v·∫Ω tr√™n sample 50k ƒë·ªÉ nhanh h∆°n (t√≠nh to√°n n·∫∑ng v·∫´n c√≥ th·ªÉ ch·∫°y tr√™n full)."
    )
with st.sidebar.expander("2) Risk & Advanced", expanded=False):
    SS["risk_diff_threshold"] = st.slider(
        "Benford diff% threshold", 0.01, 0.10, SS["risk_diff_threshold"], 0.01,
        help="Ng∆∞·ª°ng c·∫£nh b√°o ch√™nh l·ªách quan s√°t so v·ªõi k·ª≥ v·ªçng (Benford)."
    )
    SS["advanced_visuals"] = st.checkbox(
        "Advanced visuals (Violin, Lorenz/Gini)", SS["advanced_visuals"],
        help="T·∫Øt m·∫∑c ƒë·ªãnh ƒë·ªÉ g·ªçn giao di·ªán; b·∫≠t khi c·∫ßn ph√¢n t√≠ch s√¢u."
    )

with st.sidebar.expander("3) Cache", expanded=False):
    # n·∫øu kh√¥ng c√≥ pyarrow th√¨ v√¥ hi·ªáu ho√° cache xu·ªëng ƒëƒ©a
    if not HAS_PYARROW:
        st.caption("‚ö†Ô∏è PyArrow ch∆∞a s·∫µn s√†ng ‚Äî Disk cache (Parquet) s·∫Ω b·ªã t·∫Øt.")
        SS["use_parquet_cache"] = False
    SS["use_parquet_cache"] = st.checkbox(
        "Disk cache (Parquet) for faster reloads",
        value=SS["use_parquet_cache"] and HAS_PYARROW,
        help="L∆∞u b·∫£ng ƒë√£ load xu·ªëng ƒëƒ©a (Parquet) ƒë·ªÉ m·ªü l·∫°i nhanh."
    )
    if st.button("üßπ Clear cache", use_container_width=True):
        st.cache_data.clear()
        st.toast("Cache cleared", icon="üßπ")

# -- Main: Title + File Gate --
st.title("üìä Audit Statistics")
if SS["file_bytes"] is None:
    st.info("Upload a file ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    st.stop()

fname = SS["uploaded_name"]; fb = SS["file_bytes"]; sha = SS["sha12"]

topL, topR = st.columns([3, 2])
with topL:
    st.text_input("File", value=fname or "", disabled=True)
with topR:
    SS["pv_n"] = st.slider("Preview rows", 50, 500, SS["pv_n"], 50)
    do_preview = st.button("üîé Quick preview", key="btn_preview")

# -- Ingest: CSV vs XLSX --
if fname.lower().endswith(".csv"):
    # CSV: preview
    if do_preview or SS["df_preview"] is None:
        try:
            SS["df_preview"] = read_csv_fast(fb).head(SS["pv_n"])
        except Exception as e:
            st.error(f"L·ªói ƒë·ªçc CSV: {e}")
            SS["df_preview"] = None
    if SS["df_preview"] is not None:
        st.dataframe(SS["df_preview"], use_container_width=True, height=260)
        headers = list(SS["df_preview"].columns)
        selected = st.multiselect(
            "Columns to load", headers, default=headers, key="csv_cols"
        )
        if st.button("üì• Load full CSV with selected columns", key="btn_load_csv"):
            sel_key = ";".join(selected) if selected else "ALL"
            import hashlib as _hl
            key = f"csv_{_hl.sha1(sel_key.encode()).hexdigest()[:10]}"
            df_cached = read_parquet_cache(sha, key) if SS["use_parquet_cache"] else None
            if df_cached is None:
                df_full = read_csv_fast(fb, usecols=(selected or None))
                if SS["use_parquet_cache"]:
                    write_parquet_cache(df_full, sha, key)
            else:
                df_full = df_cached
            SS["df"] = df_full
            st.success(f"Loaded: {len(SS['df']):,} rows √ó {len(SS['df'].columns)} cols ‚Ä¢ SHA12={sha}")

else:
    # XLSX: select sheet, header, dtype map + preview
    sheets = list_sheets_xlsx(fb)
    with st.expander("üìÅ Select sheet & header (XLSX)", expanded=True):
        c1, c2, c3 = st.columns([2, 1, 1])
        idx = 0 if sheets else 0
        SS["xlsx_sheet"] = c1.selectbox("Sheet", sheets, index=idx)
        SS["header_row"] = c2.number_input("Header row (1‚Äëbased)", 1, 100, SS["header_row"])
        SS["skip_top"] = c3.number_input("Skip N rows after header", 0, 1000, SS["skip_top"])
        SS["dtype_choice"] = st.text_area(
            "dtype mapping (JSON, optional)", SS.get("dtype_choice", ""), height=60, key="dtype_json"
        )
        dtype_map = None
        if SS["dtype_choice"].strip():
            try:
                dtype_map = json.loads(SS["dtype_choice"])
            except Exception as e:
                st.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c dtype JSON: {e}")

        try:
            prev = read_xlsx_fast(
                fb, SS["xlsx_sheet"], usecols=None,
                header_row=SS["header_row"], skip_top=SS["skip_top"], dtype_map=dtype_map
            ).head(SS["pv_n"])
        except Exception as e:
            st.error(f"L·ªói ƒë·ªçc XLSX: {e}")
            prev = pd.DataFrame()

        st.dataframe(prev, use_container_width=True, height=260)
        headers = list(prev.columns)
        st.caption(f"Columns: {len(headers)} ‚Ä¢ SHA12={sha}")

        SS["col_filter"] = st.text_input("üîé Filter columns", SS.get("col_filter", ""))
        filtered = [h for h in headers if SS["col_filter"].lower() in h.lower()] if SS["col_filter"] else headers
        selected = st.multiselect(
            "üßÆ Columns to load",
            filtered if filtered else headers,
            default=filtered if filtered else headers,
            key="xlsx_cols"
        )

        if st.button("üì• Load full data", key="btn_load_xlsx"):
            import hashlib as _hl
            key_tuple = (SS["xlsx_sheet"], SS["header_row"], SS["skip_top"], tuple(selected) if selected else ("ALL",))
            key = f"xlsx_{_hl.sha1(str(key_tuple).encode()).hexdigest()[:10]}"
            df_cached = read_parquet_cache(sha, key) if SS["use_parquet_cache"] else None
            if df_cached is None:
                df_full = read_xlsx_fast(
                    fb, SS["xlsx_sheet"], usecols=(selected or None),
                    header_row=SS["header_row"], skip_top=SS["skip_top"], dtype_map=dtype_map
                )
                if SS["use_parquet_cache"]:
                    write_parquet_cache(df_full, sha, key)
            else:
                df_full = df_cached
            SS["df"] = df_full
            st.success(f"Loaded: {len(SS['df']):,} rows √ó {len(SS['df'].columns)} cols ‚Ä¢ SHA12={sha}")

# Gate after ingest (ƒë·ªÉ c√°c m·∫£ng sau s·ª≠ d·ª•ng df/preview)
if SS["df"] is None and SS["df_preview"] is None:
    st.stop()
# ==== COLUMN TYPING + DOWNSAMPLE VIEW + SPEARMAN SUGGESTION + TABS ====

# -- Pick source df (full if loaded, else from preview) --
df_src: pd.DataFrame = SS["df"] if SS["df"] is not None else SS["df_preview"].copy()

# -- Detect column types (lightweight, name-aware datetime) --
dt_cols = [c for c in df_src.columns if is_datetime_like(c, df_src[c])]
num_cols = df_src.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df_src.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

# -- View dataframe (downsample for visuals) --
DF_SAMPLE_MAX = 50_000
df_view = df_src
if SS.get("downsample_view", True) and len(df_view) > DF_SAMPLE_MAX:
    df_view = df_view.sample(DF_SAMPLE_MAX, random_state=42)
    st.caption("‚¨áÔ∏è Downsampled view to 50k rows (visuals & quick stats reflect this sample).")

# -- Expose for other tabs --
SS["df_view"] = df_view
SS["num_cols"] = num_cols
SS["cat_cols"] = cat_cols
SS["dt_cols"] = dt_cols

# Also keep handy aliases for later chunks
DF_VIEW = SS["df_view"]
DF_FULL = SS["df"] if SS["df"] is not None else DF_VIEW

# -- Spearman auto-recommendation (cache) --
@st.cache_data(ttl=900, show_spinner=False, max_entries=64)
def spearman_flag(df: pd.DataFrame, cols: List[str]) -> bool:
    """
    Return True if distributions suggest non-normal/outlier-heavy -> prefer Spearman.
    Heuristics: |skew|>1, |kurtosis|>3, tail share > 2% beyond p99, or normality p<0.05.
    """
    for c in cols[:20]:
        if c not in df.columns:
            continue
        s = pd.to_numeric(df[c], errors="coerce").replace([np.inf, -np.inf], np.nan).dropna()
        if len(s) < 50:
            continue
        try:
            sk = float(stats.skew(s)) if len(s) > 2 else 0.0
            ku = float(stats.kurtosis(s, fisher=True)) if len(s) > 3 else 0.0
        except Exception:
            sk = ku = 0.0
        p99 = s.quantile(0.99)
        tail = float((s > p99).mean())
        try:
            p_norm = float(stats.normaltest(s)[1]) if len(s) > 20 else 1.0
        except Exception:
            p_norm = 1.0
        if (abs(sk) > 1) or (abs(ku) > 3) or (tail > 0.02) or (p_norm < 0.05):
            return True
    return False

spearman_recommended = spearman_flag(DF_VIEW, num_cols)
SS["spearman_recommended"] = spearman_recommended
# -- Tabs scaffold (content filled in next chunks) --
TAB1, TAB2, TAB3, TAB4, TAB5, TAB6, TAB7 = st.tabs([
    "1) Profiling", "2) Trend & Corr", "3) Benford",
    "4) Tests", "5) Regression", "6) Flags", "7) Risk & Export"])
    
# ==== TAB 1: PROFILING (Distribution & Shape) ====
    
with TAB1:
    st.subheader("üìà Distribution & Shape")
    # --- Test Navigator (nhanh, kh√¥ng l·∫∑p ƒë·ªì th·ªã n·∫∑ng) ---
    navL, navR = st.columns([2, 3])
    with navL:
        col_nav = st.selectbox("Ch·ªçn c·ªôt", DF_VIEW.columns.tolist(), key="t1_nav_col")
        s_nav = DF_VIEW[col_nav]
        if col_nav in SS["num_cols"]:
            dtype_nav = "Numeric"
        elif col_nav in SS["dt_cols"] or is_datetime_like(col_nav, s_nav):
            dtype_nav = "Datetime"
        else:
            dtype_nav = "Categorical"
        st.write(f"**Lo·∫°i d·ªØ li·ªáu:** {dtype_nav}")

    with navR:
        sugg = []
        if dtype_nav == "Numeric":
            sugg += ["Histogram + KDE", "Box/ECDF/QQ", "Outlier review (IQR)", "Benford 1D/2D (n‚â•300, >0)"]
        elif dtype_nav == "Categorical":
            sugg += ["Top‚ÄëN + Pareto", "Chi‚Äësquare GoF vs Uniform", "Rare category flag/Group 'Others'"]
        else:
            sugg += ["Weekday/Hour distribution", "Seasonality (Month/Quarter)", "Gap/Sequence test"]
        st.write("**G·ª£i √Ω test:**")
        for si in sugg:
            st.write(f"- {si}")
    st.divider()
    # --- Sub‚Äëtabs ---
    sub_num, sub_cat, sub_dt = st.tabs(["Numeric", "Categorical", "Datetime"])
    # ==================== NUMERIC ====================
    with sub_num:
        if not SS["num_cols"]:
            st.info("Kh√¥ng ph√°t hi·ªán c·ªôt numeric.")
        else:
            c1, c2 = st.columns(2)
            with c1:
                num_col = st.selectbox("Numeric column", SS["num_cols"], key="t1_num")
            with c2:
                kde_on = st.checkbox("KDE (n ‚â§ ng∆∞·ª°ng)", value=True, help="T·ª± t·∫Øt khi n qu√° l·ªõn/variance=0.")

            s0 = pd.to_numeric(DF_VIEW[num_col], errors="coerce").replace([np.inf, -np.inf], np.nan)
            s = s0.dropna()
            n_na = int(s0.isna().sum())

            if s.empty:
                st.warning("Kh√¥ng c√≤n gi√° tr·ªã numeric sau khi l√†m s·∫°ch.")
            else:
                # Stats table (chu·∫©n ho√°, ng·∫Øn g·ªçn)
                desc_dict, skew, kurt, p_norm, p95, p99, zero_ratio = numeric_profile_stats(s)
                stat_df = pd.DataFrame([{
                    "count": int(desc_dict.get("count", 0)),
                    "n_missing": n_na,
                    "mean": desc_dict.get("mean"),
                    "std": desc_dict.get("std"),
                    "min": desc_dict.get("min"),
                    "p1": desc_dict.get("1%"),
                    "p5": desc_dict.get("5%"),
                    "q1": desc_dict.get("25%"),
                    "median": desc_dict.get("50%"),
                    "q3": desc_dict.get("75%"),
                    "p95": desc_dict.get("95%"),
                    "p99": desc_dict.get("99%"),
                    "max": desc_dict.get("max"),
                    "skew": skew,
                    "kurtosis": kurt,
                    "zero_ratio": zero_ratio,
                    "tail>p95": float((s > p95).mean()) if not np.isnan(p95) else None,
                    "tail>p99": float((s > p99).mean()) if not np.isnan(p99) else None,
                    "normality_p": (round(p_norm, 4) if not np.isnan(p_norm) else None),
                }])
                st.dataframe(stat_df, use_container_width=True, height=220)

                # ---- Visuals ----
                if HAS_PLOTLY:
                    gA, gB = st.columns(2)

                    with gA:
                        fig1 = go.Figure()
                        fig1.add_trace(go.Histogram(x=s, nbinsx=SS["bins"], name="Histogram", opacity=0.8))
                        # KDE guarded (n, variance, threshold)
                        if kde_on and (len(s) <= SS["kde_threshold"]) and (s.var() > 0) and (len(s) > 10):
                            try:
                                from scipy.stats import gaussian_kde
                                xs = np.linspace(s.min(), s.max(), 256)
                                kde = gaussian_kde(s)
                                ys = kde(xs)
                                # scale l√™n c√πng ƒë∆°n v·ªã count
                                ys_scaled = ys * len(s) * (xs[1] - xs[0])
                                fig1.add_trace(go.Scatter(x=xs, y=ys_scaled, name="KDE",
                                                          line=dict(color="#E4572E")))
                            except Exception:
                                pass
                        if SS["log_scale"] and (s > 0).all():
                            fig1.update_xaxes(type="log")
                        fig1.update_layout(title=f"{num_col} ‚Äî Histogram+KDE", height=320)
                        st_plotly(fig1)
                        register_fig("Profiling", f"{num_col} ‚Äî Histogram+KDE", fig1,
                                     "H√¨nh d·∫°ng ph√¢n ph·ªëi & ƒëu√¥i; KDE l√†m m∆∞·ª£t m·∫≠t ƒë·ªô.")

                    with gB:
                        fig2 = px.box(pd.DataFrame({num_col: s}), x=num_col,
                                      points="outliers", title=f"{num_col} ‚Äî Box")
                        st_plotly(fig2)
                        register_fig("Profiling", f"{num_col} ‚Äî Box", fig2, "Trung v·ªã/IQR & outliers.")

                    gC, gD = st.columns(2)
                    with gC:
                        try:
                            fig3 = px.ecdf(s, title=f"{num_col} ‚Äî ECDF")
                            st_plotly(fig3)
                            register_fig("Profiling", f"{num_col} ‚Äî ECDF", fig3, "Ph√¢n ph·ªëi t√≠ch lu·ªπ P(X‚â§x).")
                        except Exception:
                            st.caption("ECDF y√™u c·∫ßu plotly phi√™n b·∫£n h·ªó tr·ª£ px.ecdf.")

                    with gD:
                        try:
                            osm, osr = stats.probplot(s, dist="norm", fit=False)
                            xq = np.array(osm[0]); yq = np.array(osm[1])
                            fig4 = go.Figure()
                            fig4.add_trace(go.Scatter(x=xq, y=yq, mode="markers", name="QQ points"))
                            lim = [min(xq.min(), yq.min()), max(xq.max(), yq.max())]
                            fig4.add_trace(go.Scatter(x=lim, y=lim, mode="lines",
                                                      line=dict(dash="dash"), name="45¬∞"))
                            fig4.update_layout(title=f"{num_col} ‚Äî QQ Normal", height=320)
                            st_plotly(fig4)
                            register_fig("Profiling", f"{num_col} ‚Äî QQ Normal", fig4, "L·ªách so v·ªõi Normal.")
                        except Exception:
                            st.caption("C·∫ßn SciPy cho QQ plot.")

                    # Advanced (·∫©n m·∫∑c ƒë·ªãnh)
                    if SS["advanced_visuals"]:
                        gE, gF = st.columns(2)
                        with gE:
                            figv = px.violin(pd.DataFrame({num_col: s}), x=num_col, points="outliers",
                                             box=True, title=f"{num_col} ‚Äî Violin")
                            st_plotly(figv)
                            register_fig("Profiling", f"{num_col} ‚Äî Violin", figv, "M·∫≠t ƒë·ªô + Box overlay.")
                        with gF:
                            v = np.sort(s.values)
                            if len(v) > 0 and v.sum() != 0:
                                cum = np.cumsum(v)
                                lor = np.insert(cum, 0, 0) / cum.sum()
                                x = np.linspace(0, 1, len(lor))
                                gini = 1 - 2 * np.trapz(lor, dx=1 / len(v))
                                figL = go.Figure()
                                figL.add_trace(go.Scatter(x=x, y=lor, name="Lorenz", mode="lines"))
                                figL.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode="lines",
                                                          name="Equality", line=dict(dash="dash")))
                                figL.update_layout(title=f"{num_col} ‚Äî Lorenz (Gini={gini:.3f})", height=320)
                                st_plotly(figL)
                                register_fig("Profiling", f"{num_col} ‚Äî Lorenz", figL, "T·∫≠p trung gi√° tr·ªã.")
                            else:
                                st.caption("Kh√¥ng th·ªÉ t√≠nh Lorenz/Gini do t·ªïng = 0 ho·∫∑c d·ªØ li·ªáu r·ªóng.")
                # Optional GoF (n·∫øu h√†m c√≥ m·∫∑t ·ªü project)
                if "gof_models" in globals():
                    try:
                        gof, best, suggest = gof_models(s)
                        st.markdown("### üìò GoF (Normal / Lognormal / Gamma) ‚Äî AIC & Transform")
                        st.dataframe(gof, use_container_width=True, height=150)
                        st.info(f"**Best fit:** {best}. **Suggested transform:** {suggest}")
                    except Exception:
                        pass
                # G·ª£i √Ω test ng·∫Øn g·ªçn
                recs = []
                if (not np.isnan(skew) and abs(skew) > 1) or (not np.isnan(kurt) and abs(kurt) > 3) or \
                        (not np.isnan(p_norm) and p_norm < 0.05):
                    recs.append("∆Øu ti√™n Spearman/phi tham s·ªë, ho·∫∑c transform r·ªìi ANOVA/t‚Äëtest.")
                if zero_ratio and zero_ratio > 0.3:
                    recs.append("Zero‚Äëheavy ‚Üí Proportion œá¬≤/Fisher theo nh√≥m; so√°t policy/threshold.")
                if float((s > p99).mean()) > 0.02:
                    recs.append("ƒêu√¥i ph·∫£i d√†y (p99) ‚Üí Benford 1D/2D; outlier review; cut‚Äëoff cu·ªëi k·ª≥.")
                if len(SS["num_cols"]) >= 2:
                    recs.append("Xem Correlation (∆∞u ti√™n Spearman n·∫øu outlier/non‚Äënormal).")
                st.markdown("**Recommended tests (Numeric):**\n" + "\n".join([f"- {x}" for x in recs]) if recs
                            else "- Kh√¥ng c√≥ ƒë·ªÅ xu·∫•t ƒë·∫∑c bi·ªát.")
    # ==================== CATEGORICAL ====================
    with sub_cat:
        if not SS["cat_cols"]:
            st.info("Kh√¥ng ph√°t hi·ªán c·ªôt categorical.")
        else:
            cat_col = st.selectbox("Categorical column", SS["cat_cols"], key="t1_cat")
            df_freq = cat_freq(DF_VIEW[cat_col])
            topn = st.number_input("Top‚ÄëN (Pareto)", 3, 50, 15, step=1)
            st.dataframe(df_freq.head(int(topn)), use_container_width=True, height=240)

            if HAS_PLOTLY and not df_freq.empty:
                d = df_freq.head(int(topn)).copy()
                d["cum_share"] = d["count"].cumsum() / d["count"].sum()
                figp = make_subplots(specs=[[{"secondary_y": True}]])
                figp.add_trace(go.Bar(x=d["category"], y=d["count"], name="Count"))
                figp.add_trace(go.Scatter(x=d["category"], y=d["cum_share"] * 100,
                                          name="Cumulative %", mode="lines+markers"), secondary_y=True)
                figp.update_yaxes(title_text="Count", secondary_y=False)
                figp.update_yaxes(title_text="Cumulative %", range=[0, 100], secondary_y=True)
                figp.update_layout(title=f"{cat_col} ‚Äî Pareto (Top {int(topn)})", height=360)
                st_plotly(figp)
                register_fig("Profiling", f"{cat_col} ‚Äî Pareto Top{int(topn)}", figp, "Pareto 80/20.")

            with st.expander("üî¨ Chi‚Äësquare GoF vs Uniform (tu·ª≥ ch·ªçn)"):
                if st.checkbox("Ch·∫°y œá¬≤ GoF vs Uniform", value=False, key="t1_gof_uniform"):
                    obs = df_freq.set_index("category")["count"]
                    if len(obs) >= 2 and obs.sum() > 0:
                        k = len(obs)
                        exp = pd.Series([obs.sum()/k]*k, index=obs.index)
                        chi2 = float(((obs - exp)**2 / exp).sum()); dof = k - 1
                        p = float(1 - stats.chi2.cdf(chi2, dof))
                        std_resid = (obs - exp) / np.sqrt(exp)
                        res_tbl = pd.DataFrame({"count": obs, "expected": exp, "std_resid": std_resid}) \
                            .sort_values("std_resid", key=lambda s: s.abs(), ascending=False)
                        st.write({"Chi2": round(chi2, 3), "dof": dof, "p": round(p, 4)})
                        st.dataframe(res_tbl, use_container_width=True, height=240)
                        if HAS_PLOTLY:
                            figr = px.bar(res_tbl.reset_index().head(20), x="category", y="std_resid",
                                          title="Standardized residuals (Top |resid|)",
                                          color="std_resid", color_continuous_scale="RdBu")
                            st_plotly(figr)
                            register_fig("Profiling", f"{cat_col} ‚Äî œá¬≤ GoF residuals", figr,
                                         "Nh√≥m l·ªách m·∫°nh vs uniform.")
                    else:
                        st.warning("C·∫ßn ‚â•2 nh√≥m c√≥ quan s√°t.")

            # Recommendations
            recs_c = []
            if not df_freq.empty:
                top1_share = float(df_freq["share"].iloc[0])
                if top1_share > 0.5:
                    recs_c.append("Ph√¢n b·ªï t·∫≠p trung (Top1>50%) ‚Üí Independence œá¬≤ v·ªõi bi·∫øn tr·∫°ng th√°i/ƒë∆°n v·ªã.")
                if df_freq["share"].head(10).sum() > 0.9:
                    recs_c.append("Pareto d·ªëc (Top10>90%) ‚Üí t·∫≠p trung ki·ªÉm th·ª≠ nh√≥m Top; g·ªôp nh√≥m nh·ªè v√†o 'Others'.")
                recs_c.append("N·∫øu c√≥ bi·∫øn k·∫øt qu·∫£ (flag/status) ‚Üí œá¬≤ ƒë·ªôc l·∫≠p (b·∫£ng ch√©o Category √ó Flag).")
            st.markdown("**Recommended tests (Categorical):**\n" + "\n".join([f"- {x}" for x in recs_c]) if recs_c
                        else "- Kh√¥ng c√≥ ƒë·ªÅ xu·∫•t ƒë·∫∑c bi·ªát.")
    # ==================== DATETIME ====================
    with sub_dt:
        # t·∫≠p h·ª£p ·ª©ng vi√™n datetime theo dtype/name
        dt_candidates = SS["dt_cols"]
        if not dt_candidates:
            st.info("Kh√¥ng ph√°t hi·ªán c·ªôt datetime‚Äëlike.")
        else:
            dt_col = st.selectbox("Datetime column", dt_candidates, key="t1_dt")
            t = pd.to_datetime(DF_VIEW[dt_col], errors="coerce")
            t_clean = t.dropna()
            n_missing = int(t.isna().sum())

            meta = pd.DataFrame([{
                "count": int(len(t)),
                "n_missing": n_missing,
                "min": (t_clean.min() if not t_clean.empty else None),
                "max": (t_clean.max() if not t_clean.empty else None),
                "span_days": (int((t_clean.max() - t_clean.min()).days) if len(t_clean) > 1 else None),
                "n_unique_dates": int(t_clean.dt.date.nunique()) if not t_clean.empty else 0
            }])
            st.dataframe(meta, use_container_width=True, height=120)

            if HAS_PLOTLY and not t_clean.empty:
                d1, d2 = st.columns(2)
                with d1:
                    dow = t_clean.dt.dayofweek
                    dow_share = dow.value_counts(normalize=True).sort_index()
                    figD = px.bar(x=["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"],
                                  y=dow_share.reindex(range(7), fill_value=0).values,
                                  title="DOW distribution", labels={"x": "DOW", "y": "Share"})
                    st_plotly(figD)
                    register_fig("Profiling", "DOW distribution", figD, "Ph√¢n b·ªë theo th·ª© trong tu·∫ßn.")
                with d2:
                    if not t_clean.dt.hour.isna().all():
                        hour = t_clean.dt.hour
                        hcnt = hour.value_counts().sort_index()
                        figH = px.bar(x=hcnt.index, y=hcnt.values,
                                      title="Hourly histogram (0‚Äì23)", labels={"x": "Hour", "y": "Count"})
                        st_plotly(figH)
                        register_fig("Profiling", "Hourly histogram (0‚Äì23)", figH, "M·∫´u ho·∫°t ƒë·ªông theo gi·ªù.")

                d3, d4 = st.columns(2)
                with d3:
                    m = t_clean.dt.month
                    m_cnt = m.value_counts().sort_index()
                    figM = px.bar(x=m_cnt.index, y=m_cnt.values, title="Monthly seasonality (count)",
                                  labels={"x": "Month", "y": "Count"})
                    st_plotly(figM)
                    register_fig("Profiling", "Monthly seasonality", figM, "T√≠nh m√πa v·ª• theo th√°ng.")
                with d4:
                    q = t_clean.dt.quarter
                    q_cnt = q.value_counts().sort_index()
                    figQ = px.bar(x=q_cnt.index, y=q_cnt.values, title="Quarterly seasonality (count)",
                                  labels={"x": "Quarter", "y": "Count"})
                    st_plotly(figQ)
                    register_fig("Profiling", "Quarterly seasonality", figQ, "T√≠nh m√πa v·ª• theo qu√Ω.")

            # G·ª£i √Ω ki·ªÉm th·ª≠ th·ªùi gian
            recs_t = []
            if not t_clean.empty:
                try:
                    eom_share = float(t_clean.dt.is_month_end.mean())
                    if eom_share > 0.1:
                        recs_t.append("Spike cu·ªëi th√°ng >10% ‚Üí ki·ªÉm tra cut‚Äëoff; œá¬≤ theo bucket th·ªùi gian √ó status.")
                except Exception:
                    pass
                try:
                    if not t_clean.dt.hour.isna().all():
                        off = ((t_clean.dt.hour < 7) | (t_clean.dt.hour > 20)).mean()
                        if float(off) > 0.15:
                            recs_t.append("Ho·∫°t ƒë·ªông off‚Äëhours >15% ‚Üí review ph√¢n quy·ªÅn/ca tr·ª±c; œá¬≤ (Hour √ó Flag).")
                except Exception:
                    pass
                recs_t.append("C√≥ bi·∫øn numeric ‚Üí Trend (D/W/M/Q + Rolling) & test c·∫•u tr√∫c (pre/post k·ª≥).")
            else:
                recs_t.append("Chuy·ªÉn c·ªôt sang datetime (pd.to_datetime) ƒë·ªÉ k√≠ch ho·∫°t ph√¢n t√≠ch th·ªùi gian.")
            st.markdown("**Recommended tests (Datetime):**\n" + "\n".join([f"- {x}" for x in recs_t]))
# ==== TAB 2: TREND & CORRELATION ====
with TAB2:
    st.subheader("üìà Trend & üîó Correlation")

    # --- Input detection ---
    dt_candidates = SS["dt_cols"]
    num_cols = SS["num_cols"]

    # ==================== TREND ====================
    trendL, trendR = st.columns(2)
    with trendL:
        num_for_trend = st.selectbox(
            "Numeric (trend)",
            num_cols or DF_VIEW.columns.tolist(),
            key="t2_num"
        )
        dt_for_trend = st.selectbox(
            "Datetime column",
            dt_candidates or DF_VIEW.columns.tolist(),
            key="t2_dt"
        )
        freq = st.selectbox("Aggregate frequency", ["D", "W", "M", "Q"], index=2)
        agg_opt = st.radio("Aggregate by", ["sum", "mean", "count"], index=0, horizontal=True)
        win = st.slider("Rolling window (periods)", 2, 24, 3)
@st.cache_data(ttl=900, show_spinner=False, max_entries=64)
def ts_aggregate_cached(df: pd.DataFrame, dt_col: str, y_col: str, freq: str, agg: str, win: int) -> pd.DataFrame:
    # Chu·∫©n ho√° ki·ªÉu
    t = pd.to_datetime(df[dt_col], errors="coerce")
    y = pd.to_numeric(df[y_col], errors="coerce")

    # L·ªçc NA v√† s·∫Øp x·∫øp theo th·ªùi gian
    sub = pd.DataFrame({"t": t, "y": y}).dropna().sort_values("t")
    if sub.empty:
        return pd.DataFrame()

    ts = sub.set_index("t")["y"]
    if agg == "count":
        ser = ts.resample(freq).count()
    elif agg == "mean":
        ser = ts.resample(freq).mean()
    else:
        ser = ts.resample(freq).sum()

    out = ser.to_frame("y")
    out["roll"] = out["y"].rolling(win, min_periods=1).mean()

    # Fallback cho pandas c≈© (reset_index kh√¥ng h·ªó tr·ª£ names=)
    try:
        return out.reset_index(names="t")
    except TypeError:
        return out.reset_index().rename(columns={"index": "t"})

    with trendR:
        if (dt_for_trend in DF_VIEW.columns) and (num_for_trend in DF_VIEW.columns):
            tsdf = ts_aggregate_cached(DF_VIEW, dt_for_trend, num_for_trend, freq, agg_opt, win)
            if tsdf.empty:
                st.warning("Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi chu·∫©n ho√° datetime/numeric.")
            else:
                if HAS_PLOTLY:
                    figt = go.Figure()
                    figt.add_trace(go.Scatter(x=tsdf["t"], y=tsdf["y"], name=f"{agg_opt.capitalize()}"))
                    figt.add_trace(go.Scatter(
                        x=tsdf["t"], y=tsdf["roll"], name=f"Rolling{win}", line=dict(dash="dash")))
                    figt.update_layout(title=f"{num_for_trend} ‚Äî Trend ({freq})", height=360)
                    st_plotly(figt)
                    register_fig("Trend", f"{num_for_trend} ‚Äî Trend ({freq})", figt,
                                 "Chu·ªói th·ªùi gian (aggregate + rolling).")
                st.caption("**G·ª£i √Ω**: Spike cu·ªëi k·ª≥ ‚Üí test cut‚Äëoff; so s√°nh c√°c k·ª≥/t·∫ßng theo ƒë∆°n v·ªã.")
        else:
            st.info("Ch·ªçn 1 c·ªôt numeric v√† 1 c·ªôt datetime h·ª£p l·ªá ƒë·ªÉ xem Trend.")

    st.divider()

    # ==================== CORRELATION ====================
# ==================== CORRELATION ====================
st.markdown("### üîó Correlation heatmap")

num_cols = SS["num_cols"]

if len(num_cols) < 2:
    st.info("C·∫ßn ‚â•2 c·ªôt numeric ƒë·ªÉ t√≠nh t∆∞∆°ng quan.")
else:
    # 1) Ch·ªçn subset c·ªôt (tr√°nh heatmap qu√° l·ªõn)
    with st.expander("üß∞ Tu·ª≥ ch·ªçn c·ªôt (m·∫∑c ƒë·ªãnh: t·∫•t c·∫£ numeric)"):
        default_cols = num_cols[:30]  # b·∫£o v·ªá UI n·∫øu qu√° nhi·ªÅu bi·∫øn
        pick_cols = st.multiselect(
            "Ch·ªçn c·ªôt ƒë·ªÉ t√≠nh t∆∞∆°ng quan",
            options=num_cols,
            default=default_cols,
            key="t2_corr_cols"
        )
        if len(pick_cols) < 2:
            st.warning("Ch·ªçn √≠t nh·∫•t 2 c·ªôt ƒë·ªÉ t√≠nh t∆∞∆°ng quan.")

    # 2) Ch·ªçn ph∆∞∆°ng ph√°p (t·ª± ƒë·ªÅ xu·∫•t Spearman)
    method_label = "Spearman (recommended)" if SS.get("spearman_recommended") else "Spearman"
    method = st.radio(
        "Correlation method",
        ["Pearson", method_label],
        index=(1 if SS.get("spearman_recommended") else 0),
        horizontal=True,
        key="t2_corr_m"
    )
    mth = "pearson" if method.startswith("Pearson") else "spearman"

    # 3) T√≠nh v√† v·∫Ω heatmap
    if len(pick_cols) >= 2:
        corr = corr_cached(DF_VIEW, pick_cols, mth)  # <-- ch·ªâ G·ªåI, kh√¥ng ƒë·ªãnh nghƒ©a ·ªü ƒë√¢y
        if corr.empty:
            st.warning("Kh√¥ng th·ªÉ t√≠nh ma tr·∫≠n t∆∞∆°ng quan (c√≥ th·ªÉ do c√°c c·ªôt h·∫±ng ho·∫∑c NA).")
        else:
            if HAS_PLOTLY:
                figH = px.imshow(
                    corr, color_continuous_scale="RdBu_r", zmin=-1, zmax=1,
                    title=f"Correlation heatmap ({mth.capitalize()})", aspect="auto"
                )
                figH.update_xaxes(tickangle=45)
                st_plotly(figH)
                register_fig("Correlation", f"Correlation heatmap ({mth.capitalize()})", figH,
                             "Li√™n h·ªá tuy·∫øn t√≠nh/h·∫°ng gi·ªØa c√°c bi·∫øn.")
            # Top pairs (|r| cao)
            with st.expander("üìå Top t∆∞∆°ng quan theo |r| (b·ªè ƒë∆∞·ªùng ch√©o)"):
                tri = corr.where(~np.eye(len(corr), dtype=bool))  # mask diagonal
                pairs = []
                cols = list(tri.columns)
                for i in range(len(cols)):
                    for j in range(i + 1, len(cols)):
                        r = tri.iloc[i, j]
                        if pd.notna(r):
                            pairs.append((cols[i], cols[j], float(r), abs(float(r))))
                pairs = sorted(pairs, key=lambda x: x[3], reverse=True)[:30]
                if pairs:
                    df_pairs = pd.DataFrame(pairs, columns=["var1", "var2", "r", "|r|"])
                    st.dataframe(df_pairs, use_container_width=True, height=260)
                else:
                    st.write("Kh√¥ng c√≥ c·∫∑p ƒë√°ng k·ªÉ.")

# (t√πy ch·ªçn) Scatter nhanh hai bi·∫øn ‚Äî c√≥ th·ªÉ ƒë·ªÉ sau correlation
with st.expander("üîé Scatter nhanh hai bi·∫øn (tu·ª≥ ch·ªçn)"):
    others = [c for c in SS["num_cols"]]
    if others:
        xvar = st.selectbox("X", options=others, index=0, key="t2_sc_x")
        y_candidates = [c for c in others if c != xvar] or others[:1]
        yvar = st.selectbox("Y", options=y_candidates, index=0, key="t2_sc_y")
        run_sc = st.button("V·∫Ω scatter", key="t2_sc_btn")
        if run_sc:
            sub = DF_VIEW[[xvar, yvar]].apply(pd.to_numeric, errors="coerce").dropna()
            if len(sub) < 10:
                st.warning("Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi lo·∫°i NA (c·∫ßn ‚â•10).")
            else:
                try:
                    if mth == "pearson":
                        r, pv = stats.pearsonr(sub[xvar], sub[yvar])
                        trendline = "ols"
                    else:
                        r, pv = stats.spearmanr(sub[xvar], sub[yvar])
                        trendline = None
                    if HAS_PLOTLY:
                        fig = px.scatter(sub, x=xvar, y=yvar, trendline=trendline,
                                         title=f"{xvar} vs {yvar} ({mth.capitalize()})")
                        st_plotly(fig)
                        register_fig("Correlation", f"{xvar} vs {yvar} ({mth.capitalize()})", fig,
                                     "Minh ho·∫° quan h·ªá hai bi·∫øn.")
                    st.json({"method": mth, "r": round(float(r), 4), "p": round(float(pv), 5)})
                except Exception as e:
                    st.error(f"Scatter error: {e}")
    else:
        st.info("Kh√¥ng c√≥ c·ªôt numeric ƒë·ªÉ v·∫Ω scatter.")

# ==== TAB 3: BENFORD (1D / 2D) ====
# -- Benford helpers (module-level) --
@st.cache_data(ttl=3600, show_spinner=False, max_entries=64)
def _benford_1d(series: pd.Series):
    s = pd.to_numeric(series, errors="coerce").replace([np.inf, -np.inf], np.nan).dropna().abs()
    if s.empty:
        return None
    def _digits(x):
        xs = ("%.15g" % float(x))
        return re.sub(r"[^0-9]", "", xs).lstrip("0")
    d1 = s.apply(lambda v: int(_digits(v)[0]) if len(_digits(v)) >= 1 else np.nan).dropna()
    d1 = d1[(d1 >= 1) & (d1 <= 9)]
    if d1.empty: 
        return None
    obs = d1.value_counts().sort_index().reindex(range(1, 10), fill_value=0).astype(float)
    n = obs.sum()
    obs_p = obs / n
    idx = np.arange(1, 10)
    exp_p = np.log10(1 + 1/idx)
    exp = exp_p * n
    with np.errstate(divide="ignore", invalid="ignore"):
        chi2 = np.nansum((obs - exp) ** 2 / exp)
        pval = 1 - stats.chi2.cdf(chi2, len(idx) - 1)
    mad = float(np.mean(np.abs(obs_p - exp_p)))
    var_tbl = pd.DataFrame({"digit": idx, "expected": exp, "observed": obs.values})
    var_tbl["diff"] = var_tbl["observed"] - var_tbl["expected"]
    var_tbl["diff_pct"] = (var_tbl["observed"] - var_tbl["expected"]) / var_tbl["expected"]
    table = pd.DataFrame({"digit": idx, "observed_p": obs_p.values, "expected_p": exp_p})
    return {"table": table, "variance": var_tbl, "n": int(n), "chi2": float(chi2), "p": float(pval), "MAD": float(mad)}

@st.cache_data(ttl=3600, show_spinner=False, max_entries=64)
def _benford_2d(series: pd.Series):
    s = pd.to_numeric(series, errors="coerce").replace([np.inf, -np.inf], np.nan).dropna().abs()
    if s.empty:
        return None
    def _digits(x):
        xs = ("%.15g" % float(x))
        return re.sub(r"[^0-9]", "", xs).lstrip("0")
    def _first2(v):
        ds = _digits(v)
        if len(ds) >= 2:
            return int(ds[:2])
        if len(ds) == 1 and ds != "0":
            return int(ds)
        return np.nan
    d2 = s.apply(_first2).dropna()
    d2 = d2[(d2 >= 10) & (d2 <= 99)]
    if d2.empty:
        return None
    obs = d2.value_counts().sort_index().reindex(range(10, 100), fill_value=0).astype(float)
    n = obs.sum()
    obs_p = obs / n
    idx = np.arange(10, 100)
    exp_p = np.log10(1 + 1/idx)
    exp = exp_p * n
    with np.errstate(divide="ignore", invalid="ignore"):
        chi2 = np.nansum((obs - exp) ** 2 / exp)
        pval = 1 - stats.chi2.cdf(chi2, len(idx) - 1)
    mad = float(np.mean(np.abs(obs_p - exp_p)))
    var_tbl = pd.DataFrame({"digit": idx, "expected": exp, "observed": obs.values})
    var_tbl["diff"] = var_tbl["observed"] - var_tbl["expected"]
    var_tbl["diff_pct"] = (var_tbl["observed"] - var_tbl["expected"]) / var_tbl["expected"]
    table = pd.DataFrame({"digit": idx, "observed_p": obs_p.values, "expected_p": exp_p})
    return {"table": table, "variance": var_tbl, "n": int(n), "chi2": float(chi2), "p": float(pval), "MAD": float(mad)}

def _benford_ready(series: pd.Series) -> tuple[bool, str]:
    s = pd.to_numeric(series, errors="coerce")
    n_pos = int((s > 0).sum())
    if n_pos < 300:
        return False, f"Kh√¥ng ƒë·ªß m·∫´u >0 cho Benford (hi·ªán {n_pos}, c·∫ßn ‚â•300)."
    s_non = s.dropna()
    if s_non.shape[0] > 0:
        ratio_unique = s_non.nunique() / s_non.shape[0]
        if ratio_unique > 0.95:
            return False, "T·ªâ l·ªá unique qu√° cao (kh·∫£ nƒÉng ID/Code) ‚Äî tr√°nh Benford."
    return True, ""

# -- Maintain state for results (song song) --
for k in ["bf1_res", "bf2_res", "bf1_col", "bf2_col"]:
    if k not in SS:
        SS[k] = None

with TAB3:
    st.subheader("üî¢ Benford Law ‚Äî 1D & 2D")
    if "bf_use_full" not in SS:
        SS["bf_use_full"] = True
    if not SS["num_cols"]:
        st.info("Kh√¥ng c√≥ c·ªôt numeric ƒë·ªÉ ch·∫°y Benford.")
    else:
        run_on_full = (SS["df"] is not None) and st.checkbox(
            "Use FULL dataset thay v√¨ sample (khuy·∫øn ngh·ªã cho Benford)", value=True, key="bf_use_full"
        )
        data_for_benford = SS["df"] if (run_on_full and SS["df"] is not None) else DF_VIEW
        if (not run_on_full) and (SS["df"] is not None):
            st.caption("‚ÑπÔ∏è ƒêang d√πng SAMPLE do b·∫°n t·∫Øt 'Use FULL'. B·∫≠t l·∫°i ƒë·ªÉ c√≥ k·∫øt qu·∫£ Benford ·ªïn ƒë·ªãnh h∆°n.")

        c1, c2 = st.columns(2)
        with c1:
            amt1 = st.selectbox("Amount (1D)", SS["num_cols"], key="bf1_col")
            if st.button("Run Benford 1D", key="btn_bf1"):
                ok, msg = _benford_ready(data_for_benford[amt1])
                if not ok:
                    st.warning(msg)
                else:
                    r = _benford_1d(data_for_benford[amt1])
                    if not r:
                        st.error("Kh√¥ng th·ªÉ tr√≠ch ch·ªØ s·ªë ƒë·∫ßu ti√™n.")
                    else:
                        SS["bf1_res"] = r

        with c2:
            default_idx = 1 if len(SS["num_cols"]) > 1 else 0
            amt2 = st.selectbox("Amount (2D)", SS["num_cols"], index=default_idx, key="bf2_col")
            if st.button("Run Benford 2D", key="btn_bf2"):
                ok, msg = _benford_ready(data_for_benford[amt2])
                if not ok:
                    st.warning(msg)
                else:
                    r2 = _benford_2d(data_for_benford[amt2])
                    if not r2:
                        st.error("Kh√¥ng th·ªÉ tr√≠ch ch·ªØ s·ªë ƒë·∫ßu ti√™n‚Äìhai.")
                    else:
                        SS["bf2_res"] = r2

    # --- Parallel render (n·∫øu c√≥ k·∫øt qu·∫£) ---
    g1, g2 = st.columns(2)

    with g1:
        if SS.get("bf1_res"):
            r = SS["bf1_res"]
            tb, var, p, MAD = r["table"], r["variance"], r["p"], r["MAD"]
            if HAS_PLOTLY:
                fig1 = go.Figure()
                fig1.add_trace(go.Bar(x=tb["digit"], y=tb["observed_p"], name="Observed"))
                fig1.add_trace(go.Scatter(
                    x=tb["digit"], y=tb["expected_p"], name="Expected",
                    mode="lines", line=dict(color="#F6AE2D")))
                src_tag = "FULL" if (SS["df"] is not None and SS.get("bf_use_full")) else "SAMPLE"
                fig1.update_layout(title=f"Benford 1D ‚Äî Obs vs Exp ({SS.get('bf1_col')}, {src_tag})", height=340)
                st_plotly(fig1)
                register_fig("Benford 1D", "Benford 1D ‚Äî Obs vs Exp", fig1, "Benford 1D check.")

            st.dataframe(var, use_container_width=True, height=220)

            thr = SS["risk_diff_threshold"]
            maxdiff = float(var["diff_pct"].abs().max()) if len(var) > 0 else 0.0
            # Diff badge
            msg = "üü¢ Green"
            if maxdiff >= 2 * thr:
                msg = "üö® Red"
            elif maxdiff >= thr:
                msg = "üü° Yellow"
            # Severity by p & MAD (th·ª±c h√†nh)
            sev = "üü¢ Green"
            if (p < 0.01) or (MAD > 0.015):
                sev = "üö® Red"
            elif (p < 0.05) or (MAD > 0.012):
                sev = "üü° Yellow"
            st.info(f"Diff% status: {msg} ‚Ä¢ p={p:.4f}, MAD={MAD:.4f} ‚áí Benford severity: {sev}")

    with g2:
        if SS.get("bf2_res"):
            r2 = SS["bf2_res"]
            tb2, var2, p2, MAD2 = r2["table"], r2["variance"], r2["p"], r2["MAD"]
            if HAS_PLOTLY:
                fig2 = go.Figure()
                fig2.add_trace(go.Bar(x=tb2["digit"], y=tb2["observed_p"], name="Observed"))
                fig2.add_trace(go.Scatter(
                    x=tb2["digit"], y=tb2["expected_p"], name="Expected",
                    mode="lines", line=dict(color="#F6AE2D")))
                src_tag = "FULL" if (SS["df"] is not None and SS.get("bf_use_full")) else "SAMPLE"
                fig2.update_layout(title=f"Benford 2D ‚Äî Obs vs Exp ({SS.get('bf2_col')}, {src_tag})", height=340)
                st_plotly(fig2)
                register_fig("Benford 2D", "Benford 2D ‚Äî Obs vs Exp", fig2, "Benford 2D check.")

            st.dataframe(var2, use_container_width=True, height=220)

            thr = SS["risk_diff_threshold"]
            maxdiff2 = float(var2["diff_pct"].abs().max()) if len(var2) > 0 else 0.0
            msg2 = "üü¢ Green"
            if maxdiff2 >= 2 * thr:
                msg2 = "üö® Red"
            elif maxdiff2 >= thr:
                msg2 = "üü° Yellow"
            sev2 = "üü¢ Green"
            if (p2 < 0.01) or (MAD2 > 0.015):
                sev2 = "üö® Red"
            elif (p2 < 0.05) or (MAD2 > 0.012):
                sev2 = "üü° Yellow"
            st.info(f"Diff% status: {msg2} ‚Ä¢ p={p2:.4f}, MAD={MAD2:.4f} ‚áí Benford severity: {sev2}")
# ==== TAB 4: TESTS (Guardrails + Insight) ====

with TAB4:
    st.subheader("üß™ Statistical Tests ‚Äî h∆∞·ªõng d·∫´n & di·ªÖn gi·∫£i")
    st.caption("Navigator g·ª£i √Ω test theo lo·∫°i d·ªØ li·ªáu; Tab n√†y ch·ªâ hi·ªÉn th·ªã output test tr·ªçng y·∫øu "
               "v√† di·ªÖn gi·∫£i g·ªçn. C√°c bi·ªÉu ƒë·ªì h√¨nh d·∫°ng v√† trend/correlation vui l√≤ng xem Tab 1/2/3.")

    # ---------- Helpers (light) ----------
    def is_numeric_series(s: pd.Series) -> bool:
        return pd.api.types.is_numeric_dtype(s)

    def is_datetime_series(s: pd.Series) -> bool:
        return pd.api.types.is_datetime64_any_dtype(s)

    def chi_square_gof_uniform(freq_df: pd.DataFrame):
        """freq_df: columns ['category','count']"""
        obs = freq_df.set_index("category")["count"]
        k = len(obs)
        if k < 2 or obs.sum() <= 0:
            return None
        exp = pd.Series([obs.sum() / k] * k, index=obs.index)
        chi2 = float(((obs - exp) ** 2 / exp).sum())
        dof = k - 1
        p = float(1 - stats.chi2.cdf(chi2, dof))
        std_resid = (obs - exp) / np.sqrt(exp)
        res_tbl = (
            pd.DataFrame({"count": obs, "expected": exp, "std_resid": std_resid})
            .sort_values("std_resid", key=lambda s: s.abs(), ascending=False)
        )
        return {"chi2": chi2, "dof": dof, "p": p, "tbl": res_tbl}

    def concentration_hhi(freq_df: pd.DataFrame) -> float:
        share = freq_df["share"].values
        return float(np.sum(share ** 2)) if len(share) > 0 else np.nan
        
@st.cache_data(ttl=1200, show_spinner=False, max_entries=64)
def time_gaps_hours(series: pd.Series) -> Optional[pd.DataFrame]:
    """T√≠nh kho·∫£ng c√°ch th·ªùi gian li√™n ti·∫øp theo gi·ªù."""
    t = pd.to_datetime(series, errors="coerce").dropna().sort_values()
    if len(t) < 3:
        return None
    gaps = (t.diff().dropna().dt.total_seconds() / 3600.0)
    return pd.DataFrame({"gap_hours": gaps})

    # ---------- Navigator ----------
    navL, navR = st.columns([2, 3])
    with navL:
        selected_col = st.selectbox("Ch·ªçn c·ªôt ƒë·ªÉ test", DF_VIEW.columns.tolist(), key="t4_col")
        s0 = DF_VIEW[selected_col]
        dtype = (
            "Datetime" if (selected_col in SS["dt_cols"] or is_datetime_like(selected_col, s0))
            else "Numeric" if is_numeric_series(s0)
            else "Categorical"
        )
        st.write(f"**Lo·∫°i d·ªØ li·ªáu nh·∫≠n di·ªán:** {dtype}")

        st.markdown("**G·ª£i √Ω test ∆∞u ti√™n**")
        if dtype == "Numeric":
            st.write("- Benford 1D/2D (n‚â•300 & >0)")
            st.write("- Normality/Outlier: Ecdf/Box/QQ (xem Tab 1)")
        elif dtype == "Categorical":
            st.write("- Top‚ÄëN + HHI")
            st.write("- Chi‚Äësquare GoF vs Uniform")
            st.write("- œá¬≤ ƒë·ªôc l·∫≠p v·ªõi bi·∫øn tr·∫°ng th√°i (n·∫øu c√≥)")
        else:
            st.write("- DOW/Hour distribution, Seasonality (xem Tab 1)")
            st.write("- Gap/Sequence test (kho·∫£ng c√°ch th·ªùi gian)")

    with navR:
        st.markdown("**ƒêi·ªÅu khi·ªÉn ch·∫°y test**")
        use_full = st.checkbox(
            "D√πng FULL dataset (n·∫øu ƒë√£ load) cho test th·ªùi gian/Benford",
            value=SS["df"] is not None, key="t4_use_full"
        )
        # Toggle theo lo·∫°i d·ªØ li·ªáu
        run_benford = st.checkbox("Benford 1D/2D (Numeric)", value=(dtype == "Numeric"), key="t4_run_benford")
        run_cgof = st.checkbox("Chi‚Äësquare GoF vs Uniform (Categorical)", value=(dtype == "Categorical"), key="t4_run_cgof")
        run_hhi = st.checkbox("Concentration HHI (Categorical)", value=(dtype == "Categorical"), key="t4_run_hhi")
        run_timegap = st.checkbox("Gap/Sequence test (Datetime)", value=(dtype == "Datetime"), key="t4_run_timegap")

        go = st.button("Ch·∫°y c√°c test ƒë√£ ch·ªçn", type="primary", key="t4_run_btn")

    # ---------- Th·ª±c thi & l∆∞u k·∫øt qu·∫£ ----------
    if "t4_results" not in SS:
        SS["t4_results"] = {}
    if go:
        out = {}
        data_src = SS["df"] if (use_full and SS["df"] is not None) else DF_VIEW

        if run_benford and dtype == "Numeric":
            ok, msg = _benford_ready(data_src[selected_col])  # d√πng helper ·ªü M·∫¢NG 6
            if not ok:
                st.warning(msg)
            else:
                out["benford"] = {
                    "r1": _benford_1d(data_src[selected_col]),
                    "r2": _benford_2d(data_src[selected_col]),
                    "col": selected_col,
                    "src": "FULL" if (use_full and SS["df"] is not None) else "SAMPLE"
                }

        if (run_cgof or run_hhi) and dtype == "Categorical":
            freq = cat_freq(s0.astype(str))
            if run_cgof:
                cg = chi_square_gof_uniform(freq)
                if cg:
                    out["cgof"] = cg
            if run_hhi:
                out["hhi"] = {"hhi": concentration_hhi(freq), "freq": freq}

        if run_timegap and dtype == "Datetime":
            gaps_df = time_gaps_hours(data_src[selected_col])
            if gaps_df is None:
                st.warning("Kh√¥ng ƒë·ªß d·ªØ li·ªáu th·ªùi gian ƒë·ªÉ t√≠nh kho·∫£ng c√°ch (c·∫ßn ‚â•3 b·∫£n ghi h·ª£p l·ªá).")
            else:
                out["gap"] = {"gaps": gaps_df, "col": selected_col,
                              "src": "FULL" if (use_full and SS["df"] is not None) else "SAMPLE"}

        SS["t4_results"] = out

    # ---------- Hi·ªÉn th·ªã k·∫øt qu·∫£ ----------
    out = SS.get("t4_results", {})
    if not out:
        st.info("Ch·ªçn c·ªôt v√† nh·∫•n **Ch·∫°y c√°c test ƒë√£ ch·ªçn** ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£.")
    else:
        # Benford song song
        if "benford" in out and out["benford"].get("r1") and out["benford"].get("r2"):
            st.markdown("#### Benford 1D & 2D (song song)")
            c1, c2 = st.columns(2)
            with c1:
                r = out["benford"]["r1"]
                tb, var, p, MAD = r["table"], r["variance"], r["p"], r["MAD"]
                if HAS_PLOTLY:
                    fig = go.Figure()
                    fig.add_trace(go.Bar(x=tb["digit"], y=tb["observed_p"], name="Observed"))
                    fig.add_trace(go.Scatter(x=tb["digit"], y=tb["expected_p"], name="Expected",
                                             mode="lines", line=dict(color="#F6AE2D")))
                    fig.update_layout(title=f"Benford 1D ‚Äî Obs vs Exp ({out['benford']['col']}, {out['benford']['src']})",
                                      height=320)
                    st_plotly(fig)
                    register_fig("Tests", "Benford 1D ‚Äî Obs vs Exp", fig, "Benford 1D (Tab 4).")
                st.dataframe(var, use_container_width=True, height=200)

                thr = SS["risk_diff_threshold"]
                maxdiff = float(var["diff_pct"].abs().max()) if len(var) > 0 else 0.0
                badge = "üü¢ Green"
                if maxdiff >= 2 * thr: badge = "üö® Red"
                elif maxdiff >= thr:   badge = "üü° Yellow"
                sev = "üü¢ Green"
                if (p < 0.01) or (MAD > 0.015): sev = "üö® Red"
                elif (p < 0.05) or (MAD > 0.012): sev = "üü° Yellow"
                st.info(f"Diff% status: {badge} ‚Ä¢ p={p:.4f}, MAD={MAD:.4f} ‚áí Benford severity: {sev}")

                st.markdown("""
- **√ù nghƒ©a**: L·ªách m·∫°nh ·ªü ch·ªØ s·ªë ƒë·∫ßu ‚Üí kh·∫£ nƒÉng thresholding/l√†m tr√≤n/chia nh·ªè h√≥a ƒë∆°n.  
- **T√°c ƒë·ªông**: R√† so√°t policy ph√™ duy·ªát theo ng∆∞·ª°ng; drill‚Äëdown theo vendor/k·ª≥.  
- **L∆∞u √Ω m·∫´u**: p nh·ªè nh∆∞ng n th·∫•p ‚Üí r·ªßi ro k·∫øt lu·∫≠n s·ªõm; tƒÉng n b·∫±ng c√°ch g·ªôp k·ª≥/nh√≥m.
                """)

            with c2:
                r2 = out["benford"]["r2"]
                tb2, var2, p2, MAD2 = r2["table"], r2["variance"], r2["p"], r2["MAD"]
                if HAS_PLOTLY:
                    fig2 = go.Figure()
                    fig2.add_trace(go.Bar(x=tb2["digit"], y=tb2["observed_p"], name="Observed"))
                    fig2.add_trace(go.Scatter(x=tb2["digit"], y=tb2["expected_p"], name="Expected",
                                              mode="lines", line=dict(color="#F6AE2D")))
                    fig2.update_layout(title=f"Benford 2D ‚Äî Obs vs Exp ({out['benford']['col']}, {out['benford']['src']})",
                                       height=320)
                    st_plotly(fig2)
                    register_fig("Tests", "Benford 2D ‚Äî Obs vs Exp", fig2, "Benford 2D (Tab 4).")
                st.dataframe(var2, use_container_width=True, height=200)

                thr = SS["risk_diff_threshold"]
                maxdiff2 = float(var2["diff_pct"].abs().max()) if len(var2) > 0 else 0.0
                badge2 = "üü¢ Green"
                if maxdiff2 >= 2 * thr: badge2 = "üö® Red"
                elif maxdiff2 >= thr:   badge2 = "üü° Yellow"
                sev2 = "üü¢ Green"
                if (p2 < 0.01) or (MAD2 > 0.015): sev2 = "üö® Red"
                elif (p2 < 0.05) or (MAD2 > 0.012): sev2 = "üü° Yellow"
                st.info(f"Diff% status: {badge2} ‚Ä¢ p={p2:.4f}, MAD={MAD2:.4f} ‚áí Benford severity: {sev2}")

                st.markdown("""
- **√ù nghƒ©a**: Hotspot ·ªü c·∫∑p 19/29/... ph·∫£n √°nh ƒë·ªãnh gi√° ‚Äú.99‚Äù ho·∫∑c c·∫•u tr√∫c gi√°.  
- **T√°c ƒë·ªông**: ƒê·ªëi chi·∫øu ch√≠nh s√°ch gi√°/nh√† cung c·∫•p; kh√¥ng m·∫∑c ƒë·ªãnh l√† gian l·∫≠n.  
- **S·ªë tr√≤n**: T·ªâ tr·ªçng .00/.50 cao ‚Üí kh·∫£ nƒÉng nh·∫≠p tay/∆∞·ªõc l∆∞·ª£ng.
                """)

        # Chi-square GoF
        if "cgof" in out and isinstance(out["cgof"], dict):
            st.markdown("#### Chi‚Äësquare GoF vs Uniform (Categorical)")
            cg = out["cgof"]
            st.write({"Chi2": round(cg["chi2"], 3), "dof": cg["dof"], "p": round(cg["p"], 4)})
            st.dataframe(cg["tbl"], use_container_width=True, height=220)
            if HAS_PLOTLY:
                figr = px.bar(cg["tbl"].reset_index().head(20), x="category", y="std_resid",
                              title="Standardized residuals (Top |resid|)",
                              color="std_resid", color_continuous_scale="RdBu")
                st_plotly(figr)
                register_fig("Tests", "œá¬≤ GoF residuals", figr, "Nh√≥m l·ªách m·∫°nh vs uniform.")
            st.markdown("""
- **√ù nghƒ©a**: Residual d∆∞∆°ng ‚Üí nhi·ªÅu h∆°n k·ª≥ v·ªçng; √¢m ‚Üí √≠t h∆°n.  
- **T√°c ƒë·ªông**: Drill‚Äëdown nh√≥m l·ªách ƒë·ªÉ ki·ªÉm tra policy/quy tr√¨nh v√† ngu·ªìn d·ªØ li·ªáu.
            """)

        # HHI
        if "hhi" in out and isinstance(out["hhi"], dict):
            st.markdown("#### Concentration HHI (Categorical)")
            st.write({"HHI": round(out["hhi"]["hhi"], 3)})
            st.dataframe(out["hhi"]["freq"].head(20), use_container_width=True, height=200)
            st.markdown("""
- **√ù nghƒ©a**: HHI cao ‚Üí t·∫≠p trung v√†i nh√≥m (vendor/GL).  
- **T√°c ƒë·ªông**: R√† so√°t r·ªßi ro ph·ª• thu·ªôc nh√† cung c·∫•p, ki·ªÉm so√°t ph√™ duy·ªát/ƒë·ªãnh gi√°.
            """)

        # Time gap
        if "gap" in out and isinstance(out["gap"], dict):
            st.markdown("#### Gap/Sequence test (Datetime)")
            gdf = out["gap"]["gaps"]
            ddesc = gdf.describe()
            if isinstance(ddesc, pd.Series):
                st.dataframe(ddesc.to_frame(name="gap_hours"), use_container_width=True, height=200)
            else:
                st.dataframe(ddesc, use_container_width=True, height=200)
            st.markdown("""
- **√ù nghƒ©a**: Kho·∫£ng tr·ªëng d√†i ho·∫∑c c·ª•m d√†y b·∫•t th∆∞·ªùng ‚Üí kh·∫£ nƒÉng b·ªè s√≥t/ch√®n nghi·ªáp v·ª•.  
- **T√°c ƒë·ªông**: So√°t log h·ªá th·ªëng, l·ªãch l√†m vi·ªác/ca tr·ª±c, ƒë·ªëi so√°t theo k·ª≥ ch·ªët.
            """)

    # Nh·∫Øc tr√°nh tr√πng l·∫∑p tr·ª±c quan
    st.info("Bi·ªÉu ƒë·ªì h√¨nh d·∫°ng (Histogram/KDE/Box/ECDF/QQ) c√≥ ·ªü Tab 1; Trend/Correlation ·ªü Tab 2; Benford chi ti·∫øt ·ªü Tab 3. "
            "Tab 4 t·∫≠p trung ch·∫°y test v√† di·ªÖn gi·∫£i k·∫øt qu·∫£.")

# ==== TAB 5: REGRESSION (Linear / Logistic) ====
with TAB5:
    st.subheader("üìò Regression (Linear / Logistic)")

    if not HAS_SK:
        st.info("C·∫ßn c√†i scikit‚Äëlearn ƒë·ªÉ ch·∫°y Regression: `pip install scikit-learn`.")
        st.stop()

    # Ch·ªçn ngu·ªìn d·ªØ li·ªáu
    use_full_reg = st.checkbox(
        "D√πng FULL dataset (n·∫øu ƒë√£ load) cho Regression",
        value=(SS["df"] is not None), key="reg_use_full"
    )
    REG_DF = DF_FULL if (use_full_reg and SS["df"] is not None) else DF_VIEW
    if REG_DF is DF_VIEW and SS["df"] is not None:
        st.caption("‚ÑπÔ∏è ƒêang d√πng SAMPLE cho Regression (t·∫Øt checkbox ƒë·ªÉ ƒë·ªïi).")

    tab_lin, tab_log = st.tabs(["Linear Regression", "Logistic Regression"])
    # ==================== LINEAR ====================
    with tab_lin:
        num_cols = SS["num_cols"]
        if len(num_cols) < 2:
            st.info("C·∫ßn ‚â•2 bi·∫øn numeric ƒë·ªÉ ch·∫°y Linear Regression.")
        else:
            c1, c2, c3 = st.columns([2, 2, 1])
            with c1:
                y_lin = st.selectbox("Target (numeric)", num_cols, key="lin_y")
            with c2:
                X_lin = st.multiselect(
                    "Features (X) - numeric",
                    options=[c for c in num_cols if c != y_lin],
                    default=[c for c in num_cols if c != y_lin][:3],
                    key="lin_X"
                )
            with c3:
                test_size = st.slider("Test size", 0.1, 0.5, 0.25, 0.05, key="lin_ts")

            optL, optR = st.columns(2)
            with optL:
                impute_na = st.checkbox("Impute NA (median)", value=True, key="lin_impute")
                drop_const = st.checkbox("Lo·∫°i c·ªôt variance=0", value=True, key="lin_drop_const")
            with optR:
                show_diag = st.checkbox("Hi·ªán ch·∫©n ƒëo√°n residuals", value=True, key="lin_diag")

            run_lin = st.button("‚ñ∂Ô∏è Run Linear Regression", key="btn_run_lin", use_container_width=True)

            if run_lin:
                try:
                    sub = REG_DF[[y_lin] + X_lin].copy()
                    # √©p numeric & x·ª≠ l√Ω NA
                    for c in [y_lin] + X_lin:
                        if not pd.api.types.is_numeric_dtype(sub[c]):
                            sub[c] = pd.to_numeric(sub[c], errors="coerce")

                    if impute_na:
                        med = sub[X_lin].median(numeric_only=True)
                        sub[X_lin] = sub[X_lin].fillna(med)
                        sub = sub.dropna(subset=[y_lin])
                    else:
                        sub = sub.dropna()

                    # Lo·∫°i c·ªôt h·∫±ng
                    removed = []
                    if drop_const:
                        nunique = sub[X_lin].nunique()
                        keep = [c for c in X_lin if nunique.get(c, 0) > 1]
                        removed = [c for c in X_lin if c not in keep]
                        X_lin = keep

                    if (len(sub) < (len(X_lin) + 5)) or (len(X_lin) == 0):
                        st.error("Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi x·ª≠ l√Ω NA/const (c·∫ßn ‚â• s·ªë features + 5).")
                    else:
                        X = sub[X_lin]
                        y = sub[y_lin]
                        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)
                        mdl = LinearRegression().fit(Xtr, ytr)
                        yhat = mdl.predict(Xte)

                        r2 = r2_score(yte, yhat)
                        adj = 1 - (1 - r2) * (len(yte) - 1) / max(len(yte) - Xte.shape[1] - 1, 1)
                        rmse = float(np.sqrt(mean_squared_error(yte, yhat)))
                        mae = float(np.mean(np.abs(yte - yhat)))

                        meta_cols = {
                            "R2": round(r2, 4),
                            "Adj_R2": round(adj, 4),
                            "RMSE": round(rmse, 4),
                            "MAE": round(mae, 4),
                            "n_test": int(len(yte)),
                            "k_features": int(Xte.shape[1]),
                        }
                        if removed:
                            meta_cols["removed_const"] = ", ".join(removed[:5]) + ("..." if len(removed) > 5 else "")
                        st.json(meta_cols)

                        # Coefficients
                        coef_df = pd.DataFrame({
                            "feature": X_lin,
                            "coef": mdl.coef_
                        }).sort_values("coef", key=lambda s: s.abs(), ascending=False)
                        st.dataframe(coef_df, use_container_width=True, height=240)
                        if HAS_PLOTLY and not coef_df.empty:
                            figc = px.bar(coef_df, x="feature", y="coef", title="Linear coefficients", color="coef",
                                          color_continuous_scale="RdBu")
                            figc.update_layout(xaxis_tickangle=45, height=360)
                            st_plotly(figc)
                            register_fig("Regression", "Linear coefficients", figc,
                                         "ƒê·ªô nh·∫°y m·ª•c ti√™u theo thay ƒë·ªïi ƒë∆°n v·ªã c·ªßa bi·∫øn (coef).")

                        if show_diag and HAS_PLOTLY:
                            resid = yte - yhat
                            g1, g2 = st.columns(2)
                            with g1:
                                fig1 = px.scatter(x=yhat, y=resid, labels={"x": "Fitted", "y": "Residuals"},
                                                  title="Residuals vs Fitted")
                                st_plotly(fig1)
                                register_fig("Regression", "Residuals vs Fitted", fig1,
                                             "Homoscedastic & mean‚Äëzero residuals mong ƒë·ª£i.")
                            with g2:
                                fig2 = px.histogram(resid, nbins=SS["bins"], title="Residuals distribution")
                                st_plotly(fig2)
                                register_fig("Regression", "Residuals histogram", fig2,
                                             "Ph√¢n ph·ªëi residuals (chu·∫©n/k·ªách).")
                            # Normality test (nh·∫π)
                            try:
                                if len(resid) > 7:
                                    p_norm = float(stats.normaltest(resid)[1])
                                    st.caption(f"Normality test (residuals) p-value: {p_norm:.4f}")
                            except Exception:
                                pass
                except Exception as e:
                    st.error(f"Linear Regression error: {e}")

    # ==================== LOGISTIC ====================
    with tab_log:
        # X√°c ƒë·ªãnh c·ªôt nh·ªã ph√¢n: bool ho·∫∑c ƒë√∫ng 2 gi√° tr·ªã kh√°c NA
        bin_candidates = []
        for c in REG_DF.columns:
            s = REG_DF[c].dropna()
            if s.nunique() == 2:
                bin_candidates.append(c)
        if len(bin_candidates) == 0:
            st.info("Kh√¥ng t√¨m th·∫•y c·ªôt nh·ªã ph√¢n (ch√≠nh x√°c 2 gi√° tr·ªã duy nh·∫•t).")
        else:
            c1, c2 = st.columns([2, 3])
            with c1:
                y_col = st.selectbox("Target (binary)", bin_candidates, key="logit_y")
                # Ch·ªçn l·ªõp d∆∞∆°ng (positive class)
                uniq = sorted(REG_DF[y_col].dropna().unique().tolist())
                pos_label = st.selectbox("Positive class", uniq, index=len(uniq)-1, key="logit_pos")
            with c2:
                # Ch·ªâ cho numeric l√†m feature (g·ªçn, b·ªÅn)
                X_cand = [c for c in REG_DF.columns if c != y_col and pd.api.types.is_numeric_dtype(REG_DF[c])]
                X_sel = st.multiselect(
                    "Features (X) - numeric only",
                    options=X_cand,
                    default=X_cand[:4],
                    key="logit_X"
                )

            optA, optB, optC = st.columns([2, 2, 1.4])
            with optA:
                impute_na_l = st.checkbox("Impute NA (median)", value=True, key="logit_impute")
                drop_const_l = st.checkbox("Lo·∫°i c·ªôt variance=0", value=True, key="logit_drop_const")
            with optB:
                class_bal = st.checkbox("Class weight = 'balanced'", value=True, key="logit_cw")
                thr = st.slider("Ng∆∞·ª°ng ph√¢n lo·∫°i (threshold)", 0.1, 0.9, 0.5, 0.05, key="logit_thr")
            with optC:
                test_size_l = st.slider("Test size", 0.1, 0.5, 0.25, 0.05, key="logit_ts")

            run_log = st.button("‚ñ∂Ô∏è Run Logistic Regression", key="btn_run_log", use_container_width=True)

            if run_log:
                try:
                    # Chu·∫©n ho√° y
                    sub = REG_DF[[y_col] + X_sel].copy()
                    # map target -> {neg:0, pos:1}
                    y_raw = sub[y_col]
                    y = (y_raw == pos_label).astype(int)

                    # √©p numeric X & x·ª≠ l√Ω NA
                    for c in X_sel:
                        if not pd.api.types.is_numeric_dtype(sub[c]):
                            sub[c] = pd.to_numeric(sub[c], errors="coerce")
                    if impute_na_l:
                        med = sub[X_sel].median(numeric_only=True)
                        sub[X_sel] = sub[X_sel].fillna(med)
                        df_ready = pd.concat([y, sub[X_sel]], axis=1).dropna()
                    else:
                        df_ready = pd.concat([y, sub[X_sel]], axis=1).dropna()

                    # Lo·∫°i c·ªôt h·∫±ng
                    removed = []
                    if drop_const_l:
                        nunique = df_ready[X_sel].nunique()
                        keep = [c for c in X_sel if nunique.get(c, 0) > 1]
                        removed = [c for c in X_sel if c not in keep]
                        X_sel = keep

                    if (len(df_ready) < (len(X_sel) + 10)) or (len(X_sel) == 0):
                        st.error("Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi x·ª≠ l√Ω NA/const (c·∫ßn ‚â• s·ªë features + 10).")
                    else:
                        X = df_ready[X_sel]
                        yb = df_ready[y_col] if y_col in df_ready.columns else y.loc[df_ready.index]
                        # train/test split (stratify)
                        Xtr, Xte, ytr, yte = train_test_split(
                            X, yb, test_size=test_size_l, random_state=42, stratify=yb
                        )
                        model = LogisticRegression(
                            max_iter=1000,
                            class_weight=("balanced" if class_bal else None),
                        ).fit(Xtr, ytr)
                        proba = model.predict_proba(Xte)[:, 1]
                        pred = (proba >= thr).astype(int)

                        # Metrics
                        acc = accuracy_score(yte, pred)
                        # Precision/Recall/F1 an to√†n v·ªõi edge cases
                        def _safe_div(a, b): return (a / b) if b else 0.0
                        tp = int(((pred == 1) & (yte == 1)).sum())
                        fp = int(((pred == 1) & (yte == 0)).sum())
                        fn = int(((pred == 0) & (yte == 1)).sum())
                        tn = int(((pred == 0) & (yte == 0)).sum())
                        prec = _safe_div(tp, (tp + fp))
                        rec = _safe_div(tp, (tp + fn))
                        f1 = _safe_div(2 * prec * rec, (prec + rec)) if (prec + rec) else 0.0
                        try:
                            auc = roc_auc_score(yte, proba)
                        except Exception:
                            auc = np.nan

                        st.json({
                            "Accuracy": round(float(acc), 4),
                            "Precision": round(float(prec), 4),
                            "Recall": round(float(rec), 4),
                            "F1": round(float(f1), 4),
                            "ROC_AUC": (round(float(auc), 4) if not np.isnan(auc) else None),
                            "n_test": int(len(yte)),
                            "threshold": float(thr),
                            "removed_const": (", ".join(removed[:5]) + ("..." if len(removed) > 5 else "")) if removed else None
                        })

                        # Confusion matrix
 # Confusion matrix (M·∫¢NG 8)
                    if HAS_PLOTLY:
                        try:
                            fcm = px.imshow(cm, text_auto=True, color_continuous_scale="Blues",
                                            labels=dict(x="Pred", y="Actual", color="Count"),
                                            x=["0", "1"], y=["0", "1"],
                                            title="Confusion Matrix")
                        except TypeError:
                            fcm = px.imshow(cm, color_continuous_scale="Blues",
                                            labels=dict(x="Pred", y="Actual", color="Count"),
                                            x=["0", "1"], y=["0", "1"],
                                            title="Confusion Matrix")
                        st_plotly(fcm)
                        register_fig("Regression", "Confusion Matrix", fcm, "Hi·ªáu qu·∫£ ph√¢n lo·∫°i t·∫°i ng∆∞·ª°ng ƒë√£ ch·ªçn.")
                    
                        # ROC curve
                        if HAS_PLOTLY and (len(np.unique(yte)) == 2):
                            try:
                                fpr, tpr, thr_arr = roc_curve(yte, proba)
                                fig = px.area(x=fpr, y=tpr, title="ROC Curve",
                                              labels={"x": "False Positive Rate", "y": "True Positive Rate"})
                                fig.add_shape(type="line", line=dict(dash="dash"), x0=0, x1=1, y0=0, y1=1)
                                st_plotly(fig)
                                register_fig("Regression", "ROC Curve", fig, "Kh·∫£ nƒÉng ph√¢n bi·ªát c·ªßa m√¥ h√¨nh.")
                            except Exception:
                                pass

                        # Coefficients & Odds ratios
                        if hasattr(model, "coef_"):
                            coefs = model.coef_[0]
                            coef_df = pd.DataFrame({
                                "feature": X_sel,
                                "coef": coefs,
                                "odds_ratio": np.exp(coefs)
                            }).sort_values("coef", key=lambda s: s.abs(), ascending=False)
                            st.dataframe(coef_df, use_container_width=True, height=240)
                            if HAS_PLOTLY and not coef_df.empty:
                                figb = px.bar(coef_df, x="feature", y="odds_ratio",
                                              title="Odds Ratio (exp(coef))", color="coef",
                                              color_continuous_scale="RdBu")
                                figb.update_layout(xaxis_tickangle=45, height=360)
                                st_plotly(figb)
                                register_fig("Regression", "Odds Ratio", figb, "T√°c ƒë·ªông ƒë·∫øn odds l·ªõp d∆∞∆°ng.")
                except Exception as e:
                    st.error(f"Logistic Regression error: {e}")
# ==== TAB 6: FRAUD FLAGS ====

with TAB6:
    st.subheader("üö© Fraud Flags")

    # --- Controls ---
    use_full_flags = st.checkbox(
        "D√πng FULL dataset (n·∫øu ƒë√£ load) cho Flags",
        value=(SS["df"] is not None),
        key="ff_use_full"
    )
    FLAG_DF = DF_FULL if (use_full_flags and SS["df"] is not None) else DF_VIEW
    if FLAG_DF is DF_VIEW and SS["df"] is not None:
        st.caption("‚ÑπÔ∏è ƒêang d√πng SAMPLE cho Fraud Flags.")

    amount_col = st.selectbox("Amount (optional)", options=["(None)"] + SS["num_cols"], key="ff_amt")
    dt_col = st.selectbox("Datetime (optional)", options=["(None)"] + SS["dt_cols"], key="ff_dt")
    group_cols = st.multiselect("Composite key ƒë·ªÉ d√≤ tr√πng (t√πy ch·ªçn)", options=FLAG_DF.columns.tolist(), key="ff_groups")

    # --- Parameters ---
    with st.expander("‚öôÔ∏è Tham s·ªë qu√©t c·ªù (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)"):
        c1, c2, c3 = st.columns(3)
        with c1:
            thr_zero = st.number_input("Ng∆∞·ª°ng Zero ratio (m·∫∑c ƒë·ªãnh 0.30)", 0.0, 1.0, 0.30, 0.05, key="ff_thr_zero")
            thr_tail99 = st.number_input("Ng∆∞·ª°ng Tail >P99 share", 0.0, 1.0, 0.02, 0.01, key="ff_thr_p99")
            thr_round = st.number_input("Ng∆∞·ª°ng .00/.50 share", 0.0, 1.0, 0.20, 0.05, key="ff_thr_round")
        with c2:
            thr_offh = st.number_input("Ng∆∞·ª°ng Off‚Äëhours share", 0.0, 1.0, 0.15, 0.05, key="ff_thr_offh")
            thr_weekend = st.number_input("Ng∆∞·ª°ng Weekend share", 0.0, 1.0, 0.25, 0.05, key="ff_thr_weekend")
            dup_min = st.number_input("S·ªë l·∫ßn tr√πng key t·ªëi thi·ªÉu (‚â•)", 2, 100, 2, 1, key="ff_dup_min")
        with c3:
            near_str = st.text_input("Near approval thresholds (vd: 1,000,000; 2,000,000)", key="ff_near_list")
            near_eps_pct = st.number_input("Bi√™n ¬±% quanh ng∆∞·ª°ng", 0.1, 10.0, 1.0, 0.1, key="ff_near_eps")
            use_daily_dups = st.checkbox("D√≤ tr√πng Amount theo ng√†y (khi c√≥ Datetime)", value=True, key="ff_dup_day")

    run_flags = st.button("üîé Scan Flags", key="ff_scan", use_container_width=True)

    # --- Flag engine ---
    def _parse_near_thresholds(txt: str) -> list[float]:
        out = []
        if not txt: return out
        for token in re.split(r"[;,]", txt):
            tok = token.strip().replace(",", "")
            if not tok:
                continue
            try:
                out.append(float(tok))
            except Exception:
                pass
        return out

    def _share_round_amounts(s: pd.Series) -> dict:
        """T·ªâ l·ªá s·ªë ti·ªÅn c√≥ ph·∫ßn th·∫≠p ph√¢n .00 ho·∫∑c .50 (sau khi chu·∫©n ho√° v·ªÅ cent)."""
        x = pd.to_numeric(s, errors="coerce").dropna()  # <-- s·ª≠a 'coerce'
        if x.empty:
            return {"p_00": np.nan, "p_50": np.nan}
        cents = (np.abs(x) * 100).round().astype("Int64") % 100
        p00 = float((cents == 0).mean())
        p50 = float((cents == 50).mean())
        return {"p_00": p00, "p_50": p50}

    def _near_threshold_share(s: pd.Series, thresholds: list[float], eps_pct: float) -> pd.DataFrame:
        x = pd.to_numeric(s, errors="coerce").dropna()  # <-- s·ª≠a 'coerce'
        if x.empty or not thresholds:
            return pd.DataFrame(columns=["threshold", "share"])
        eps = np.array(thresholds) * (eps_pct / 100.0)
        res = []
        for t, e in zip(thresholds, eps):
            if t <= 0:
                continue
            share = float(((x >= (t - e)) & (x <= (t + e))).mean())
            res.append({"threshold": t, "share": share})
        return pd.DataFrame(res)
        
    def compute_fraud_flags(
        df: pd.DataFrame,
        amount_col: str | None,
        datetime_col: str | None,
        group_id_cols: list[str],
        params: dict
    ):
        flags: list[dict] = []
        visuals: list[tuple] = []

        # --- Zero ratio cho t·∫•t c·∫£ numeric ---
        num_cols2 = df.select_dtypes(include=[np.number]).columns.tolist()
        if num_cols2:
            zr_rows = []
            for c in num_cols2:
                s = pd.to_numeric(df[c], errors="coerce")
                if len(s) == 0: 
                    continue
                zero_ratio = float((s == 0).mean())
                zr_rows.append({"column": c, "zero_ratio": round(zero_ratio, 4)})
                if zero_ratio > params["thr_zero"]:
                    flags.append({
                        "flag": "High zero ratio",
                        "column": c,
                        "threshold": params["thr_zero"],
                        "value": round(zero_ratio, 4),
                        "note": "Threshold/rounding ho·∫∑c kh√¥ng s·ª≠ d·ª•ng tr∆∞·ªùng."
                    })
            if zr_rows:
                visuals.append(("Zero ratios (numeric)", pd.DataFrame(zr_rows).sort_values("zero_ratio", ascending=False)))

        # --- Ph√¢n t√≠ch Amount ---
        amt = amount_col if (amount_col and amount_col != "(None)" and amount_col in df.columns) else None
        if amt:
            s_amt = pd.to_numeric(df[amt], errors="coerce").dropna()
            if len(s_amt) > 20:
                p95 = s_amt.quantile(0.95); p99 = s_amt.quantile(0.99)
                tail99 = float((s_amt > p99).mean())
                if tail99 > params["thr_tail99"]:
                    flags.append({
                        "flag": "Too‚Äëheavy right tail (>P99)",
                        "column": amt,
                        "threshold": params["thr_tail99"],
                        "value": round(tail99, 4),
                        "note": "Ki·ªÉm tra outliers/segmentation/cut‚Äëoff."
                    })
                visuals.append(("P95/P99 thresholds", pd.DataFrame({"metric": ["P95", "P99"], "value": [p95, p99]})))

                # .00/.50 share
                rshare = _share_round_amounts(s_amt)
                if not np.isnan(rshare["p_00"]) and rshare["p_00"] > params["thr_round"]:
                    flags.append({
                        "flag": "High .00 ending share",
                        "column": amt,
                        "threshold": params["thr_round"],
                        "value": round(rshare["p_00"], 4),
                        "note": "L√†m tr√≤n/ph√°t sinh t·ª´ nh·∫≠p tay."
                    })
                if not np.isnan(rshare["p_50"]) and rshare["p_50"] > params["thr_round"]:
                    flags.append({
                        "flag": "High .50 ending share",
                        "column": amt,
                        "threshold": params["thr_round"],
                        "value": round(rshare["p_50"], 4),
                        "note": "Pattern gi√° tr·ªã tr√≤n .50 b·∫•t th∆∞·ªùng."
                    })
                visuals.append((".00/.50 share", pd.DataFrame([rshare])))

                # Near thresholds (n·∫øu c√≥)
                thrs = _parse_near_thresholds(params.get("near_str", ""))
                if thrs:
                    near_tbl = _near_threshold_share(s_amt, thrs, params.get("near_eps_pct", 1.0))
                    if not near_tbl.empty:
                        visuals.append(("Near-approval windows", near_tbl))
                        # flag khi b·∫•t k·ª≥ share v∆∞·ª£t thr_round (t√°i d√πng ng∆∞·ª°ng tr·ª±c quan)
                        for _, row in near_tbl.iterrows():
                            if row["share"] > params["thr_round"]:
                                flags.append({
                                    "flag": "Near approval threshold cluster",
                                    "column": amt,
                                    "threshold": params["thr_round"],
                                    "value": round(float(row["share"]), 4),
                                    "note": f"C·ª•m quanh ng∆∞·ª°ng {int(row['threshold']):,} (¬±{params['near_eps_pct']}%)."
                                })

        # --- Ph√¢n t√≠ch th·ªùi gian ---
        dtc = datetime_col if (datetime_col and datetime_col != "(None)" and datetime_col in df.columns) else None
        if dtc:
            t = pd.to_datetime(df[dtc], errors="coerce")
            hour = t.dt.hour
            weekend = t.dt.dayofweek.isin([5, 6])  # Sat/Sun
            if hour.notna().any():
                off_hours = ((hour < 7) | (hour > 20)).mean()
                if float(off_hours) > params["thr_offh"]:
                    flags.append({
                        "flag": "High off‚Äëhours activity",
                        "column": dtc,
                        "threshold": params["thr_offh"],
                        "value": round(float(off_hours), 4),
                        "note": "Xem l·∫°i ph√¢n quy·ªÅn/ca tr·ª±c/t·ª± ƒë·ªông ho√°."
                    })
                if HAS_PLOTLY:
                    hcnt = hour.dropna().value_counts().sort_index()
                    fig = px.bar(x=hcnt.index, y=hcnt.values, title="Hourly distribution (0‚Äì23)",
                                 labels={"x": "Hour", "y": "Txns"})
                    st_plotly(fig)
                    register_fig("Fraud Flags", "Hourly distribution (0‚Äì23)", fig, "Ch·ªâ b√°o ho·∫°t ƒë·ªông off‚Äëhours.")
            if weekend.notna().any():
                w_share = float(weekend.mean())
                if w_share > params["thr_weekend"]:
                    flags.append({
                        "flag": "High weekend activity",
                        "column": dtc,
                        "threshold": params["thr_weekend"],
                        "value": round(w_share, 4),
                        "note": "R√† so√°t quy·ªÅn x·ª≠ l√Ω cu·ªëi tu·∫ßn/quy tr√¨nh ph√™ duy·ªát."
                    })

        # --- Tr√πng composite key ---
        if group_id_cols:
            cols = [c for c in group_id_cols if c in df.columns]
            if cols:
                ddup = (
                    df[cols]
                    .astype(object)  # tr√°nh l·ªói type khi c√≥ mix
                    .groupby(cols, dropna=False)
                    .size()
                    .reset_index(name="count")
                    .sort_values("count", ascending=False)
                )
                top_dup = ddup[ddup["count"] >= params["dup_min"]].head(50)
                if not top_dup.empty:
                    flags.append({
                        "flag": "Duplicate composite keys",
                        "column": " + ".join(cols),
                        "threshold": f">={params['dup_min']}",
                        "value": int(top_dup["count"].max()),
                        "note": "R√† so√°t tr√πng l·∫∑p/ghost entries/ghi nh·∫≠n nhi·ªÅu l·∫ßn."
                    })
                    visuals.append(("Top duplicate keys (‚â• threshold)", top_dup))

        # --- Tr√πng Amount theo ng√†y (khi c√≥ datetime & amount) ---
        if amt and dtc and params.get("use_daily_dups", True):
            tmp = pd.DataFrame({
                "amt": pd.to_numeric(df[amt], errors="coerce"),
                "t": pd.to_datetime(df[dtc], errors="coerce")
            }).dropna()
            if not tmp.empty:
                tmp["date"] = tmp["t"].dt.date
                grp_cols = (group_id_cols or [])
                agg_cols = grp_cols + ["amt", "date"]
                d2 = tmp.join(df[grp_cols]) if grp_cols else tmp.copy()
                gb = d2.groupby(agg_cols, dropna=False).size().reset_index(name="count") \
                       .sort_values("count", ascending=False)
                top_amt_dup = gb[gb["count"] >= params["dup_min"]].head(50)
                if not top_amt_dup.empty:
                    flags.append({
                        "flag": "Repeated amounts within a day",
                        "column": " + ".join(grp_cols + [amt, "date"]) if grp_cols else f"{amt} + date",
                        "threshold": f">={params['dup_min']}",
                        "value": int(top_amt_dup["count"].max()),
                        "note": "Kh·∫£ nƒÉng chia nh·ªè giao d·ªãch / ch·∫°y l·∫∑p."
                    })
                    visuals.append(("Same amount duplicates per day", top_amt_dup))

        return flags, visuals

    if run_flags:
        amt_in = None if amount_col == "(None)" else amount_col
        dt_in = None if dt_col == "(None)" else dt_col
        params = dict(
            thr_zero=thr_zero, thr_tail99=thr_tail99, thr_round=thr_round,
            thr_offh=thr_offh, thr_weekend=thr_weekend, dup_min=int(dup_min),
            near_str=near_str, near_eps_pct=near_eps_pct, use_daily_dups=use_daily_dups
        )
        flags, visuals = compute_fraud_flags(FLAG_DF, amt_in, dt_in, group_cols, params)

        if flags:
            for fl in flags:
                v = to_float(fl.get("value")); thr = to_float(fl.get("threshold"))
                alarm = "üö®" if (v is not None and thr is not None and v > thr) else "üü°"
                st.warning(f"{alarm} [{fl['flag']}] {fl['column']} ‚Ä¢ thr:{fl.get('threshold')} ‚Ä¢ "
                           f"val:{fl.get('value')} ‚Äî {fl['note']}")
        else:
            st.success("üü¢ Kh√¥ng c√≥ c·ªù ƒë√°ng ch√∫ √Ω theo tham s·ªë hi·ªán t·∫°i.")

        for title, obj in visuals:
            st.markdown(f"**{title}**")
            if isinstance(obj, pd.DataFrame):
                st.dataframe(obj, use_container_width=True, height=min(320, 40 + 24 * min(len(obj), 10)))

# ---------- TAB 7: Risk Assessment & Export (RESTORED) ----------
# ==== TAB 7: RISK ASSESSMENT & EXPORT ====

with TAB7:
    left, right = st.columns([3, 2])

    # -------------------- LEFT: RISK ASSESSMENT --------------------
    with left:
        st.subheader("üß≠ Automated Risk Assessment ‚Äî Signals ‚Üí Next tests ‚Üí Interpretation")

        # ---- Helpers n·ªôi b·ªô (c·ª•c b·ªô tab ƒë·ªÉ tr√°nh xung ƒë·ªôt t√™n) ----
        def _detect_mixed_types(ser: pd.Series, sample: int = 1000) -> bool:
            v = ser.dropna().head(sample).apply(lambda x: type(x)).unique()
            return len(v) > 1

        def _quality_report(df_in: pd.DataFrame) -> tuple[pd.DataFrame, int]:
            rep_rows = []
            for c in df_in.columns:
                s = df_in[c]
                rep_rows.append({
                    "column": c,
                    "dtype": str(s.dtype),
                    "missing_ratio": round(float(s.isna().mean()), 4),
                    "n_unique": int(s.nunique(dropna=True)),
                    "constant": bool(s.nunique(dropna=True) <= 1),
                    "mixed_types": _detect_mixed_types(s),
                })
            dupes = int(df_in.duplicated().sum())
            return pd.DataFrame(rep_rows), dupes

        def _quick_signals(df_in: pd.DataFrame, num_cols: list[str]) -> list[dict]:
            sig = []
            # duplicate rows
            _, n_dupes = _quality_report(df_in)
            if n_dupes > 0:
                sig.append({
                    "signal": "Duplicate rows",
                    "severity": "Medium",
                    "action": "ƒê·ªãnh nghƒ©a kh√≥a t·ªïng h·ª£p & walkthrough duplicates",
                    "why": "Double posting/ghost entries",
                    "followup": "N·∫øu c√≤n tr√πng theo (Vendor,Bank,Amount,Date) ‚Üí so√°t ph√™ duy·ªát & ki·ªÉm so√°t."
                })
            # per numeric column
            for c in num_cols[:20]:
                s = pd.to_numeric(df_in[c], errors="coerce").replace([np.inf, -np.inf], np.nan).dropna()
                if len(s) == 0:
                    continue
                zr = float((s == 0).mean())
                p99 = s.quantile(0.99)
                share99 = float((s > p99).mean())
                if zr > 0.30:
                    sig.append({
                        "signal": f"Zero‚Äëheavy numeric {c} ({zr:.0%})",
                        "severity": "Medium",
                        "action": "œá¬≤/Fisher theo ƒë∆°n v·ªã; review policy/thresholds",
                        "why": "Thresholding/non‚Äëusage",
                        "followup": "N·∫øu gom theo ƒë∆°n v·ªã th·∫•y t·∫≠p trung ‚Üí nghi sai c·∫•u h√¨nh."
                    })
                if share99 > 0.02:
                    sig.append({
                        "signal": f"Heavy right tail in {c} (>P99 share {share99:.1%})",
                        "severity": "High",
                        "action": "Benford 1D/2D; cut‚Äëoff near period end; outlier review",
                        "why": "Outliers/fabrication",
                        "followup": "N·∫øu Benford l·ªách + spike cu·ªëi k·ª≥ ‚Üí nghi smoothing r·ªßi ro."
                    })
            return sig

        # ---- Th·ª±c thi ƒë√°nh gi√° ----
        rep_df, _ = _quality_report(DF_VIEW)
        signals = _quick_signals(DF_FULL if SS["df"] is not None else DF_VIEW, SS["num_cols"])

        st.dataframe(
            pd.DataFrame(signals) if signals else pd.DataFrame([{"status": "No strong risk signals"}]),
            use_container_width=True, height=320
        )

        with st.expander("üìã H∆∞·ªõng d·∫´n ng·∫Øn (logic)"):
            st.markdown("""
- **Distribution & Shape**: ƒë·ªçc mean/std/quantiles/shape/tails/normality; ƒë·ªëi chi·∫øu Histogram/Box/ECDF/QQ.
- **Tail d√†y / l·ªách l·ªõn** ‚Üí **Benford 1D/2D**; n·∫øu `diff%` ‚â• ng∆∞·ª°ng ‚Üí c·∫£nh b√°o ‚Üí **drill‚Äëdown + cut‚Äëoff**.
- **Zero‚Äëheavy** ho·∫∑c ch√™nh t·ª∑ l·ªá gi·ªØa nh√≥m ‚Üí **Proportion œá¬≤ / Independence œá¬≤**.
- **Trend** (D/W/M/Q + Rolling + YoY); n·∫øu c√≥ **m√πa v·ª•/spike** ‚Üí test **cut‚Äëoff/œá¬≤ th·ªùi gian√óstatus**.
- **Quan h·ªá bi·∫øn** ‚Üí **Correlation** (Pearson/Spearman); n·∫øu d·ª± b√°o/gi·∫£i th√≠ch ‚Üí **Regression**.
""")

    # -------------------- RIGHT: EXPORT (DOCX/PDF) --------------------
    with right:
        st.subheader("üßæ Export (Plotly snapshots) ‚Äî DOCX / PDF")

        # danh s√°ch section hi·ªán c√≥ t·ª´ registry
        registry = SS.get("fig_registry", []) or []
        sections_present = sorted(list({it["section"] for it in registry}))
        if not sections_present:
            st.info("Ch∆∞a c√≥ h√¨nh n√†o ƒë∆∞·ª£c capture. V√†o c√°c tab tr∆∞·ªõc ƒë·ªÉ t·∫°o bi·ªÉu ƒë·ªì r·ªìi quay l·∫°i Export.")
        else:
            incl = st.multiselect(
                "Include sections",
                options=sections_present,
                default=[s for s in sections_present if s in {"Profiling", "Benford 1D", "Benford 2D", "Tests"}]
                        or sections_present[:3]
            )
            title = st.text_input("Report title", value="Audit Statistics ‚Äî Findings", key="exp_title")
            scale = st.slider("Export scale (DPI factor)", 1.0, 3.0, 2.0, 0.5, key="exp_scale")

            # helper save ·∫£nh plotly -> PNG (kaleido)
            def _save_plotly_png(fig, name_prefix="fig", scale_val=2.0) -> str | None:
                if not HAS_PLOTLY:
                    return None
                try:
                    # d√πng fig.to_image (c·∫ßn kaleido). N·∫øu thi·∫øu kaleido, s·∫Ω raise -> tr·∫£ None.
                    img_bytes = fig.to_image(format="png", scale=scale_val)
                    path = f"{name_prefix}_{int(time.time()*1000)}.png"
                    with open(path, "wb") as f:
                        f.write(img_bytes)
                    return path
                except Exception:
                    return None

            export_bundle = [it for it in registry if it["section"] in incl]

            def _export_docx(img_items: list[tuple[str, str, str, str]], meta: dict) -> str | None:
                if not HAS_DOCX or not img_items:
                    return None
                try:
                    doc = docx.Document()
                    doc.add_heading(meta.get("title", "Audit Statistics ‚Äî Findings"), 0)
                    doc.add_paragraph(f"File: {meta.get('file')} ‚Ä¢ SHA12={meta.get('sha12')} ‚Ä¢ Time: {meta.get('time')}")
                    cur_sec = None
                    for title_i, sec, cap, img in img_items:
                        if cur_sec != sec:
                            cur_sec = sec
                            doc.add_heading(sec, level=1)
                        doc.add_heading(title_i, level=2)
                        doc.add_picture(img, width=Inches(6.5))
                        if cap:
                            doc.add_paragraph(cap)
                    out = f"report_{int(time.time())}.docx"
                    doc.save(out)
                    return out
                except Exception:
                    return None
            def _export_pdf(img_items: list[tuple[str, str, str, str]], meta: dict) -> str | None:
                if not HAS_PDF or not img_items:
                    return None
                try:
                    doc = fitz.open()
                    page = doc.new_page()
                    y = 36
                    # Header
                    title_txt = meta.get("title", "Audit Statistics ‚Äî Findings")
                    page.insert_text((36, y), title_txt, fontsize=16); y += 22
                    page.insert_text((36, y), f"File: {meta.get('file')} ‚Ä¢ SHA12={meta.get('sha12')} ‚Ä¢ Time: {meta.get('time')}", fontsize=10); y += 18
                    cur_sec = None
                    for title_i, sec, cap, img in img_items:
                        if y > 740:
                            page = doc.new_page(); y = 36
                        if cur_sec != sec:
                            page.insert_text((36, y), sec, fontsize=13); y += 18; cur_sec = sec
                        page.insert_text((36, y), title_i, fontsize=12); y += 14
                        rect = fitz.Rect(36, y, 559, y + 300)
                        page.insert_image(rect, filename=img); y += 305
                        if cap:
                            # d√πng textbox ƒë·ªÉ wrap
                            page.insert_textbox(fitz.Rect(36, y, 559, y + 40), cap, fontsize=10)
                            y += 44
                    out = f"report_{int(time.time())}.pdf"
                    doc.save(out); doc.close()
                    return out
                except Exception:
                    return None

            if st.button("üñºÔ∏è Capture & Export DOCX/PDF", key="btn_export", use_container_width=True):
                if not export_bundle:
                    st.warning("Kh√¥ng c√≥ h√¨nh trong c√°c section ƒë√£ ch·ªçn.")
                else:
                    # ch·ª•p ·∫£nh t·∫•t c·∫£ figure
                    img_paths: list[tuple[str, str, str, str]] = []
                    for i, it in enumerate(export_bundle, 1):
                        prefix = f"{it['section']}_{i}"
                        pth = _save_plotly_png(it["fig"], name_prefix=prefix, scale_val=scale)
                        if pth:
                            img_paths.append((it["title"], it["section"], it.get("caption", ""), pth))

                    if not img_paths:
                        if not HAS_PLOTLY:
                            st.error("Plotly ch∆∞a s·∫µn s√†ng ƒë·ªÉ xu·∫•t ·∫£nh.")
                        else:
                            st.error("Kh√¥ng th·ªÉ t·∫°o ·∫£nh. H√£y ƒë·∫£m b·∫£o ƒë√£ c√†i `kaleido` cho Plotly.")
                    else:
                        meta = {
                            "title": title,
                            "file": SS.get("uploaded_name"),
                            "sha12": SS.get("sha12"),
                            "time": datetime.now().isoformat(timespec="seconds")
                        }
                        docx_path = _export_docx(img_paths, meta)
                        pdf_path = _export_pdf(img_paths, meta)

                        outs = [p for p in [docx_path, pdf_path] if p]
                        if outs:
                            st.success("Exported: " + ", ".join(outs))
                            for pth in outs:
                                with open(pth, "rb") as f:
                                    st.download_button(f"‚¨áÔ∏è Download {os.path.basename(pth)}", data=f.read(),
                                                       file_name=os.path.basename(pth))
                        else:
                            if not HAS_DOCX and not HAS_PDF:
                                st.error("C·∫ßn c√†i `python-docx` v√†/ho·∫∑c `pymupdf` (fitz) ƒë·ªÉ xu·∫•t DOCX/PDF.")
                            else:
                                st.error("Export failed. Ki·ªÉm tra l·∫°i m√¥i tr∆∞·ªùng (kaleido/docx/pymupdf).")
                        # d·ªçn ·∫£nh t·∫°m
                        for _, _, _, img in img_paths:
                            with contextlib.suppress(Exception):
                                os.remove(img)
# ==== GoF MODELS (Normal / Lognormal / Gamma) ====
@st.cache_data(ttl=1800, show_spinner=False, max_entries=64)
def gof_models(series: pd.Series):
    """
    Tr·∫£ v·ªÅ:
      - gof: DataFrame ['model','AIC'] tƒÉng d·∫ßn
      - best: model t·ªët nh·∫•t theo AIC
      - suggest: g·ª£i √Ω bi·∫øn ƒë·ªïi (log/Box-Cox) cho ph√¢n t√≠ch tham s·ªë
    """
    s = pd.to_numeric(series, errors='coerce').replace([np.inf, -np.inf], np.nan).dropna()
    if s.empty:
        return pd.DataFrame(columns=['model', 'AIC']), 'Normal', 'Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng.'

    out = []
    mu = float(s.mean())
    sigma = float(s.std(ddof=0))
    sigma = sigma if sigma > 0 else 1e-9

    # Normal
    logL_norm = float(np.sum(stats.norm.logpdf(s, loc=mu, scale=sigma)))
    AIC_norm = 2 * 2 - 2 * logL_norm
    out.append({'model': 'Normal', 'AIC': AIC_norm})

    # Ch·ªâ x√©t ph√¢n ph·ªëi d∆∞∆°ng cho Lognormal/Gamma
    s_pos = s[s > 0]
    lam = None
    if len(s_pos) >= 5:
        # Lognormal (kh√≥a loc=0 ƒë·ªÉ ·ªïn ƒë·ªãnh fit)
        try:
            shape_ln, loc_ln, scale_ln = stats.lognorm.fit(s_pos, floc=0)
            logL_ln = float(np.sum(stats.lognorm.logpdf(s_pos, shape_ln, loc=loc_ln, scale=scale_ln)))
            AIC_ln = 2 * 3 - 2 * logL_ln
            out.append({'model': 'Lognormal', 'AIC': AIC_ln})
        except Exception:
            pass

        # Gamma (kh√≥a loc=0 ƒë·ªÉ ·ªïn ƒë·ªãnh fit)
        try:
            a_g, loc_g, scale_g = stats.gamma.fit(s_pos, floc=0)
            logL_g = float(np.sum(stats.gamma.logpdf(s_pos, a_g, loc=loc_g, scale=scale_g)))
            AIC_g = 2 * 3 - 2 * logL_g
            out.append({'model': 'Gamma', 'AIC': AIC_g})
        except Exception:
            pass

        # Box-Cox Œª g·ª£i √Ω
        try:
            lam = float(stats.boxcox_normmax(s_pos))
        except Exception:
            lam = None

    gof = pd.DataFrame(out).sort_values('AIC').reset_index(drop=True)
    best = gof.iloc[0]['model'] if not gof.empty else 'Normal'
    if best == 'Lognormal':
        suggest = 'Log-transform tr∆∞·ªõc test tham s·ªë; c√¢n nh·∫Øc Median/IQR.'
    elif best == 'Gamma':
        suggest = f'Box-Cox (Œª‚âà{lam:.2f}) ho·∫∑c log-transform; sau ƒë√≥ test tham s·ªë.' if lam is not None else \
                  'Box-Cox ho·∫∑c log-transform; sau ƒë√≥ test tham s·ªë.'
    else:
        suggest = 'Kh√¥ng c·∫ßn bi·∫øn ƒë·ªïi (g·∫ßn Normal).'
    return gof, best, suggest
# ==== FOOTER: ENV SNAPSHOT (optional) ====
with st.expander("‚ÑπÔ∏è Environment snapshot (optional)"):
    st.write({
        "plotly": HAS_PLOTLY,
        "kaleido": HAS_KALEIDO,
        "python-docx": HAS_DOCX,
        "pymupdf": HAS_PDF,
        "pyarrow": HAS_PYARROW,
        "scikit-learn": HAS_SK
    })
    st.caption("N·∫øu thi·∫øu g√≥i, tham kh·∫£o: pip install plotly kaleido python-docx pymupdf pyarrow scikit-learn")

