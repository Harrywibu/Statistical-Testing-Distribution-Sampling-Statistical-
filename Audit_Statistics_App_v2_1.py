import io, os, re, json, time, warnings, contextlib
from datetime import datetime
import numpy as np
import pandas as pd
import streamlit as st
from scipy import stats
warnings.filterwarnings('ignore')

# Optional deps
HAS_PLOTLY=True
try:
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
except Exception:
    HAS_PLOTLY=False
HAS_KALEIDO=False
try:
    import plotly.io as pio
    HAS_KALEIDO=True
except Exception:
    HAS_KALEIDO=False
HAS_SM=False
try:
    from statsmodels.stats.multicomp import pairwise_tukeyhsd
    HAS_SM=True
except Exception:
    HAS_SM=False
HAS_SK=False
try:
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, roc_auc_score, roc_curve, confusion_matrix
    HAS_SK=True
except Exception:
    HAS_SK=False
HAS_DOCX=False
try:
    import docx
    from docx.shared import Inches
    HAS_DOCX=True
except Exception:
    HAS_DOCX=False
HAS_PDF=False
try:
    import fitz  # PyMuPDF
    HAS_PDF=True
except Exception:
    HAS_PDF=False

st.set_page_config(page_title='Audit Statistics ‚Äî v2.1.6 Unified FULL (PLUS)', layout='wide')

# --- Plotly safe wrapper (unique key, default width/config) ---

def st_plotly(fig, **kwargs):
    SS = st.session_state
    if '_plt_seq' not in SS:
        SS['_plt_seq'] = 0
    SS['_plt_seq'] += 1
    kwargs.setdefault('width', 'stretch')
    kwargs.setdefault('config', {'displaylogo': False})
    kwargs.setdefault('key', f"plt_{SS['_plt_seq']}")
    return st.plotly_chart(fig, **kwargs)

# ---------- Utils ----------

def file_sha12(b: bytes) -> str:
    import hashlib
    return hashlib.sha256(b).hexdigest()[:12]

def sanitize_digits(x: float) -> str:
    xs = ("%.15g" % float(x))
    return re.sub(r"[^0-9]","", xs).lstrip('0')

@st.cache_data(ttl=3600)
def read_csv_cached(b: bytes):
    try:
        return pd.read_csv(io.BytesIO(b)), None
    except UnicodeDecodeError:
        return pd.read_csv(io.BytesIO(b), encoding='cp1252'), None
    except Exception as e:
        return None, str(e)

@st.cache_data(ttl=3600)
def list_sheets_xlsx(file_bytes: bytes):
    from openpyxl import load_workbook
    wb = load_workbook(io.BytesIO(file_bytes), read_only=True, data_only=True)
    try:
        return wb.sheetnames
    finally:
        wb.close()

@st.cache_data(ttl=3600)
def get_headers_xlsx(file_bytes: bytes, sheet_name: str, header_row: int = 1, dtype_map: dict|None=None):
    return pd.read_excel(io.BytesIO(file_bytes), sheet_name=sheet_name, nrows=0,
                         header=header_row-1, dtype=dtype_map, engine='openpyxl').columns.tolist()

@st.cache_data(ttl=3600)
def read_selected_columns_xlsx(file_bytes: bytes, sheet_name: str, usecols: list[str],
                               nrows: int|None=None, header_row: int = 1, skip_top: int = 0,
                               dtype_map: dict|None=None):
    skiprows = list(range(header_row, header_row+skip_top)) if skip_top>0 else None
    return pd.read_excel(io.BytesIO(file_bytes), sheet_name=sheet_name, usecols=usecols, nrows=nrows,
                         header=header_row-1, skiprows=skiprows, dtype=dtype_map, engine='openpyxl')

# ---------- Stats helpers ----------

def cramers_v(confusion: pd.DataFrame):
    chi2 = stats.chi2_contingency(confusion)[0]
    n = confusion.values.sum(); r, k = confusion.shape
    return np.sqrt(chi2/(n*(min(r-1,k-1)))) if min(r-1,k-1)>0 else np.nan

# ---------- Benford helpers ----------

def benford_1d(series: pd.Series):
    s = pd.to_numeric(series, errors='coerce').replace([np.inf,-np.inf], np.nan).dropna().abs()
    d1 = s.apply(lambda v: int(sanitize_digits(v)[0]) if len(sanitize_digits(v))>=1 else np.nan).dropna()
    d1 = d1[(d1>=1) & (d1<=9)]
    if d1.empty: return None
    obs = d1.value_counts().sort_index().reindex(range(1,10), fill_value=0).astype(float)
    n = obs.sum(); obs_p = obs/n
    idx = np.arange(1,10); exp_p = np.log10(1 + 1/idx); exp = exp_p*n
    with np.errstate(divide='ignore', invalid='ignore'):
        chi2 = np.nansum((obs-exp)**2/exp)
    pval = 1 - stats.chi2.cdf(chi2, len(idx)-1)
    mad = float(np.mean(np.abs(obs_p-exp_p)))
    var_tbl = pd.DataFrame({'digit': idx, 'expected': exp, 'observed': obs.values})
    var_tbl['diff'] = var_tbl['observed'] - var_tbl['expected']
    var_tbl['diff_pct'] = (var_tbl['observed'] - var_tbl['expected']) / var_tbl['expected']
    table = pd.DataFrame({'digit': idx, 'observed_p': obs_p.values, 'expected_p': exp_p})
    return {'table': table, 'variance': var_tbl, 'n': int(n), 'chi2': float(chi2), 'p': float(pval), 'MAD': float(mad)}

def benford_2d(series: pd.Series):
    s = pd.to_numeric(series, errors='coerce').replace([np.inf,-np.inf], np.nan).dropna().abs()
    def f2(v):
        ds = sanitize_digits(v)
        if len(ds)>=2: return int(ds[:2])
        if len(ds)==1 and ds!='0': return int(ds)
        return np.nan
    d2 = s.apply(f2).dropna(); d2 = d2[(d2>=10) & (d2<=99)]
    if d2.empty: return None
    obs = d2.value_counts().sort_index().reindex(range(10,100), fill_value=0).astype(float)
    n = obs.sum(); obs_p = obs/n
    idx = np.arange(10,100); exp_p = np.log10(1 + 1/idx); exp = exp_p*n
    with np.errstate(divide='ignore', invalid='ignore'):
        chi2 = np.nansum((obs-exp)**2/exp)
    pval = 1 - stats.chi2.cdf(chi2, len(idx)-1)
    mad = float(np.mean(np.abs(obs_p-exp_p)))
    var_tbl = pd.DataFrame({'digit': idx, 'expected': exp, 'observed': obs.values})
    var_tbl['diff'] = var_tbl['observed'] - var_tbl['expected']
    var_tbl['diff_pct'] = (var_tbl['observed'] - var_tbl['expected']) / var_tbl['expected']
    table = pd.DataFrame({'digit': idx, 'observed_p': obs_p.values, 'expected_p': exp_p})
    return {'table': table, 'variance': var_tbl, 'n': int(n), 'chi2': float(chi2), 'p': float(pval), 'MAD': float(mad)}

# ---------- GoF (Normal/Lognormal/Gamma) ----------

def gof_models(s: pd.Series):
    s = pd.Series(s).dropna()
    out = []
    # Normal
    mu, sigma = float(np.mean(s)), float(np.std(s, ddof=0))
    logL_norm = float(np.sum(stats.norm.logpdf(s, loc=mu, scale=sigma if sigma>0 else 1e-9)))
    AIC_norm = 2*2 - 2*logL_norm
    out.append({'model':'Normal','AIC':AIC_norm})
    # Positive-only for Lognormal/Gamma
    s_pos = s[s>0]
    lam = None
    if len(s_pos)>=5:
        try:
            shape_ln, loc_ln, scale_ln = stats.lognorm.fit(s_pos)
            logL_ln = float(np.sum(stats.lognorm.logpdf(s_pos, shape_ln, loc=loc_ln, scale=scale_ln)))
            AIC_ln = 2*3 - 2*logL_ln
            out.append({'model':'Lognormal','AIC':AIC_ln})
        except Exception:
            pass
        try:
            a_g, loc_g, scale_g = stats.gamma.fit(s_pos)
            logL_g = float(np.sum(stats.gamma.logpdf(s_pos, a_g, loc=loc_g, scale=scale_g)))
            AIC_g = 2*3 - 2*logL_g
            out.append({'model':'Gamma','AIC':AIC_g})
        except Exception:
            pass
        try:
            lam = float(stats.boxcox_normmax(s_pos))
        except Exception:
            lam = None
    gof = pd.DataFrame(out).sort_values('AIC').reset_index(drop=True)
    best = gof.iloc[0]['model'] if not gof.empty else 'Normal'
    if best=='Lognormal':
        suggest = 'Log-transform tr∆∞·ªõc test tham s·ªë; ho·∫∑c ph√¢n t√≠ch Median/IQR.'
    elif best=='Gamma':
        suggest = f'Box-Cox (Œª‚âà{lam:.2f}) ho·∫∑c log-transform; sau ƒë√≥ test tham s·ªë.' if lam is not None else 'Box-Cox ho·∫∑c log-transform; sau ƒë√≥ test tham s·ªë.'
    else:
        suggest = 'Kh√¥ng c·∫ßn bi·∫øn ƒë·ªïi (g·∫ßn Normal).'
    return gof, best, suggest

# ---------- App State ----------
SS = st.session_state
if 'fig_registry' not in SS: SS['fig_registry'] = []
for k,v in {
    'df': None, 'df_preview': None, 'file_bytes': None, 'sha12': None, 'uploaded_name': None,
    'xlsx_sheet': None, 'header_row': 1, 'skip_top': 0, 'dtype_choice': '', 'pv_n': 100,
    'bins': 50, 'log_scale': False, 'kde_threshold': 50000,
    'risk_diff_threshold': 0.05,
    'advanced_visuals': True
}.items():
    if k not in SS: SS[k] = v

# ---------- Sidebar ----------
st.sidebar.title('Workflow')
with st.sidebar.expander('0) Ingest', expanded=True):
    uploaded = st.file_uploader('Upload CSV/XLSX', type=['csv','xlsx'])
    if uploaded is not None:
        pos = uploaded.tell(); uploaded.seek(0); fb = uploaded.read(); uploaded.seek(pos)
        SS['file_bytes'] = fb; SS['sha12'] = file_sha12(fb); SS['uploaded_name'] = uploaded.name
    st.caption('SHA12: ' + (SS['sha12'] or '‚Äî'))
with st.sidebar.expander('Plot & Performance', expanded=True):
    SS['bins'] = st.slider('Histogram bins', 10, 200, SS['bins'], 5)
    SS['log_scale'] = st.checkbox('Log scale (X)', SS['log_scale'])
    SS['kde_threshold'] = st.number_input('KDE max n', 1000, 500000, SS['kde_threshold'], 1000)
    SS['risk_diff_threshold'] = st.slider('Risk threshold ‚Äî |diff%| Benford', 0.01, 0.10, SS['risk_diff_threshold'], 0.01)
    SS['advanced_visuals'] = st.checkbox('Advanced visuals (Violin, Lorenz/Gini)', SS['advanced_visuals'])
    downsample = st.checkbox('Downsample view 50k', value=True)
    if st.button('üßπ Clear cache'): st.cache_data.clear(); st.toast('Cache cleared', icon='üßπ')

st.title('üìä Audit Statistics ‚Äî v2.1.6 Unified FULL (PLUS)')

# ---------- Ingest ----------
if SS['file_bytes'] is None:
    st.info('Upload a file to start.'); st.stop()

fname = SS['uploaded_name']; fb = SS['file_bytes']
colL, colR = st.columns([3,2])
with colL: st.text_input('File', value=fname or '', disabled=True)
with colR:
    SS['pv_n'] = st.slider('Preview rows', 100, 500, SS['pv_n'], 50); preview_click = st.button('üîç Quick preview')

if fname.lower().endswith('.csv'):
    if preview_click or SS['df_preview'] is None:
        df_prev, err = read_csv_cached(fb)
        if err: st.error(f'Cannot read CSV: {err}'); st.stop()
        SS['df_preview'] = df_prev.head(SS['pv_n'])
        if 'selected_default' not in SS or SS['df_preview'] is None:
            SS['selected_default'] = list(SS['df_preview'].columns)
    st.dataframe(SS['df_preview'], width='stretch', height=260)
    selected = st.multiselect('Columns to load', list(SS['df_preview'].columns), SS.get('selected_default', list(SS['df_preview'].columns)))
    if st.button('üì• Load full CSV with selected columns'):
        SS['df'] = pd.read_csv(io.BytesIO(fb), usecols=(selected or None))
        st.success(f"Loaded: {len(SS['df']):,} rows √ó {len(SS['df'].columns)} cols ‚Ä¢ SHA12={SS['sha12']}")
else:
    sheets = list_sheets_xlsx(fb)
    with st.expander('üìÅ Select sheet & header (XLSX)', expanded=True):
        c1,c2,c3 = st.columns([2,1,1])
        SS['xlsx_sheet'] = c1.selectbox('Sheet', sheets, index=0 if sheets else 0)
        SS['header_row'] = c2.number_input('Header row (1‚Äëbased)', 1, 100, SS['header_row'])
        SS['skip_top'] = c3.number_input('Skip N rows after header', 0, 1000, SS['skip_top'])
        SS['dtype_choice'] = st.text_area('dtype mapping (JSON, optional)', SS.get('dtype_choice',''), height=60)
        dtype_map = None
        if SS['dtype_choice'].strip():
            with contextlib.suppress(Exception): dtype_map = json.loads(SS['dtype_choice'])
        headers = get_headers_xlsx(fb, SS['xlsx_sheet'], SS['header_row'], dtype_map)
        st.caption(f'Columns: {len(headers)} | SHA12={SS["sha12"]}')
        q = st.text_input('üîé Filter columns', SS.get('col_filter',''))
        filtered = [h for h in headers if q.lower() in h.lower()] if q else headers
        selected = st.multiselect('üßÆ Columns to load', filtered if filtered else headers, default=filtered if filtered else headers)
        if st.button('üì• Load full data'):
            SS['df'] = read_selected_columns_xlsx(fb, SS['xlsx_sheet'], selected, nrows=None, header_row=SS['header_row'], skip_top=SS['skip_top'], dtype_map=dtype_map)
            st.success(f"Loaded: {len(SS['df']):,} rows √ó {len(SS['df'].columns)} cols ‚Ä¢ SHA12={SS['sha12']}")

if SS['df'] is None and SS['df_preview'] is None:
    st.stop()

df = SS['df'] if SS['df'] is not None else SS['df_preview'].copy()
if downsample and len(df)>50000:
    df = df.sample(50000, random_state=42)
    st.caption('Downsampled view to 50k rows (visuals & stats reflect this sample).')

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()

# ---- Outlier/normality sensitivity (for Spearman auto) ----
def is_outlier_sensitive_numeric(df, cols):
    try_cols = [c for c in cols if c in df.columns]
    for c in try_cols[:20]:
        s = pd.to_numeric(df[c], errors='coerce').replace([np.inf,-np.inf], np.nan).dropna()
        if len(s)<10: continue
        try:
            skew = float(stats.skew(s)) if len(s)>2 else 0.0
            kurt = float(stats.kurtosis(s, fisher=True)) if len(s)>3 else 0.0
        except Exception:
            skew = kurt = 0.0
        p99 = s.quantile(0.99)
        tail = float((s>p99).mean())
        try:
            p_norm = float(stats.normaltest(s)[1]) if len(s)>7 else 1.0
        except Exception:
            p_norm = 1.0
        if (abs(skew)>1) or (abs(kurt)>3) or (tail>0.02) or (p_norm<0.05):
            return True
    return False

spearman_recommended = is_outlier_sensitive_numeric(df, num_cols)

# Tabs
TAB1, TAB2, TAB3, TAB4, TAB5, TAB6, TAB7 = st.tabs([
    '1) Distribution & Shape', '2) Trend & Correlation', '3) Benford 1D/2D', '4) Tests', '5) Regression', '6) Fraud Flags', '7) Risk & Export'
])

# Helper to register fig

def register_fig(section, title, fig, caption):
    SS['fig_registry'].append({'section':section, 'title':title, 'fig':fig, 'caption':caption})

# ---------- TAB 1: Distribution & Shape (Full Profiling) ----------
with TAB1:
    st.subheader('üìà Distribution & Shape ‚Äî Full Profiling by Type')

    sub_t1, sub_t2, sub_t3 = st.tabs(['Numeric','Categorical','Datetime'])

    # --- Numeric ---
    with sub_t1:
        if not num_cols:
            st.info('No numeric columns detected.')
        else:
            c1,c2 = st.columns(2)
            with c1:
                num_col = st.selectbox('Numeric column', num_cols, key='ds_num')
            with c2:
                show_violin = st.checkbox('Show Violin & Lorenz (advanced)', value=True)
            s0 = pd.to_numeric(df[num_col], errors='coerce').replace([np.inf,-np.inf], np.nan)
            s = s0.dropna(); n_na = int(s0.isna().sum())
            if len(s)==0:
                st.warning('No numeric values after cleaning.')
            else:
                desc = s.describe(percentiles=[0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99])
                mean = float(desc['mean']); median = float(desc['50%'])
                try:
                    modes = s.mode(dropna=True).astype(float).tolist()
                except Exception:
                    modes = []
                std = float(desc['std']) if not np.isnan(desc['std']) else np.nan
                skew = float(stats.skew(s)) if len(s)>2 else np.nan
                kurt = float(stats.kurtosis(s, fisher=True)) if len(s)>3 else np.nan
                try: p_norm = float(stats.normaltest(s)[1]) if len(s)>7 else np.nan
                except Exception: p_norm = np.nan
                p95,p99 = s.quantile(0.95), s.quantile(0.99)
                se = float(s.std(ddof=1)/np.sqrt(len(s))) if len(s)>1 else np.nan
                ci_l = float(mean - 1.96*se) if not np.isnan(se) else np.nan
                ci_u = float(mean + 1.96*se) if not np.isnan(se) else np.nan

                stat_df = pd.DataFrame([{
                    'count': int(desc['count']), 'n_missing': n_na,
                    'mean': mean, 'std': std, 'min': float(desc['min']),
                    'p1': float(desc['1%']), 'p5': float(desc['5%']), 'p10': float(desc['10%']),
                    'q1': float(desc['25%']), 'median': median, 'q3': float(desc['75%']),
                    'p90': float(desc['90%']), 'p95': float(desc['95%']), 'p99': float(desc['99%']), 'max': float(desc['max']),
                    'skew': skew, 'kurtosis': kurt,
                    'zero_ratio': float((s==0).mean()), 'tail>p95': float((s>p95).mean()), 'tail>p99': float((s>p99).mean()),
                    'normality_p': (round(p_norm,4) if not np.isnan(p_norm) else None)
                }])
                st.dataframe(stat_df, width='stretch', height=230)

                # Visuals + captions
                if HAS_PLOTLY:
                    gA,gB = st.columns(2)
                    with gA:
                        fig1 = go.Figure(); fig1.add_trace(go.Histogram(x=s, nbinsx=SS['bins'], name='Histogram', opacity=0.75))
                        if len(s)<=SS['kde_threshold'] and len(s)>10:
                            try:
                                from scipy.stats import gaussian_kde
                                kde = gaussian_kde(s); xs = np.linspace(s.min(), s.max(), 256); ys = kde(xs)
                                ys_scaled = ys * len(s) * (xs[1]-xs[0])
                                fig1.add_trace(go.Scatter(x=xs, y=ys_scaled, name='KDE', line=dict(color='#E4572E')))
                            except Exception: pass
                        if SS['log_scale']: fig1.update_xaxes(type='log')
                        fig1.update_layout(title=f'{num_col} ‚Äî Histogram+KDE', height=320)
                        st_plotly(fig1); register_fig('Distribution', f'{num_col} ‚Äî Histogram+KDE', fig1, 'H√¨nh d·∫°ng ph√¢n ph·ªëi & ƒëu√¥i; KDE l√†m m∆∞·ª£t m·∫≠t ƒë·ªô.')
                        st.caption('**√ù nghƒ©a**: Nh√¨n shape, l·ªách, ƒëa ƒë·ªânh; KDE gi√∫p ph√°t hi·ªán modal/ƒëu√¥i n·∫∑ng.')
                    with gB:
                        fig2 = px.box(pd.DataFrame({num_col:s}), x=num_col, points='outliers', title=f'{num_col} ‚Äî Box')
                        st_plotly(fig2); register_fig('Distribution', f'{num_col} ‚Äî Box', fig2, 'Trung v·ªã/IQR; outliers.')
                        st.caption('**√ù nghƒ©a**: X√°c ƒë·ªãnh trung v·ªã & IQR; ƒëi·ªÉm b·∫≠t ra l√† ·ª©ng vi√™n ngo·∫°i l·ªá.')
                gC,gD = st.columns(2)
                with gC:
                    try:
                        fig3 = px.ecdf(s, title=f'{num_col} ‚Äî ECDF')
                        st_plotly(fig3); register_fig('Distribution', f'{num_col} ‚Äî ECDF', fig3, 'P(X‚â§x) t√≠ch lu·ªπ.')
                        st.caption('**√ù nghƒ©a**: H·ªó tr·ª£ ƒë·∫∑t ng∆∞·ª°ng/policy (v√≠ d·ª• limit, cut‚Äëoff).')
                    except Exception:
                        st.caption('ECDF requires plotly>=5.9.')
                with gD:
                    try:
                        osm, osr = stats.probplot(s, dist='norm', fit=False)
                        xq=np.array(osm[0]); yq=np.array(osm[1])
                        fig4=go.Figure(); fig4.add_trace(go.Scatter(x=xq,y=yq,mode='markers'))
                        lim=[min(xq.min(),yq.min()), max(xq.max(),yq.max())]; fig4.add_trace(go.Scatter(x=lim,y=lim,mode='lines',line=dict(dash='dash')))
                        fig4.update_layout(title=f'{num_col} ‚Äî QQ Normal', height=320)
                        st_plotly(fig4); register_fig('Distribution', f'{num_col} ‚Äî QQ Normal', fig4, 'ƒê·ªô l·ªách so v·ªõi normal.')
                        st.caption('**√ù nghƒ©a**: L·ªách xa ƒë∆∞·ªùng 45¬∞ ‚Üí d·ªØ li·ªáu kh√¥ng chu·∫©n ‚Üí c√¢n nh·∫Øc log/Box‚ÄëCox ho·∫∑c non‚Äëparametric.')
                    except Exception:
                        st.caption('SciPy required for QQ.')
                if show_violin and HAS_PLOTLY:
                    gE,gF = st.columns(2)
                    with gE:
                        figv = px.violin(pd.DataFrame({num_col:s}), x=num_col, points='outliers', box=True, title=f'{num_col} ‚Äî Violin')
                        st_plotly(figv); register_fig('Distribution', f'{num_col} ‚Äî Violin', figv, 'M·∫≠t ƒë·ªô + Box overlay.')
                        st.caption('**√ù nghƒ©a**: Hi·ªÉn th·ªã m·∫≠t ƒë·ªô & v·ªã tr√≠ trung t√¢m/ph√¢n t√°n r√µ r√†ng.')
                    with gF:
                        v = np.sort(s.values); cum = np.cumsum(v); lor = np.insert(cum,0,0)/cum.sum(); x = np.linspace(0,1,len(lor))
                        gini = 1 - 2*np.trapz(lor, dx=1/len(v)) if len(v)>0 else np.nan
                        figL = go.Figure(); figL.add_trace(go.Scatter(x=x,y=lor, name='Lorenz', mode='lines'))
                        figL.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Equality', line=dict(dash='dash')))
                        figL.update_layout(title=f'{num_col} ‚Äî Lorenz (Gini={gini:.3f})', height=320)
                        st_plotly(figL); register_fig('Distribution', f'{num_col} ‚Äî Lorenz', figL, 'T·∫≠p trung gi√° tr·ªã.')
                        st.caption('**√ù nghƒ©a**: ƒê·ªô cong l·ªõn ‚Üí gi√° tr·ªã t·∫≠p trung v√†o √≠t quan s√°t.')

                # GoF + AIC + suggestion
                st.markdown('### üìê Goodness‚Äëof‚ÄëFit (Normal / Lognormal / Gamma) ‚Äî AIC')
                gof, best, suggest = gof_models(s)
                st.dataframe(gof, width='stretch', height=160)
                st.info(f'**Best fit:** {best}. **Suggested transform:** {suggest}')

                # Recommendations (numeric)
                st.markdown('### üß≠ Recommended tests (Numeric)')
                recs = []
                if float((s>p99).mean())>0.02: recs.append('Benford 1D/2D; cut‚Äëoff cu·ªëi k·ª≥; outlier review.')
                if float((s==0).mean())>0.30: recs.append('Zero‚Äëheavy ‚Üí Proportion œá¬≤/Fisher theo nh√≥m (ƒë∆°n v·ªã/chi nh√°nh).')
                if (not np.isnan(skew) and abs(skew)>1) or (not np.isnan(kurt) and abs(kurt)>3) or (not np.isnan(p_norm) and p_norm<0.05):
                    recs.append('Non‚Äëparametric (Mann‚ÄìWhitney/Kruskal‚ÄìWallis) ho·∫∑c transform r·ªìi ANOVA/t‚Äëtest.')
                if len(num_cols)>=2: recs.append('Correlation (∆∞u ti√™n Spearman n·∫øu outlier/non‚Äënormal).')
                st.write('\n'.join([f'- {x}' for x in recs]) if recs else '- Kh√¥ng c√≥ ƒë·ªÅ xu·∫•t ƒë·∫∑c bi·ªát.')

                # Quick Runner
                with st.expander('‚ö° Quick Runner (Benford / ANOVA / Correlation)'):
                    qtype = st.selectbox('Choose test', ['Benford 1D','Benford 2D','ANOVA (Group means)','Correlation (Pearson/Spearman)'])
                    if qtype.startswith('Benford'):
                        if st.button('Run now', key='qr_ben'):
                            if '1D' in qtype:
                                r = benford_1d(s)
                            else:
                                r = benford_2d(s)
                            if not r:
                                st.error('Kh√¥ng th·ªÉ tr√≠ch ch·ªØ s·ªë ƒë·∫ßu/2 ch·ªØ s·ªë ƒë·∫ßu.')
                            else:
                                tb, var = r['table'], r['variance']
                                if HAS_PLOTLY:
                                    fig = go.Figure(); fig.add_trace(go.Bar(x=tb['digit'], y=tb['observed_p'], name='Observed'))
                                    fig.add_trace(go.Scatter(x=tb['digit'], y=tb['expected_p'], name='Expected', mode='lines', line=dict(color='#F6AE2D')))
                                    fig.update_layout(title=qtype + ' ‚Äî Obs vs Exp', height=340)
                                    st_plotly(fig); register_fig('Benford Quick', qtype + ' ‚Äî Obs vs Exp', fig, 'Quick Benford check.')
                                st.dataframe(var, width='stretch', height=220)
                    elif qtype.startswith('ANOVA'):
                        if len(cat_cols)==0:
                            st.warning('C·∫ßn m·ªôt bi·∫øn nh√≥m (categorical).')
                        else:
                            grp = st.selectbox('Grouping factor (categorical)', cat_cols)
                            if st.button('Run now', key='qr_anova'):
                                if grp not in df.columns or num_col not in df.columns:
                                    st.error('C·ªôt ƒë√£ ch·ªçn kh√¥ng c√≤n t·ªìn t·∫°i.');
                                else:
                                    sub = df[[num_col, grp]].dropna()
                                    if sub[grp].nunique()<2: st.error('C·∫ßn ‚â•2 nh√≥m.')
                                    else:
                                        groups = [d[num_col].values for _,d in sub.groupby(grp)]
                                        _, p_lev = stats.levene(*groups, center='median'); F, p = stats.f_oneway(*groups)
                                        if HAS_PLOTLY:
                                            fig = px.box(sub, x=grp, y=num_col, color=grp, title=f'{num_col} by {grp}')
                                            st_plotly(fig); register_fig('Tests', f'{num_col} by {grp} (Quick ANOVA)', fig, 'Group mean.')
                                        st.json({'ANOVA F': float(F), 'p': float(p), 'Levene p': float(p_lev)})
                    else:  # Correlation
                        others = [c for c in num_cols if c!=num_col]
                        if not others:
                            st.warning('C·∫ßn th√™m m·ªôt bi·∫øn numeric kh√°c.')
                        else:
                            y2 = st.selectbox('Other numeric', others)
                            method = st.radio('Method', ['Pearson','Spearman'], index=(1 if spearman_recommended else 0), horizontal=True)
                            if st.button('Run now', key='qr_corr'):
                                if y2 not in df.columns or num_col not in df.columns:
                                    st.error('C·ªôt ƒë√£ ch·ªçn kh√¥ng c√≤n t·ªìn t·∫°i.')
                                else:
                                    sub = df[[num_col, y2]].dropna()
                                    if len(sub)<3: st.error('Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi drop NA.')
                                    else:
                                        if method=='Pearson':
                                            r, pv = stats.pearsonr(sub[num_col], sub[y2])
                                        else:
                                            r, pv = stats.spearmanr(sub[num_col], sub[y2])
                                        if HAS_PLOTLY:
                                            fig = px.scatter(sub, x=num_col, y=y2, trendline=('ols' if method=='Pearson' else None), title=f'{num_col} vs {y2} ({method})')
                                            st_plotly(fig); register_fig('Tests', f'{num_col} vs {y2} (Quick Corr {method})', fig, 'Linear/rank association.')
                                        st.json({'method': method, 'r': float(r), 'p': float(pv)})

    # --- Categorical ---
    with sub_t2:
        if not cat_cols:
            st.info('No categorical columns detected.')
        else:
            cat_col = st.selectbox('Categorical column', cat_cols, key='ds_cat')
            vc = df[cat_col].astype(str).value_counts(dropna=True)
            df_freq = pd.DataFrame({'category': vc.index, 'count': vc.values})
            df_freq['share'] = df_freq['count']/df_freq['count'].sum()
            mode_cat = df_freq.iloc[0]['category'] if not df_freq.empty else None
            stats_cat = pd.DataFrame([{
                'count': int(df[cat_col].shape[0]), 'n_missing': int(df[cat_col].isna().sum()),
                'n_unique': int(df[cat_col].nunique(dropna=True)), 'mode': mode_cat
            }])
            st.dataframe(stats_cat, width='stretch', height=120)
            topn = st.number_input('Top‚ÄëN (Pareto)', 3, 50, 15)
            st.dataframe(df_freq.head(int(topn)), width='stretch', height=260)
            if HAS_PLOTLY:
                d = df_freq.head(int(topn)).copy(); d['cum_share'] = d['count'].cumsum()/d['count'].sum()
                figp = make_subplots(specs=[[{"secondary_y": True}]])
                figp.add_trace(go.Bar(x=d['category'], y=d['count'], name='Count'))
                figp.add_trace(go.Scatter(x=d['category'], y=d['cum_share']*100, name='Cumulative %', mode='lines+markers'), secondary_y=True)
                figp.update_yaxes(title_text='Count', secondary_y=False)
                figp.update_yaxes(title_text='Cumulative %', range=[0,100], secondary_y=True)
                figp.update_layout(title=f'{cat_col} ‚Äî Pareto (Top {int(topn)})', height=360)
                st_plotly(figp); register_fig('Distribution', f'{cat_col} ‚Äî Pareto Top{int(topn)}', figp, 'Pareto 80/20.')
                st.caption('**√ù nghƒ©a**: Nh·∫≠n di·ªán nh√≥m tr·ªçng y·∫øu (√≠t nh√≥m chi·∫øm ƒëa s·ªë t·∫ßn su·∫•t).')
            with st.expander('üî¨ Chi‚Äësquare Goodness‚Äëof‚ÄëFit vs Uniform (tu·ª≥ ch·ªçn)'):
                if st.checkbox('Ch·∫°y œá¬≤ GoF vs Uniform', value=False):
                    obs = df_freq.set_index('category')['count']
                    k = len(obs)
                    exp = pd.Series([obs.sum()/k]*k, index=obs.index)
                    chi2 = float(((obs-exp)**2/exp).sum()); dof = k-1; p = float(1 - stats.chi2.cdf(chi2, dof))
                    std_resid = (obs-exp)/np.sqrt(exp)
                    res_tbl = pd.DataFrame({'count': obs, 'expected': exp, 'std_resid': std_resid}).sort_values('std_resid', key=lambda s: s.abs(), ascending=False)
                    st.write({'Chi2': round(chi2,3), 'dof': dof, 'p': round(p,4)})
                    st.dataframe(res_tbl, width='stretch', height=260)
                    if HAS_PLOTLY:
                        figr = px.bar(res_tbl.reset_index().head(20), x='category', y='std_resid', title='Standardized residuals (Top |resid|)', color='std_resid', color_continuous_scale='RdBu')
                        st_plotly(figr); register_fig('Distribution', f'{cat_col} ‚Äî œá¬≤ GoF residuals', figr, 'Nh√≥m l·ªách m·∫°nh vs uniform.')
                    st.caption('**√ù nghƒ©a**: Residual d∆∞∆°ng ‚Üí nhi·ªÅu h∆°n k·ª≥ v·ªçng; √¢m ‚Üí √≠t h∆°n. G·ª£i √Ω drill‚Äëdown nh√≥m b·∫•t th∆∞·ªùng.')

    # --- Datetime ---
    with sub_t3:
        dt_candidates = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c]) or re.search(r"(date|time)", str(c), re.IGNORECASE)]
        if not dt_candidates:
            st.info('No datetime-like columns detected (name contains date/time or dtype is datetime).')
        else:
            dt_col = st.selectbox('Datetime column', dt_candidates, key='ds_dt')
            t = pd.to_datetime(df[dt_col], errors='coerce')
            n_missing = int(t.isna().sum()); t_clean = t.dropna()
            meta = pd.DataFrame([{ 'count': int(len(t)), 'n_missing': n_missing,
                                   'min': (t_clean.min() if not t_clean.empty else None),
                                   'max': (t_clean.max() if not t_clean.empty else None),
                                   'span_days': (int((t_clean.max()-t_clean.min()).days) if len(t_clean)>1 else None),
                                   'n_unique_dates': int(t_clean.dt.date.nunique()) if not t_clean.empty else 0 }])
            st.dataframe(meta, width='stretch', height=120)
            if HAS_PLOTLY and not t_clean.empty:
                # DOW
                dow = t_clean.dt.dayofweek; dow_share = dow.value_counts(normalize=True).sort_index()
                figD = px.bar(x=["Mon","Tue","Wed","Thu","Fri","Sat","Sun"], y=dow_share.reindex(range(7), fill_value=0).values,
                              title='DOW distribution', labels={'x':'DOW','y':'Share'})
                st_plotly(figD); register_fig('Distribution', 'DOW distribution', figD, 'Ph√¢n b·ªë theo th·ª© trong tu·∫ßn.')
                # Hour (if has time)
                if not t_clean.dt.hour.isna().all():
                    hour = t_clean.dt.hour
                    hcnt = hour.value_counts().sort_index()
                    figH = px.bar(x=hcnt.index, y=hcnt.values, title='Hourly histogram (0‚Äì23)', labels={'x':'Hour','y':'Count'})
                    st_plotly(figH); register_fig('Distribution', 'Hourly histogram (0‚Äì23)', figH, 'M·∫´u ho·∫°t ƒë·ªông theo gi·ªù.')
                # Quick Trend in Distribution module
                st.markdown('**Quick Trend (aggregate by time)**')
                num_for_trend = st.selectbox('Numeric to aggregate', ['(None)'] + num_cols, key='ds_num_trend')
                if num_for_trend and num_for_trend!='(None)':
                    freq = st.selectbox('Aggregate frequency', ['D','W','M','Q'], index=2, key='ds_freq')
                    win = st.slider('Rolling window (periods)', 2, 24, 3, key='ds_win')
                    y = pd.to_numeric(df[num_for_trend], errors='coerce')
                    sub = pd.DataFrame({'t':t, 'y':y}).dropna()
                    if not sub.empty and HAS_PLOTLY:
                        ts = sub.set_index('t')['y'].resample(freq).sum().to_frame('y')
                        ts['roll'] = ts['y'].rolling(win, min_periods=1).mean()
                        figt = go.Figure(); figt.add_trace(go.Scatter(x=ts.index, y=ts['y'], name='Aggregated'))
                        figt.add_trace(go.Scatter(x=ts.index, y=ts['roll'], name=f'Rolling{win}', line=dict(dash='dash')))
                        figt.update_layout(title=f'{num_for_trend} ‚Äî Quick Trend ({freq})', height=340)
                        st_plotly(figt); register_fig('Distribution', f'{num_for_trend} ‚Äî Quick Trend ({freq})', figt, 'Chu·ªói th·ªùi gian + rolling mean.')
                        st.caption('**√ù nghƒ©a**: Theo d√µi bi·∫øn ƒë·ªông; spike cu·ªëi k·ª≥ ‚Üí test cut‚Äëoff.')

# ---------- TAB 2: Trend & Correlation ----------
with TAB2:
    st.subheader('üìä Trend & üîó Correlation')
    dt_candidates = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c]) or re.search(r"(date|time)", str(c), re.IGNORECASE)]
    cA, cB = st.columns(2)
    with cA:
        num_col2 = st.selectbox('Numeric (trend)', num_cols or df.columns.tolist(), key='t2_num')
        dt_col2 = st.selectbox('Datetime column', dt_candidates or df.columns.tolist(), key='t2_dt')
        freq = st.selectbox('Aggregate frequency', ['D','W','M','Q'], index=2)
        win = st.slider('Rolling window (periods)', 2, 24, 3)
        if HAS_PLOTLY and (dt_col2 in df.columns) and (num_col2 in df.columns):
            t = pd.to_datetime(df[dt_col2], errors='coerce'); y = pd.to_numeric(df[num_col2], errors='coerce')
            sub = pd.DataFrame({'t':t, 'y':y}).dropna()
            if not sub.empty:
                ts = sub.set_index('t')['y'].resample(freq).sum().to_frame('y')
                ts['roll'] = ts['y'].rolling(win, min_periods=1).mean()
                ts['yoy'] = ts['y'].pct_change(12 if freq=='M' else (4 if freq=='Q' else None))
                figt = go.Figure(); figt.add_trace(go.Scatter(x=ts.index, y=ts['y'], name='Aggregated'))
                figt.add_trace(go.Scatter(x=ts.index, y=ts['roll'], name=f'Rolling{win}', line=dict(dash='dash')))
                figt.update_layout(title=f'{num_col2} ‚Äî Trend ({freq})', height=360)
                st_plotly(figt); register_fig('Trend', f'{num_col2} ‚Äî Trend ({freq})', figt, 'Chu·ªói th·ªùi gian + rolling mean.')
                st.caption('**√ù nghƒ©a**: Theo d√µi bi·∫øn ƒë·ªông; spike cu·ªëi k·ª≥ ‚Üí xem cut‚Äëoff.')
    with cB:
        if len(num_cols)>=2 and HAS_PLOTLY:
            method = st.radio('Correlation method', ['Pearson','Spearman (recommended)'] if spearman_recommended else ['Pearson','Spearman'],
                              index=(1 if spearman_recommended else 0), horizontal=True)
            mth = 'pearson' if method.startswith('Pearson') else 'spearman'
            corr = df[num_cols].corr(numeric_only=True, method=mth)
            figH = px.imshow(corr, color_continuous_scale='RdBu_r', zmin=-1, zmax=1, title=f'Correlation heatmap ({method.split()[0]})')
            st_plotly(figH); register_fig('Correlation', f'Correlation heatmap ({method.split()[0]})', figH, 'Li√™n h·ªá tuy·∫øn t√≠nh/h·∫°ng.')
            st.caption('**√ù nghƒ©a**: Pearson nh·∫°y v·ªõi outliers/kh√¥ng chu·∫©n; Spearman b·ªÅn v·ªØng h∆°n khi l·ªách/outliers.')
        else:
            st.info('Need ‚â•2 numeric columns for correlation.')

# ---------- TAB 3: Benford ----------
with TAB3:
    st.subheader('üî¢ Benford Law ‚Äî 1D & 2D')
    if not num_cols:
        st.info('No numeric columns available.')
    else:
        c1,c2 = st.columns(2)
        with c1:
            amt1 = st.selectbox('Amount (1D)', num_cols, key='bf1_col')
            if st.button('Run Benford 1D'):
                r = benford_1d(df[amt1])
                if not r: st.error('Cannot extract first digit.')
                else:
                    tb, var, p, MAD = r['table'], r['variance'], r['p'], r['MAD']
                    if HAS_PLOTLY:
                        fig1 = go.Figure(); fig1.add_trace(go.Bar(x=tb['digit'], y=tb['observed_p'], name='Observed'))
                        fig1.add_trace(go.Scatter(x=tb['digit'], y=tb['expected_p'], name='Expected', mode='lines', line=dict(color='#F6AE2D')))
                        fig1.update_layout(title='Benford 1D ‚Äî Observed vs Expected', height=340)
                        st_plotly(fig1); register_fig('Benford 1D', 'Benford 1D ‚Äî Obs vs Exp', fig1, 'Benford 1D check.')
                        st.caption('**√ù nghƒ©a**: Sai l·ªách l·ªõn ·ªü m·ªôt s·ªë ch·ªØ s·ªë ‚Üí d·∫•u hi·ªáu b·∫•t th∆∞·ªùng/nh·∫≠p li·ªáu ƒë·ªãnh h√¨nh.')
                    st.markdown('**Variance (counts)**'); st.dataframe(var, width='stretch', height=220)
                    thr = SS['risk_diff_threshold']
                    maxdiff = float(var['diff_pct'].abs().max()) if len(var)>0 else 0.0
                    msg = 'üü¢ Green'
                    if maxdiff >= 2*thr: msg='üö® Red'
                    elif maxdiff >= thr: msg='üü° Yellow'
                    sev = 'üü¢ Green'
                    if (p<0.01) or (MAD>0.015): sev='üö® Red'
                    elif (p<0.05) or (MAD>0.012): sev='üü° Yellow'
                    st.info(f"Diff% status: {msg} ‚Ä¢ p={p:.4f}, MAD={MAD:.4f} ‚áí Benford severity: {sev}")
        with c2:
            amt2 = st.selectbox('Amount (2D)', num_cols, index=min(1,len(num_cols)-1), key='bf2_col')
            if st.button('Run Benford 2D'):
                r2 = benford_2d(df[amt2])
                if not r2: st.error('Cannot extract first‚Äëtwo digits.')
                else:
                    tb2, var2, p2, MAD2 = r2['table'], r2['variance'], r2['p'], r2['MAD']
                    if HAS_PLOTLY:
                        fig2 = go.Figure(); fig2.add_trace(go.Bar(x=tb2['digit'], y=tb2['observed_p'], name='Observed'))
                        fig2.add_trace(go.Scatter(x=tb2['digit'], y=tb2['expected_p'], name='Expected', mode='lines', line=dict(color='#F6AE2D')))
                        fig2.update_layout(title='Benford 2D ‚Äî Observed vs Expected', height=340)
                        st_plotly(fig2); register_fig('Benford 2D','Benford 2D ‚Äî Obs vs Exp', fig2, 'Benford 2D check.')
                        st.caption('**√ù nghƒ©a**: 2D nh·∫°y h∆°n 1D; th∆∞·ªùng l·ªô pattern ch·∫ø t√°c.')
                    st.markdown('**Variance (counts)**'); st.dataframe(var2, width='stretch', height=220)
                    thr = SS['risk_diff_threshold']
                    maxdiff2 = float(var2['diff_pct'].abs().max()) if len(var2)>0 else 0.0
                    msg2 = 'üü¢ Green'
                    if maxdiff2 >= 2*thr: msg2='üö® Red'
                    elif maxdiff2 >= thr: msg2='üü° Yellow'
                    sev2 = 'üü¢ Green'
                    if (p2<0.01) or (MAD2>0.015): sev2='üö® Red'
                    elif (p2<0.05) or (MAD2>0.012): sev2='üü° Yellow'
                    st.info(f"Diff% status: {msg2} ‚Ä¢ p={p2:.4f}, MAD={MAD2:.4f} ‚áí Benford severity: {sev2}")

# ---------- TAB 4: Statistical Tests (guidance & interpretation) ----------
with TAB4:
    st.subheader('üß™ Statistical Tests ‚Äî h∆∞·ªõng d·∫´n & di·ªÖn gi·∫£i')
    WIZ = {'Group mean (ANOVA)': 'anova', 'Proportion (œá¬≤)': 'prop', 'Independence (œá¬≤)': 'chi2', 'Correlation (Pearson/Spearman)': 'corr'}
    obj = st.selectbox('Objective', list(WIZ.keys()))
    typ = WIZ[obj]

    if typ=='anova':
        st.info('**Khi d√πng**: So s√°nh **trung b√¨nh** gi·ªØa ‚â•2 nh√≥m. Ki·ªÉm tra Levene (ph∆∞∆°ng sai) & normality/ƒë·ªß l·ªõn.')
        if len(num_cols)==0 or len(cat_cols)==0:
            st.warning('Thi·∫øu Y numeric ho·∫∑c Group categorical. ƒê·ªïi c·ªôt/dtype ho·∫∑c d√πng Kruskal‚ÄìWallis.')
        else:
            y = st.selectbox('Y (numeric)', num_cols, key='an_y')
            g = st.selectbox('Group (categorical)', cat_cols, key='an_g')
            if (y not in df.columns) or (g not in df.columns):
                st.warning('Ch·ªçn c·ªôt h·ª£p l·ªá.')
            else:
                sub = df[[y,g]].dropna()
                if sub[g].nunique()<2:
                    st.warning('C·∫ßn ‚â•2 nh√≥m. Ch·ªçn c·ªôt Group kh√°c ho·∫∑c g·ªôp nh√≥m.')
                else:
                    groups = [d[y].values for _,d in sub.groupby(g)]
                    _, p_lev = stats.levene(*groups, center='median'); F, p = stats.f_oneway(*groups)
                    if HAS_PLOTLY:
                        fig = px.box(sub, x=g, y=y, color=g, title=f'{y} by {g}')
                        st_plotly(fig); register_fig('Tests', f'{y} by {g} (ANOVA)', fig, 'Group mean comparison.')
                    st.write({'ANOVA F': float(F), 'p': float(p), 'Levene p': float(p_lev)})
                    st.markdown('- **Di·ªÖn gi·∫£i**: p<0.05 ‚Üí kh√°c bi·ªát c√≥ √Ω nghƒ©a.\n- **Ti·∫øp theo**: p<0.05 ‚Üí **Tukey HSD**; n·∫øu vi ph·∫°m gi·∫£ ƒë·ªãnh ‚Üí **Kruskal‚ÄìWallis**.')
                    if p<0.05 and HAS_SM:
                        try:
                            tuk = pairwise_tukeyhsd(endog=sub[y], groups=sub[g], alpha=0.05)
                            df_tuk = pd.DataFrame(tuk.summary().data[1:], columns=tuk.summary().data[0])
                            st.markdown('**Post‚Äëhoc: Tukey HSD**')
                            st.dataframe(df_tuk, width='stretch', height=220)
                        except Exception:
                            pass
    elif typ=='prop':
        st.info('**Khi d√πng**: So s√°nh **t·ª∑ l·ªá** 0/1 gi·ªØa c√°c nh√≥m. C·ª° m·∫´u ƒë·ªß l·ªõn cho œá¬≤; n·∫øu √¥ th∆∞a ‚Üí Fisher.')
        flag_col = st.selectbox('Flag (0/1 or bool)', (num_cols + cat_cols) or df.columns.tolist(), key='pr_f')
        g = st.selectbox('Group (categorical)', cat_cols or df.columns.tolist(), key='pr_g')
        if (flag_col not in df.columns) or (g not in df.columns):
            st.warning('Ch·ªçn c·ªôt Flag & Group h·ª£p l·ªá.')
        else:
            ser = pd.to_numeric(df[flag_col], errors='coerce') if flag_col in num_cols else df[flag_col].astype(bool, copy=False)
            sub = pd.DataFrame({'flag': pd.Series(ser).astype(int), 'grp': df[g]}).dropna()
            if sub.empty:
                st.warning('Thi·∫øu d·ªØ li·ªáu h·ª£p l·ªá. Ki·ªÉm tra dtype/gi√° tr·ªã 0/1, ho·∫∑c ch·ªçn c·ªôt kh√°c.')
            else:
                ct = pd.crosstab(sub['grp'], sub['flag']); chi2, p, dof, exp = stats.chi2_contingency(ct)
                if HAS_PLOTLY:
                    fig = px.imshow(ct, text_auto=True, aspect='auto', color_continuous_scale='Blues', title='Proportion by group')
                    st_plotly(fig); register_fig('Tests', 'Proportion by group', fig, 'Compliance rate across groups.')
                st.write({'Chi2': float(chi2), 'p': float(p), 'dof': int(dof)})
                st.markdown('- **Di·ªÖn gi·∫£i**: p nh·ªè ‚Üí t·ª∑ l·ªá kh√°c nhau gi·ªØa c√°c nh√≥m.\n- **Ti·∫øp theo**: nh√≥m l·ªách m·∫°nh ‚Üí drill‚Äëdown quy tr√¨nh/nh√¢n s·ª±; ki·ªÉm tra policy/threshold.')
    elif typ=='chi2':
        st.info('**Khi d√πng**: Ki·ªÉm tra **ƒë·ªôc l·∫≠p** gi·ªØa hai bi·∫øn **categorical**. N·∫øu t·∫ßn su·∫•t th·∫•p ‚Üí d√πng Fisher.')
        a = st.selectbox('Variable A (categorical)', cat_cols or df.columns.tolist(), key='c2_a')
        b = st.selectbox('Variable B (categorical)', [c for c in (cat_cols or df.columns.tolist()) if c!=a], key='c2_b')
        if (a not in df.columns) or (b not in df.columns) or (a==b):
            st.warning('Ch·ªçn 2 c·ªôt categorical kh√°c nhau, ƒëang t·ªìn t·∫°i trong d·ªØ li·ªáu.')
        else:
            sub = df[[a,b]].dropna()
            if sub.empty:
                st.warning('Thi·∫øu d·ªØ li·ªáu sau khi drop NA; ƒë·ªïi c·ªôt ho·∫∑c gom nh√≥m.')
            else:
                ct = pd.crosstab(sub[a], sub[b]); chi2, p, dof, exp = stats.chi2_contingency(ct); cv = cramers_v(ct)
                if HAS_PLOTLY:
                    fig = px.imshow(ct, text_auto=True, aspect='auto', color_continuous_scale='Reds', title='Contingency table')
                    st_plotly(fig); register_fig('Tests', 'Contingency œá¬≤', fig, 'Dependence strength via Cram√©r V.')
                st.write({'Chi2': float(chi2), 'p': float(p), 'dof': int(dof), 'Cram√©rV': float(cv)})
                st.markdown('- **Di·ªÖn gi·∫£i**: p nh·ªè ‚Üí c√≥ ph·ª• thu·ªôc; **Cram√©r V** ~0.1 y·∫øu, ~0.3 v·ª´a, ~0.5 m·∫°nh (tham kh·∫£o).')
    elif typ=='corr':
        st.info('**Khi d√πng**: T∆∞∆°ng quan gi·ªØa hai bi·∫øn numeric. Pearson (tuy·∫øn t√≠nh); Spearman (theo h·∫°ng, b·ªÅn v·ªõi outliers/kh√¥ng chu·∫©n).')
        if len(num_cols)<2:
            st.warning('C·∫ßn ‚â•2 bi·∫øn s·ªë. Ch·ªçn th√™m bi·∫øn numeric kh√°c.')
        else:
            x = st.selectbox('X', num_cols, key='cr_x')
            y = st.selectbox('Y', [c for c in num_cols if c!=x], key='cr_y')
            method = st.radio('Method', ['Pearson','Spearman'], index=(1 if spearman_recommended else 0), horizontal=True)
            if (x not in df.columns) or (y not in df.columns) or (x==y):
                st.warning('Ch·ªçn 2 c·ªôt numeric h·ª£p l·ªá, kh√°c nhau.')
            else:
                sub = df[[x,y]].dropna()
                if len(sub)<3:
                    st.warning('Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi drop NA.')
                else:
                    if method=='Pearson': r, pv = stats.pearsonr(sub[x], sub[y])
                    else: r, pv = stats.spearmanr(sub[x], sub[y])
                    if HAS_PLOTLY:
                        fig = px.scatter(sub, x=x, y=y, trendline=('ols' if method=='Pearson' else None), title=f'{x} vs {y} ({method})')
                        st_plotly(fig); register_fig('Tests', f'{x} vs {y} ({method})', fig, 'Li√™n h·ªá tuy·∫øn t√≠nh/h·∫°ng.')
                    st.write({'method': method, 'r': float(r), 'p': float(pv)})

# ---------- TAB 5: Regression ----------
with TAB5:
    st.subheader('üìò Regression (Linear / Logistic)')
    if not HAS_SK:
        st.info('Install scikit‚Äëlearn to use Regression: `pip install scikit-learn`.')
    else:
        rtab1, rtab2 = st.tabs(['Linear Regression','Logistic Regression'])
        with rtab1:
            if len(num_cols)>=2:
                y_t = st.selectbox('Target (numeric)', num_cols, key='lin_y')
                X_t = st.multiselect('Features (X)', [c for c in num_cols if c!=y_t], default=[c for c in num_cols if c!=y_t][:2], key='lin_X')
                test_size = st.slider('Test size', 0.1, 0.5, 0.25, 0.05, key='lin_ts')
                if st.button('Run Linear Regression', key='btn_lin'):
                    if (y_t not in df.columns) or any([(x not in df.columns) for x in X_t]):
                        st.error('M·ªôt ho·∫∑c nhi·ªÅu c·ªôt kh√¥ng c√≤n t·ªìn t·∫°i trong d·ªØ li·ªáu hi·ªán t·∫°i.');
                    else:
                        sub = df[[y_t] + X_t].dropna()
                        if len(sub) < (len(X_t)+5):
                            st.error('Not enough data after dropping NA.')
                        else:
                            X = sub[X_t]; yv = sub[y_t]
                            Xtr,Xte,ytr,yte = train_test_split(X,yv,test_size=test_size,random_state=42)
                            mdl = LinearRegression().fit(Xtr,ytr); yhat = mdl.predict(Xte)
                            r2 = r2_score(yte,yhat); adj = 1-(1-r2)*(len(yte)-1)/(len(yte)-Xte.shape[1]-1)
                            rmse = float(np.sqrt(mean_squared_error(yte,yhat)))
                            st.write({"R2":round(r2,3),"Adj_R2":round(adj,3),"RMSE":round(rmse,3)})
                            if HAS_PLOTLY:
                                resid = yte - yhat
                                fig1 = px.scatter(x=yhat, y=resid, labels={'x':'Fitted','y':'Residuals'}, title='Residuals vs Fitted')
                                fig2 = px.histogram(resid, nbins=SS['bins'], title='Residuals')
                                st_plotly(fig1); register_fig('Regression', 'Residuals vs Fitted', fig1, 'Homoscedastic & mean-zero residuals desired.')
                                st_plotly(fig2); register_fig('Regression', 'Residuals histogram', fig2, 'Residual distribution check.')
            else:
                st.info('Need at least 2 numeric variables.')
        with rtab2:
            bin_candidates = []
            for c in df.columns:
                s = pd.Series(df[c]).dropna()
                if s.nunique() == 2:
                    bin_candidates.append(c)
            if len(bin_candidates)==0:
                st.info('No binary-like column detected (exactly two unique values).')
            else:
                yb = st.selectbox('Target (binary)', bin_candidates, key='logit_y')
                Xb = st.multiselect('Features (X)', [c for c in df.columns if c!=yb and pd.api.types.is_numeric_dtype(df[c])], key='logit_X')
                if st.button('Run Logistic Regression', key='btn_logit'):
                    if (yb not in df.columns) or any([(x not in df.columns) for x in Xb]):
                        st.error('M·ªôt ho·∫∑c nhi·ªÅu c·ªôt kh√¥ng c√≤n t·ªìn t·∫°i trong d·ªØ li·ªáu hi·ªán t·∫°i.');
                    else:
                        sub = df[[yb] + Xb].dropna()
                        if len(sub) < (len(Xb)+10):
                            st.error('Not enough data after dropping NA.')
                        else:
                            X = sub[Xb]
                            y = sub[yb]
                            if y.dtype != np.number:
                                classes = sorted(y.unique()); y = (y == classes[-1]).astype(int)
                            Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.25,random_state=42)
                            try:
                                model = LogisticRegression(max_iter=1000).fit(Xtr,ytr)
                                proba = model.predict_proba(Xte)[:,1]; pred = (proba>=0.5).astype(int)
                                acc = accuracy_score(yte,pred); auc = roc_auc_score(yte,proba)
                                st.write({"Accuracy":round(acc,3), "ROC AUC":round(auc,3)})
                                cm = confusion_matrix(yte,pred)
                                st.write({'ConfusionMatrix': cm.tolist()})
                                if HAS_PLOTLY:
                                    fpr,tpr,thr = roc_curve(yte, proba)
                                    fig = px.area(x=fpr, y=tpr, title='ROC Curve', labels={'x':'False Positive Rate','y':'True Positive Rate'})
                                    fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)
                                    st_plotly(fig); register_fig('Regression', 'ROC Curve', fig, 'Model discrimination power.')
                            except Exception as e:
                                st.error(f'Logistic Regression error: {e}')

# ---------- TAB 6: Fraud Flags ----------
with TAB6:
    st.subheader('üö© Fraud Flags')
    amount_col = st.selectbox('Amount (optional)', options=['(None)'] + num_cols, key='ff_amt')
    dt_col = st.selectbox('Datetime (optional)', options=['(None)'] + df.columns.tolist(), key='ff_dt')
    group_cols = st.multiselect('Composite key to check duplicates', options=df.columns.tolist(), default=[], key='ff_groups')

    def compute_fraud_flags(df: pd.DataFrame, amount_col: str|None, datetime_col: str|None, group_id_cols: list[str]):
        flags, visuals = [], []
        num_cols2 = df.select_dtypes(include=[np.number]).columns.tolist()
        if len(num_cols2)>0:
            zero_tbl = []
            for c in num_cols2:
                s = df[c]; zero_ratio = float((s==0).mean()) if len(s)>0 else 0.0
                if zero_ratio>0.3:
                    flags.append({"flag":"High zero ratio","column":c,"threshold":0.3,"value":round(zero_ratio,3),"note":"Threshold/rounding or unusual coding."})
                zero_tbl.append({"column":c, "zero_ratio": round(zero_ratio,3)})
            visuals.append(("Zero ratios", pd.DataFrame(zero_tbl)))
        if amount_col and amount_col in df.columns and pd.api.types.is_numeric_dtype(df[amount_col]):
            s = pd.to_numeric(df[amount_col], errors='coerce').dropna()
            if len(s)>20:
                p95 = s.quantile(0.95); p99 = s.quantile(0.99); tail99 = float((s>p99).mean())
                if tail99>0.02:
                    flags.append({"flag":"Too‚Äëheavy right tail (P99)","column":amount_col,"threshold":0.02,"value":round(tail99,3),"note":"Check outliers/segmentation."})
                visuals.append(("P95/P99 thresholds", pd.DataFrame({"metric":["P95","P99"], "value":[p95,p99]})))
        if datetime_col and datetime_col in df.columns:
            try:
                t = pd.to_datetime(df[datetime_col], errors='coerce'); hour = t.dt.hour
                if hour.notna().any():
                    off_hours = ((hour<7) | (hour>20)).mean()
                    if off_hours>0.15:
                        flags.append({"flag":"High off‚Äëhours activity","column":datetime_col,"threshold":0.15,"value":round(float(off_hours),3),"note":"Review privileges/shifts/automation."})
                    if HAS_PLOTLY:
                        hcnt = hour.dropna().value_counts().sort_index(); fig = px.bar(x=hcnt.index, y=hcnt.values, title='Hourly distribution (0‚Äì23)', labels={'x':'Hour','y':'Txns'})
                        st_plotly(fig); register_fig('Fraud Flags', 'Hourly distribution', fig, 'Anomaly indicator')
            except Exception:
                pass
        if datetime_col and datetime_col in df.columns:
            try:
                t = pd.to_datetime(df[datetime_col], errors='coerce'); dow = t.dt.dayofweek
                if dow.notna().any():
                    dow_share = dow.value_counts(normalize=True).sort_index(); mean_share = dow_share.mean(); std_share = dow_share.std()
                    unusual = (dow_share - mean_share).abs() > (2*std_share) if std_share>0 else pd.Series([False]*len(dow_share), index=dow_share.index)
                    if unusual.any():
                        flags.append({"flag":"Unusual day‚Äëof‚Äëweek pattern","column":datetime_col,"threshold":"¬±2œÉ","value":"; ".join([str(int(i)) for i,v in unusual.items() if v]),"note":"Check calendar/period‚Äëend pressure."})
                    if HAS_PLOTLY:
                        fig = px.bar(x=["Mon","Tue","Wed","Thu","Fri","Sat","Sun"], y=dow_share.reindex(range(7), fill_value=0).values, title='DOW distribution', labels={'x':'DOW','y':'Share'})
                        st_plotly(fig); register_fig('Fraud Flags', 'DOW distribution', fig, 'Anomaly indicator')
            except Exception:
                pass
        if group_id_cols:
            cols = [c for c in group_id_cols if c in df.columns]
            if cols:
                dup = df[cols].astype(str).value_counts().reset_index(name='count'); top_dup = dup[dup['count']>1].head(20)
                if not top_dup.empty:
                    flags.append({"flag":"Duplicate composite keys","column":" + ".join(cols),"threshold":">1","value":int(top_dup['count'].max()),"note":"Review duplicates/ghost entries."})
                visuals.append(("Top duplicate keys (>1)", top_dup))
        return flags, visuals

    if st.button('üîé Scan Flags'):
        amt = None if amount_col=='(None)' else amount_col; dtc = None if dt_col=='(None)' else dt_col
        flags, visuals = compute_fraud_flags(df, amt, dtc, group_cols); SS['fraud_flags'] = flags
        if flags:
            for fl in flags:
                alarm = 'üö®' if isinstance(fl.get('value',0),(int,float)) and fl.get('value',0)>fl.get('threshold',0) else 'üü°'
                st.warning(f"{alarm} [{fl['flag']}] {fl['column']} ‚Ä¢ thr:{fl['threshold']} ‚Ä¢ val:{fl['value']} ‚Äî {fl['note']}")
        else:
            st.success('üü¢ No notable flags based on current rules.')
        for title, obj in visuals:
            if HAS_PLOTLY and not isinstance(obj, pd.DataFrame):
                pass
            elif isinstance(obj, pd.DataFrame):
                st.markdown(f'**{title}**'); st.dataframe(obj, width='stretch', height=240)

# ---------- TAB 7: Risk Assessment & Export ----------
with TAB7:
    cA, cB = st.columns([3,2])
    with cA:
        st.subheader('üßÆ Automated Risk Assessment ‚Äî Signals ‚Üí Next tests ‚Üí Interpretation')
        signals=[]
        def detect_mixed_types(ser: pd.Series, sample=1000):
            v = ser.dropna().head(sample).apply(lambda x: type(x)).unique()
            return len(v)>1
        def quality_report(df: pd.DataFrame):
            rep = []
            for c in df.columns:
                s = df[c]
                rep.append({'column': c,'dtype': str(s.dtype),'missing_ratio': round(float(s.isna().mean()),4),
                            'n_unique': int(s.nunique(dropna=True)),'constant': bool(s.nunique(dropna=True)<=1),
                            'mixed_types': detect_mixed_types(s)})
            dupes = int(df.duplicated().sum())
            return pd.DataFrame(rep), dupes
        rep, n_dupes = quality_report(df)
        if n_dupes>0:
            signals.append({'signal':'Duplicate rows','severity':'Medium','action':'Define composite key & walkthrough duplicates','why':'Double posting/ghost entries','followup':'N·∫øu c√≤n tr√πng theo (Vendor,Bank,Amount,Date) ‚Üí so√°t ph√™ duy·ªát & ki·ªÉm so√°t.'})
        for _,row in rep.iterrows():
            if row['missing_ratio']>0.2:
                signals.append({'signal':f'High missing ratio in {row["column"]} ({row["missing_ratio"]:.0%})','severity':'Medium','action':'Impute/exclude; stratify by completeness','why':'Weak capture/ETL','followup':'N·∫øu kh√¥ng ng·∫´u nhi√™n ‚Üí ph√¢n t·∫ßng theo ngu·ªìn/chi nh√°nh.'})
        for c in num_cols[:20]:
            s = pd.to_numeric(df[c], errors='coerce').replace([np.inf,-np.inf], np.nan).dropna()
            if len(s)==0: continue
            zr=float((s==0).mean()); p99=s.quantile(0.99); share99=float((s>p99).mean())
            if zr>0.3:
                signals.append({'signal':f'Zero‚Äëheavy numeric {c} ({zr:.0%})','severity':'Medium','action':'œá¬≤/Fisher theo ƒë∆°n v·ªã; review policy/thresholds','why':'Thresholding/non‚Äëusage','followup':'N·∫øu gom theo ƒë∆°n v·ªã th·∫•y t·∫≠p trung ‚Üí nghi sai c·∫•u h√¨nh.'})
            if share99>0.02:
                signals.append({'signal':f'Heavy right tail in {c} (>P99 share {share99:.1%})','severity':'High','action':'Benford 1D/2D; cut‚Äëoff near period end; outlier review','why':'Outliers/fabrication','followup':'N·∫øu Benford l·ªách + spike cu·ªëi k·ª≥ ‚Üí nghi smoothing r·ªßi ro.'})
        st.dataframe(pd.DataFrame(signals) if signals else pd.DataFrame([{'status':'No strong risk signals'}]), width='stretch', height=320)
        with st.expander('üìã H∆∞·ªõng d·∫´n nhanh (logic)'):
            st.markdown('''
- **Distribution & Shape**: ƒë·ªçc mean/std/quantiles/SE/CI, shape/tails/normality; x√°c nh·∫≠n Histogram+KDE/Box/ECDF/QQ.
- **Tail d√†y / l·ªách l·ªõn** ‚Üí **Benford 1D/2D**; n·∫øu |diff%| ‚â• 5% ‚Üí c·∫£nh b√°o ‚Üí **drill‚Äëdown + cut‚Äëoff**.
- **Zero‚Äëheavy** ho·∫∑c t·ª∑ l·ªá kh√°c nhau theo nh√≥m ‚Üí **Proportion œá¬≤ / Independence œá¬≤**.
- **Trend** (D/W/M/Q + Rolling + YoY); th·∫•y m√πa v·ª•/spike ‚Üí test **cut‚Äëoff/œá¬≤ th·ªùi gian√óstatus**.
- **Quan h·ªá bi·∫øn** ‚Üí **Correlation** (Pearson/Spearman); n·∫øu d·ª± b√°o/gi·∫£i th√≠ch ‚Üí **Regression**.
''')

    with cB:
        st.subheader('üßæ Export (Plotly snapshots)')
        incl = st.multiselect('Include sections', ['Distribution','Trend','Correlation','Benford 1D','Benford 2D','Tests','Regression','Fraud Flags'],
                              default=['Distribution','Benford 1D','Benford 2D','Tests'])
        title = st.text_input('Report title', value='Audit Statistics ‚Äî Findings')

        def save_plotly_png(fig, name_prefix='fig', dpi=2.0):
            if not HAS_KALEIDO: return None
            try:
                img_bytes = fig.to_image(format='png', scale=dpi)
                path = f"{name_prefix}_{int(time.time()*1000)}.png"
                with open(path,'wb') as f: f.write(img_bytes)
                return path
            except Exception:
                return None

        export_bundle = [it for it in SS['fig_registry'] if it['section'] in incl]
        if st.button('üñº Capture & Export DOCX/PDF'):
            if not export_bundle:
                st.warning('No visuals captured yet. Run the modules first.')
            else:
                img_paths = []
                for i, it in enumerate(export_bundle, 1):
                    pth = save_plotly_png(it['fig'], name_prefix=f"{it['section']}_{i}") if HAS_KALEIDO else None
                    if pth: img_paths.append((it['title'], it['section'], it['caption'], pth))
                meta = {'file': fname, 'sha12': SS['sha12'], 'time': datetime.now().isoformat()}
                docx_path = None; pdf_path = None
                if HAS_DOCX and img_paths:
                    doc = docx.Document(); doc.add_heading(title, 0)
                    doc.add_paragraph(f"File: {meta['file']} ‚Ä¢ SHA12={meta['sha12']} ‚Ä¢ Time: {meta['time']}")
                    cur_sec = None
                    for title_i, sec, cap, img in img_paths:
                        if cur_sec != sec:
                            cur_sec = sec; doc.add_heading(sec, level=1)
                        doc.add_heading(title_i, level=2)
                        doc.add_picture(img, width=Inches(6.5))
                        doc.add_paragraph(cap)
                    docx_path = f"report_{int(time.time())}.docx"; doc.save(docx_path)
                if HAS_PDF and img_paths:
                    doc = fitz.open(); page = doc.new_page(); y = 36
                    page.insert_text((36,y), title, fontsize=16); y+=22
                    page.insert_text((36,y), f"File: {meta['file']} ‚Ä¢ SHA12={meta['sha12']} ‚Ä¢ Time: {meta['time']}", fontsize=10); y+=20
                    cur_sec=None
                    for title_i, sec, cap, img in img_paths:
                        if y>740: page = doc.new_page(); y=36
                        if cur_sec != sec:
                            page.insert_text((36,y), sec, fontsize=13); y+=18; cur_sec=sec
                        page.insert_text((36,y), title_i, fontsize=12); y+=14
                        rect = fitz.Rect(36, y, 559, y+300); page.insert_image(rect, filename=img); y+=310
                        page.insert_text((36,y), cap, fontsize=10); y+=16
                    pdf_path = f"report_{int(time.time())}.pdf"; doc.save(pdf_path); doc.close()
                outs = [p for p in [docx_path, pdf_path] if p]
                if outs:
                    st.success('Exported: ' + ', '.join(outs))
                    for pth in outs:
                        with open(pth,'rb') as f: st.download_button(f'‚¨áÔ∏è Download {os.path.basename(pth)}', data=f.read(), file_name=os.path.basename(pth))
                else:
                    if not HAS_KALEIDO: st.error('Kaleido is required to export exact Plotly visuals. Install package `kaleido`.')
                    else: st.error('Export failed. Make sure visuals are generated first.')
                for _,_,_,img in img_paths:
                    with contextlib.suppress(Exception): os.remove(img)


