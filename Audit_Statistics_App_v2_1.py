import io, json, time, warnings, contextlib
from datetime import datetime
import numpy as np
import pandas as pd
import streamlit as st
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from statsmodels.stats.outliers_influence import variance_inflation_factor
from openpyxl import load_workbook

# Optional engines (best effort)
with contextlib.suppress(Exception):
    import polars as pl
with contextlib.suppress(Exception):
    import duckdb

warnings.filterwarnings("ignore")

# ---------------------------- THEME & STYLE ----------------------------
st.set_page_config(page_title="Audit Statistics (v2.2)", layout="wide")
sns.set_style("whitegrid")
PALETTE = ["#2F4858", "#33658A", "#86BBD8", "#758E4F", "#F6AE2D"]  # t·ªëi gi·∫£n
plt.rcParams.update({
    "axes.facecolor": "#FAFAFA",
    "figure.facecolor": "#FFFFFF",
    "axes.labelcolor": "#2F4858",
    "text.color": "#2F4858",
})

# ---------------------------- RULE REGISTRY ----------------------------
RULES = [
    {"metric": "n", "op": ">", "value": 5000, "severity": "info",
     "message": "M·∫´u r·∫•t l·ªõn (n>5000): p-value Shapiro c√≥ th·ªÉ k√©m tin c·∫≠y; h√£y ∆∞u ti√™n Q‚ÄìQ plot, skew/kurtosis, ho·∫∑c test b·ªÅn v·ªØng.",
     "ref": "SciPy Shapiro notes", "ref_id": "local"},
    {"metric": "shapiro_p", "op": "<", "value": 0.05, "severity": "caution",
     "message": "C√≥ d·∫•u hi·ªáu l·ªách chu·∫©n (Shapiro p<0.05). Xem Q‚ÄìQ plot; c√¢n nh·∫Øc Mann‚ÄìWhitney/Kruskal thay v√¨ t/ANOVA n·∫øu c·∫ßn.",
     "ref": "SciPy Shapiro", "ref_id": "local"},
    {"metric": "levene_p", "op": "<", "value": 0.05, "severity": "action",
     "message": "Ph∆∞∆°ng sai kh√¥ng ƒë·ªìng nh·∫•t (Levene p<0.05): ∆Øu ti√™n Welch t-test / Welch ANOVA.",
     "ref": "Levene & Welch", "ref_id": "local"},
    {"metric": "missing_ratio", "op": ">", "value": 0.2, "severity": "action",
     "message": "Thi·∫øu d·ªØ li·ªáu >20%: xem l·∫°i thu th·∫≠p/ETL; c√¢n nh·∫Øc lo·∫°i bi·∫øn ho·∫∑c chi·∫øn l∆∞·ª£c x·ª≠ l√Ω missing tr∆∞·ªõc khi test.",
     "ref": "Data quality practice", "ref_id": "local"},
    {"metric": "cohen_d", "op": "between", "value": [0.5, 0.8], "severity": "info",
     "message": "Cohen‚Äôs d ·ªü m·ª©c v·ª´a (‚âà0.5‚Äì0.8). H√£y b√°o c√°o k√®m p-value ƒë·ªÉ ph·∫£n √°nh c·∫£ √Ω nghƒ©a & c·ª° ·∫£nh h∆∞·ªüng.",
     "ref": "Cohen thresholds", "ref_id": "local"},
    {"metric": "cohen_d", "op": ">", "value": 0.8, "severity": "action",
     "message": "Cohen‚Äôs d l·ªõn (>0.8): kh√°c bi·ªát th·ª±c s·ª± ƒë√°ng k·ªÉ v·ªÅ m·∫∑t th·ª±c h√†nh (practical significance).",
     "ref": "Cohen thresholds", "ref_id": "local"},
    {"metric": "eta2", "op": ">", "value": 0.14, "severity": "action",
     "message": "Eta-squared ‚â•0.14 (large): bi·∫øn nh√≥m gi·∫£i th√≠ch t·ª∑ l·ªá l·ªõn ph∆∞∆°ng sai ‚Äî c·∫ßn drill-down post‚Äëhoc.",
     "ref": "Eta-squared thresholds", "ref_id": "local"},
    {"metric": "corr_r", "op": ">", "value": 0.5, "severity": "info",
     "message": "T∆∞∆°ng quan m·∫°nh (|r|>0.5): ki·ªÉm tra quan h·ªá tuy·∫øn t√≠nh/ngo·∫°i l·ªá; x√°c nh·∫≠n b·∫±ng Spearman n·∫øu nghi ng·ªù.",
     "ref": "Cohen r thresholds", "ref_id": "local"},
]
SEVERITY_RANK = {"action": 3, "caution": 2, "info": 1}

def eval_rule(value, rule):
    if value is None or (isinstance(value, float) and np.isnan(value)):
        return False
    op = rule["op"]
    thr = rule["value"]
    if op == "<": return value < thr
    if op == "<=": return value <= thr
    if op == ">": return value > thr
    if op == ">=": return value >= thr
    if op == "==": return value == thr
    if op == "between":
        lo, hi = thr
        return (value >= lo) and (value <= hi)
    return False

def score_insights(facts: dict):
    hits = []
    for r in RULES:
        val = facts.get(r["metric"])
        if eval_rule(val, r):
            hits.append({
                "metric": r["metric"], "value": val,
                "severity": r["severity"],
                "message": r["message"],
                "ref": r["ref"], "ref_id": r["ref_id"],
                "score": SEVERITY_RANK[r["severity"]],
            })
    hits = sorted(hits, key=lambda x: (-x["score"], x["metric"]))
    return hits

# ---------------------------- HELPERS ----------------------------

def detect_mixed_types(ser: pd.Series, sample=1000):
    sample_vals = ser.dropna().head(sample).values
    if len(sample_vals) == 0:
        return False
    types = set(type(v) for v in sample_vals)
    return len(types) > 1

def quality_report(df: pd.DataFrame):
    rep = []
    for c in df.columns:
        s = df[c]
        miss = s.isna().mean()
        nunique = s.nunique(dropna=True)
        constant = (nunique <= 1)
        mixed = detect_mixed_types(s)
        rep.append({"column": c, "dtype": str(s.dtype), "missing_ratio": round(miss, 4),
                    "n_unique": int(nunique), "constant": constant, "mixed_types": mixed})
    rep = pd.DataFrame(rep)
    n_dupes = int(df.duplicated().sum())
    return rep, n_dupes

def parse_numeric(series: pd.Series, decimal=".", thousands=None, strip_currency=True):
    s = series.astype(str).str.strip()
    if strip_currency:
        s = s.str.replace(r"[^\d,\.\-eE]", "", regex=True)
    if thousands:
        s = s.str.replace(thousands, "", regex=False)
    if decimal != ".":
        s = s.str.replace(decimal, ".", regex=False)
    return pd.to_numeric(s, errors="coerce")

def robust_outlier_flags(x: pd.Series):
    x = pd.to_numeric(x, errors="coerce")
    s = x.dropna()
    if len(s) < 5:
        return pd.Series([False]*len(x), index=x.index)
    q1, q3 = np.percentile(s, [25, 75])
    iqr = q3 - q1
    lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr
    return (x < lo) | (x > hi)

def normality_summary(x: pd.Series):
    x = pd.Series(x).dropna()
    n = len(x)
    skew = stats.skew(x) if n > 2 else np.nan
    kurt = stats.kurtosis(x, fisher=True) if n > 3 else np.nan
    sh_w, sh_p = (np.nan, np.nan)
    if 3 <= n <= 5000:
        sh_w, sh_p = stats.shapiro(x)
    ad = stats.anderson(x, dist='norm')
    return {"n": n, "skew": skew, "kurtosis_fisher": kurt,
            "shapiro_W": sh_w, "shapiro_p": sh_p,
            "anderson_stat": ad.statistic, "anderson_crit": ad.critical_values}

def levene_equal_var(*groups):
    try:
        stat, p = stats.levene(*groups, center='median')
        return stat, p
    except Exception:
        return np.nan, np.nan

def cohen_d(x, y):
    x, y = pd.Series(x).dropna(), pd.Series(y).dropna()
    nx, ny = len(x), len(y)
    sx, sy = np.var(x, ddof=1), np.var(y, ddof=1)
    denom = ((nx-1)*sx + (ny-1)*sy) / (nx+ny-2) if (nx+ny-2) > 0 else np.nan
    return (x.mean() - y.mean()) / np.sqrt(denom) if denom>0 else np.nan

def hedges_g(x, y):
    d = cohen_d(x, y)
    nx, ny = len(pd.Series(x).dropna()), len(pd.Series(y).dropna())
    J = 1 - (3/(4*(nx+ny)-9)) if (nx+ny) > 2 else 1
    return d * J

def eta_omega(groups):
    y_all = pd.concat([pd.Series(v) for v in groups], axis=0)
    grand = y_all.mean()
    ss_between = sum([len(v) * (pd.Series(v).mean() - grand) ** 2 for v in groups])
    ss_within = sum([((pd.Series(v) - pd.Series(v).mean()) ** 2).sum() for v in groups])
    df_between = len(groups) - 1
    df_within = len(y_all) - len(groups)
    eta2 = ss_between / (ss_between + ss_within) if (ss_between + ss_within)>0 else np.nan
    omega2 = (ss_between - df_between * (ss_within/df_within)) / (ss_between + ss_within + (ss_within/df_within)) if df_within>0 else np.nan
    return eta2, omega2

def calc_vif(X: pd.DataFrame):
    X_ = X.copy().assign(_const=1.0)
    vifs = {}
    for i, col in enumerate(X.columns):
        try:
            vifs[col] = variance_inflation_factor(X_.values, i)
        except Exception:
            vifs[col] = np.nan
    return vifs

def sample_size_proportion(p=0.5, z=1.96, e=0.05, N=None):
    n0 = (z**2 * p*(1-p)) / (e**2)
    if N and N>0:
        n = n0 / (1 + (n0-1)/N)
    else:
        n = n0
    return int(np.ceil(n))

def sample_size_mean(sigma, z=1.96, e=1.0, N=None):
    n0 = (z**2 * sigma**2) / (e**2)
    if N and N>0:
        n = n0 / (1 + (n0-1)/N)
    else:
        n = n0
    return int(np.ceil(n))

# -------------------- EXCEL-FIRST HELPERS (cached) --------------------

def _bytes_of_uploaded(uploaded_file) -> bytes:
    pos = uploaded_file.tell()
    uploaded_file.seek(0)
    data = uploaded_file.read()
    uploaded_file.seek(pos)
    return data

@st.cache_data(ttl=3600)
def list_sheets_xlsx(file_bytes: bytes):
    wb = load_workbook(io.BytesIO(file_bytes), read_only=True, data_only=True)
    try:
        return wb.sheetnames
    finally:
        wb.close()

@st.cache_data(ttl=3600)
def get_headers_xlsx(file_bytes: bytes, sheet_name: str, dtype_map: dict | None = None):
    df0 = pd.read_excel(
        io.BytesIO(file_bytes),
        sheet_name=sheet_name,
        nrows=0,
        dtype=dtype_map,
        engine="openpyxl",
    )
    return df0.columns.tolist()

@st.cache_data(ttl=3600)
def read_selected_columns_xlsx(
    file_bytes: bytes,
    sheet_name: str,
    usecols: list[str],
    nrows: int | None = None,
    dtype_map: dict | None = None,
):
    return pd.read_excel(
        io.BytesIO(file_bytes),
        sheet_name=sheet_name,
        usecols=usecols,
        nrows=nrows,
        dtype=dtype_map,
        engine="openpyxl",
    )

# ---------- QUICK STATS & PLOT (downsample + KDE conditional) ----------

def quick_stats_series(s: pd.Series) -> dict:
    x = pd.to_numeric(s, errors="coerce").dropna()
    n = x.size
    if n == 0:
        return {"n": 0, "mean": np.nan, "p50": np.nan, "std": np.nan,
                "skew": np.nan, "kurt": np.nan, "mode": np.nan}
    mean = float(x.mean())
    p50 = float(x.median())
    std = float(x.std(ddof=1)) if n > 1 else np.nan
    skew = float(stats.skew(x)) if n > 2 else np.nan
    kurt = float(stats.kurtosis(x, fisher=True)) if n > 3 else np.nan
    counts, bins = np.histogram(x, bins="fd" if n >= 500 else 30)
    mode_center = float((bins[np.argmax(counts)] + bins[np.argmax(counts) + 1]) / 2)
    return {"n": n, "mean": mean, "p50": p50, "std": std, "skew": skew, "kurt": kurt, "mode": mode_center}


def plot_distribution_with_overlay(
    s: pd.Series,
    kde_max_n: int = 50_000,
    max_points: int = 100_000,
    palette=PALETTE
):
    x = pd.to_numeric(s, errors="coerce").dropna()
    n = x.size
    if n == 0:
        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu numeric ƒë·ªÉ v·∫Ω.")
        return
    if n > max_points:
        x = x.sample(max_points, random_state=42).sort_values()

    stats_ = quick_stats_series(x)
    show_kde = n <= kde_max_n

    fig, ax = plt.subplots(figsize=(7, 4))
    sns.histplot(x, kde=show_kde, color=palette[2], ax=ax, bins="auto")
    ax.set_title(f"Distribution (n={n:,})  -  KDE={'On' if show_kde else 'Off'}")

    m, p50, mo, sd = stats_["mean"], stats_["p50"], stats_["mode"], stats_["std"]
    for v, label, color in [(m, "Avg", "#F6AE2D"), (p50, "P50", "#33658A"), (mo, "Mode", "#758E4F")]:
        if np.isfinite(v):
            ax.axvline(v, color=color, linestyle="--", linewidth=1.5, label=label)
    if np.isfinite(m) and np.isfinite(sd):
        ax.axvspan(m - sd, m + sd, color="#86BBD8", alpha=0.15, label="¬±1œÉ")
    ax.legend()
    st.pyplot(fig, use_container_width=True)

    st.caption(
        f"KPI ‚Ä¢ N={stats_['n']:,} ‚Ä¢ Avg={stats_['mean']:.4g} ‚Ä¢ P50={stats_['p50']:.4g} ‚Ä¢ "
        f"Mode={stats_['mode']:.4g} ‚Ä¢ œÉ={stats_['std']:.4g} ‚Ä¢ Skew={stats_['skew']:.3g} ‚Ä¢ Kurt={stats_['kurt']:.3g}"
    )

# ---------------------------- APP MAIN ----------------------------

def main():
    st.sidebar.header("‚öôÔ∏è Modules & Options")
    MOD_DATA = st.sidebar.checkbox("Data Quality", True)
    MOD_PROFILE = st.sidebar.checkbox("Profiling (Descriptive + Distribution)", True)
    MOD_SAMPLING = st.sidebar.checkbox("Sampling & Size", True)
    MOD_TESTS = st.sidebar.checkbox("Statistical Tests", True)
    MOD_INSIGHTS = st.sidebar.checkbox("Insights (Auto)", True)

    st.sidebar.markdown("---")
    SHOW_PLOTS = st.sidebar.checkbox("Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì", True)
    RANDOM_SEED = st.sidebar.number_input("Random seed", value=42, step=1)
    st.sidebar.caption("M·ªôt s·ªë thao t√°c c√≥ ng·∫´u nhi√™n (train/test split).")

    st.title("üìä Audit Statistics ‚Äî Excel‚Äëfirst, Fast & Minimalist")
    st.caption("Lu·ªìng Data Auditor ‚Ä¢ T·ªëi ∆∞u th·ªëng k√™ h·ªØu √≠ch ‚Ä¢ C·∫£nh b√°o t·ª± ƒë·ªông theo chu·∫©n")

    uploaded = st.file_uploader("Upload d·ªØ li·ªáu (CSV/XLSX)", type=["csv", "xlsx"])
    if not uploaded:
        st.info("H√£y upload m·ªôt file ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        return

    @st.cache_data(ttl=3600)
    def _read_csv_cached(b: bytes):
        try:
            return pd.read_csv(io.BytesIO(b)), None
        except UnicodeDecodeError:
            return pd.read_csv(io.BytesIO(b), encoding="cp1252"), None
        except Exception as e:
            return None, str(e)

    file_bytes = _bytes_of_uploaded(uploaded)

    # Ingest
    if uploaded.name.lower().endswith(".csv"):
        df, err = _read_csv_cached(file_bytes)
        if err:
            st.error(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c CSV: {err}")
            return
        st.subheader("üëÄ Data Preview (CSV)")
        st.dataframe(df.head(10), use_container_width=True)
    else:
        sheets = list_sheets_xlsx(file_bytes)
        with st.form("excel_ingest_form", clear_on_submit=False):
            st.subheader("üìÅ Ch·ªçn sheet & c·ªôt (XLSX)")
            sheet = st.selectbox("Sheet", options=sheets, index=0)
            if len(sheets) > 1:
                st.caption("‚ö†Ô∏è Workbook c√≥ nhi·ªÅu sheet, h√£y ch·ªçn 1 sheet ƒë·ªÉ thao t√°c.")
            adv = st.expander("‚öôÔ∏è Tu·ª≥ ch·ªçn dtype=‚Ä¶ (khuy√™n d√πng cho file l·ªõn)")
            with adv:
                st.caption("Ch·ªçn ki·ªÉu d·ªØ li·ªáu ƒë·ªÉ gi·∫£m chi ph√≠ suy lu·∫≠n. B·ªè tr·ªëng n·∫øu ch∆∞a ch·∫Øc.")
                dtype_choice = st.text_area(
                    "Khai b√°o dtype d·∫°ng JSON (vd: {\"Amount\":\"float64\",\"Branch\":\"string\"})"
                    value="",
                    height=80,
                )
            submitted = st.form_submit_button("L·∫•y header & ch·ªçn c·ªôt")
        if not submitted:
            return

        dtype_map = None
        if dtype_choice.strip():
            with contextlib.suppress(Exception):
                dtype_map = json.loads(dtype_choice)

        headers = get_headers_xlsx(file_bytes, sheet_name=sheet, dtype_map=dtype_map)
        sel_cols = st.multiselect("Ch·ªçn c·ªôt c·∫ßn n·∫°p", options=headers, default=headers)
        if len(sel_cols) == 0:
            st.warning("H√£y ch·ªçn √≠t nh·∫•t 1 c·ªôt.")
            return

        df_preview = read_selected_columns_xlsx(
            file_bytes, sheet_name=sheet, usecols=sel_cols, nrows=100, dtype_map=dtype_map
        )
        st.subheader("üëÄ Preview 100 d√≤ng")
        st.dataframe(df_preview, use_container_width=True)

        c1, c2, c3 = st.columns([1,1,1])
        with c1:
            load_full = st.button("üì• N·∫°p full d·ªØ li·ªáu (c·ªôt ƒë√£ ch·ªçn)")
        with c2:
            to_parquet = st.button("üíæ Save as Parquet (c·ªôt ƒë√£ ch·ªçn)")
        with c3:
            big_mode = st.toggle("üöÄ Big‚Äëdata mode (experimental)")

        df = None
        if load_full:
            df = read_selected_columns_xlsx(
                file_bytes, sheet_name=sheet, usecols=sel_cols, nrows=None, dtype_map=dtype_map
            )
            st.success(f"ƒê√£ n·∫°p full {len(df):,} d√≤ng v·ªõi {len(sel_cols)} c·ªôt.")

        if to_parquet:
            try:
                if df is None:
                    df = read_selected_columns_xlsx(
                        file_bytes, sheet_name=sheet, usecols=sel_cols, nrows=None, dtype_map=dtype_map
                    )
                buf = io.BytesIO()
                df.to_parquet(buf, index=False)
                st.download_button(
                    "‚¨áÔ∏è T·∫£i Parquet",
                    data=buf.getvalue(),
                    file_name=f"{uploaded.name.rsplit('.',1)[0]}__{sheet}.parquet",
                    mime="application/octet-stream",
                )
                st.info("L·∫ßn sau ƒë·ªçc Parquet s·∫Ω r·∫•t nhanh (c·ªôt-ho√°, n√©n t·ªët).")
            except Exception as e:
                st.warning(f"Kh√¥ng th·ªÉ ghi Parquet (c·∫ßn pyarrow/fastparquet). L·ªói: {e}")

        if big_mode:
            st.info("Big‚Äëdata mode b·∫≠t: ∆∞u ti√™n Polars/DuckDB cho CSV/Parquet. V·ªõi XLSX, h√£y Save as Parquet r·ªìi n·∫°p l·∫°i.")

        if df is None:
            df = df_preview.copy()

    # Chu·∫©n b·ªã danh s√°ch c·ªôt theo ki·ªÉu
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()

    tabs = st.tabs(["Data Quality", "Profiling", "Sampling & Size", "Stat Tests", "Insights"])
    FACTS = {}

    # === TAB 1 ===
    with tabs[0]:
        if not MOD_DATA:
            st.info("Module ƒëang t·∫Øt trong Sidebar.")
        else:
            rep, n_dupes = quality_report(df)
            st.markdown("### üß™ Data Quality Report")
            st.write(f"üîÅ B·∫£n ghi tr√πng l·∫∑p: **{n_dupes}**")
            st.dataframe(rep, use_container_width=True)
            if (rep["mixed_types"]).any():
                st.warning("‚ö†Ô∏è Ph√°t hi·ªán c·ªôt **mixed types**. H√£y √©p ki·ªÉu tr∆∞·ªõc khi ph√¢n t√≠ch.")
            if (rep["missing_ratio"]>0.2).any():
                st.warning("‚ö†Ô∏è M·ªôt s·ªë c·ªôt thi·∫øu >20%. C·∫ßn x·ª≠ l√Ω tr∆∞·ªõc khi test/h·ªìi quy.")
            with st.expander("üßπ Chu·∫©n ho√° s·ªë & L·ªçc"):
                to_cast = st.multiselect("Ch·ªçn c·ªôt c·∫ßn √©p ki·ªÉu numeric", options=cat_cols)
                dec = st.selectbox("D·∫•u th·∫≠p ph√¢n", [".", ","], index=0, key="dec")
                thou = st.selectbox("NgƒÉn c√°ch ngh√¨n", [None, ",", "."], index=0, key="thou")
                strip_curr = st.checkbox("B·ªè k√Ω hi·ªáu ti·ªÅn t·ªá/k√Ω t·ª±", True)
                if st.button("√Åp d·ª•ng √©p ki·ªÉu"):
                    for c in to_cast:
                        df[c] = parse_numeric(df[c], decimal=dec, thousands=thou, strip_currency=strip_curr)
                    st.success("ƒê√£ √©p ki·ªÉu numeric.")
                numeric_cols[:] = df.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols[:] = df.select_dtypes(include=["object", "category"]).columns.tolist()
                keep_cols = st.multiselect("Gi·ªØ l·∫°i c·ªôt", options=df.columns.tolist(), default=df.columns.tolist())
                if set(keep_cols) != set(df.columns):
                    df = df[keep_cols]
                    st.success("ƒê√£ l·ªçc c·ªôt.")
                numeric_cols[:] = df.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols[:] = df.select_dtypes(include=["object", "category"]).columns.tolist()

    # === TAB 2 ===
    with tabs[1]:
        if not MOD_PROFILE:
            st.info("Module ƒëang t·∫Øt trong Sidebar.")
        else:
            st.markdown("### üìà Descriptive & Distribution")
            if len(numeric_cols)==0:
                st.info("Kh√¥ng c√≥ c·ªôt numeric.")
            else:
                col = st.selectbox("C·ªôt numeric", numeric_cols, key="prof_col")
                s = df[col].dropna()
                desc = s.describe().to_frame().T
                qs = quick_stats_series(s)
                desc["skew"] = qs["skew"]
                desc["kurtosis_fisher"] = qs["kurt"]
                st.dataframe(desc, use_container_width=True)
                FACTS["missing_ratio"] = float(df[col].isna().mean())
                ns = normality_summary(s)
                FACTS["n"] = ns["n"]; FACTS["shapiro_p"] = ns["shapiro_p"]
                plot_distribution_with_overlay(s, kde_max_n=50_000, max_points=100_000)
                st.caption("KDE t·ª± t·∫Øt khi n l·ªõn ƒë·ªÉ tƒÉng t·ªëc; overlay Avg/P50/Mode/¬±1œÉ.")

    # === TAB 3 ===
    with tabs[2]:
        if not MOD_SAMPLING:
            st.info("Module ƒëang t·∫Øt trong Sidebar.")
        else:
            st.markdown("### üßÆ Sample Size Calculators (FPC)")
            c1, c2 = st.columns(2)
            with c1:
                st.subheader("Proportion")
                N = st.number_input("Population size (optional)", value=0, min_value=0, step=1)
                conf = st.selectbox("Confidence", [90,95,99], index=1)
                z = {90:1.645,95:1.96,99:2.576}[conf]
                e = st.number_input("Margin of error (¬±)", value=0.05, min_value=0.0001, max_value=0.5, step=0.01)
                p0 = st.slider("Expected proportion p", 0.05, 0.95, 0.5, 0.05)
                n_prop = sample_size_proportion(p=p0, z=z, e=e, N=(N if N>0 else None))
                st.success(f"Sample size (proportion): **{n_prop}**")
            with c2:
                st.subheader("Mean")
                sigma = st.number_input("∆Ø·ªõc l∆∞·ª£ng sd (œÉ)", value=1.0, min_value=0.0001)
                e_m = st.number_input("Sai s·ªë cho mean (¬±)", value=1.0, min_value=0.0001)
                n_mean = sample_size_mean(sigma=sigma, z=z, e=e_m, N=(N if N>0 else None))
                st.success(f"Sample size (mean): **{n_mean}**")

    # === TAB 4 ===
    with tabs[3]:
        if not MOD_TESTS:
            st.info("Module ƒëang t·∫Øt trong Sidebar.")
        else:
            st.markdown("### üß™ Normality & Variance")
            if len(numeric_cols)==0:
                st.info("Kh√¥ng c√≥ c·ªôt numeric.")
            else:
                y_col = st.selectbox("Bi·∫øn numeric (target)", numeric_cols, key="y_col")
                grp = st.selectbox("Bi·∫øn nh√≥m (categorical, optional)", ["(None)"]+cat_cols, key="grp_col")
                y = df[y_col].dropna()
                if grp != "(None)":
                    groups = [d[y_col].dropna().values for _, d in df.groupby(grp)]
                    if len(groups)>=2:
                        _, lv_p = levene_equal_var(*groups)
                        st.write(f"Levene p = {lv_p:.4g} (p‚â•0.05 ‚áí ph∆∞∆°ng sai ~ b·∫±ng)")
                        FACTS["levene_p"] = float(lv_p)

                st.markdown("---")
                st.markdown("### üîÄ Group Comparisons")
                if len(numeric_cols)>=1 and grp!="(None)" and len(df[grp].dropna().unique())>=2:
                    unique_groups = df[grp].dropna().unique().tolist()
                    n_groups = len(unique_groups)
                    if n_groups==2:
                        g1, g2 = unique_groups[:2]
                        x = df[df[grp]==g1][y_col].dropna().values
                        z = df[df[grp]==g2][y_col].dropna().values
                        use_welch = st.checkbox("D√πng Welch t-test (ph∆∞∆°ng sai kh√¥ng b·∫±ng nhau)", value=(FACTS.get("levene_p",1)<0.05))
                        if st.button("Run t-test"):
                            t_stat, p_val = stats.ttest_ind(x, z, equal_var=not use_welch)
                            d = cohen_d(x, z); _g = hedges_g(x, z)
                            st.write(f"{'Welch' if use_welch else 'Student'} t: t={t_stat:.3f}, p={p_val:.4g}  |  Cohen‚Äôs d={d:.3f}")
                            FACTS["cohen_d"] = float(abs(d))
                        if st.button("Run Mann‚ÄìWhitney (phi tham s·ªë)"):
                            u_stat, p_val = stats.mannwhitneyu(x, z, alternative="two-sided")
                            st.write(f"Mann‚ÄìWhitney: U={u_stat:.3f}, p={p_val:.4g} (thay th·∫ø khi ph√¢n ph·ªëi kh√¥ng chu·∫©n)")
                    elif n_groups>=3:
                        data_groups = [df[df[grp]==g][y_col].dropna().values for g in unique_groups]
                        if st.button("Run ANOVA"):
                            f_stat, p_val = stats.f_oneway(*data_groups)
                            eta2, omega2 = eta_omega(data_groups)
                            st.write(f"ANOVA: F={f_stat:.3f}, p={p_val:.4g}  |  eta¬≤={eta2:.3f}")
                            FACTS["eta2"] = float(eta2)
                        if st.button("Run Welch ANOVA"):
                            try:
                                from statsmodels.stats.oneway import anova_oneway
                                welch = anova_oneway(data_groups, use_var="unequal", welch_corrections=True)
                                st.write(f"Welch ANOVA: p={welch.pvalue:.4g} (khuy·∫øn ngh·ªã khi var‚â†)")
                            except Exception:
                                st.warning("C·∫ßn statsmodels>=0.13.")
                        if st.button("Run Kruskal‚ÄìWallis"):
                            kw = stats.kruskal(*data_groups)
                            st.write(f"Kruskal‚ÄìWallis: H={kw.statistic:.3f}, p={kw.pvalue:.4g} (phi tham s·ªë cho ‚â•3 nh√≥m)")
                else:
                    st.caption("Ch·ªçn bi·∫øn nh√≥m (‚â•2 m·ª©c) ƒë·ªÉ b·∫≠t so s√°nh nh√≥m.")

                st.markdown("---")
                st.markdown("### üîó Correlation")
                if len(numeric_cols)>=2:
                    c1 = st.selectbox("X", numeric_cols, key="corr_x")
                    c2 = st.selectbox("Y", [c for c in numeric_cols if c!=c1], key="corr_y")
                    method = st.radio("Ph∆∞∆°ng ph√°p", ["Pearson","Spearman"], horizontal=True)
                    sub = df[[c1,c2]].dropna()
                    if len(sub)>=3:
                        if method=="Pearson":
                            r, p = stats.pearsonr(sub[c1], sub[c2]); why = "Tuy·∫øn t√≠nh, nh·∫°y ngo·∫°i l·ªá."
                        else:
                            r, p = stats.spearmanr(sub[c1], sub[c2]); why = "ƒê∆°n ƒëi·ªáu, b·ªÅn v·ªØng."
                        st.write(f"n={len(sub)}  |  r={r:.3f}, p={p:.4g} ({why})")
                        FACTS["corr_r"] = float(abs(r))
                    fig, ax = plt.subplots(1,1, figsize=(6,4))
                    if method=="Pearson":
                        sns.regplot(x=c1, y=c2, data=sub, scatter_kws={'alpha':0.5,'s':20}, line_kws={'color':'#F6AE2D'}, ax=ax)
                    else:
                        sns.scatterplot(x=c1, y=c2, data=sub, alpha=0.6, ax=ax, color=PALETTE[1])
                    ax.set_title(f"{method} correlation")
                    st.pyplot(fig, use_container_width=True)

                st.markdown("---")
                st.markdown("### üìö Regression (Linear)")
                if len(numeric_cols)>=2:
                    y_t = st.selectbox("Target (y)", numeric_cols, key="reg_y")
                    X_t = st.multiselect("Features (X)", [c for c in numeric_cols if c!=y_t], default=[c for c in numeric_cols if c!=y_t][:2])
                    test_size = st.slider("Test size", 0.1, 0.5, 0.25, 0.05)
                    if st.button("Run Linear Regression"):
                        sub = df[[y_t]+X_t].dropna()
                        if len(sub) < (len(X_t)+5):
                            st.error("Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi lo·∫°i missing.")
                        else:
                            X = sub[X_t]; yv = sub[y_t]
                            Xtr, Xte, ytr, yte = train_test_split(X, yv, test_size=test_size, random_state=RANDOM_SEED)
                            mdl = LinearRegression().fit(Xtr, ytr)
                            yhat = mdl.predict(Xte)
                            r2 = r2_score(yte, yhat)
                            adj = 1 - (1-r2)*(len(yte)-1)/(len(yte)-Xte.shape[1]-1)
                            rmse = float(np.sqrt(mean_squared_error(yte, yhat)))
                            vifs = calc_vif(Xtr)
                            st.write({"R2": round(r2,3), "Adj_R2": round(adj,3), "RMSE": round(rmse,3)})
                            st.write("VIF:", {k: round(v,3) for k,v in vifs.items()})
                            fig, axs = plt.subplots(1,2, figsize=(12,4))
                            resid = yte - yhat
                            sns.scatterplot(x=yhat, y=resid, ax=axs[0], color=PALETTE[0])
                            axs[0].axhline(0,color='r',ls='--'); axs[0].set_title("Residuals vs Fitted")
                            sns.histplot(resid, kde=(len(resid)<=50_000), ax=axs[1], color=PALETTE[2])
                            axs[1].set_title("Residuals")
                            st.pyplot(fig, use_container_width=True)

    # === TAB 5 ===
    with tabs[4]:
        if not MOD_INSIGHTS:
            st.info("Module ƒëang t·∫Øt trong Sidebar.")
        else:
            st.markdown("### üß† Insights & Khuy·∫øn c√°o (t·ª± ƒë·ªông)")
            hits = score_insights(FACTS)
            if len(hits)==0:
                st.success("Kh√¥ng c√≥ c·∫£nh b√°o ƒë√°ng ch√∫ √Ω d·ª±a tr√™n c√°c ch·ªâ s·ªë hi·ªán t·∫°i.")
            else:
                for h in hits:
                    box = st.info if h["severity"]=="info" else (st.warning if h["severity"]=="caution" else st.error)
                    ref_note = f" ‚Ä¢ Ref: {h['ref']} ({h['ref_id']})"
                    try:
                        val_fmt = f"{h['value']:.4g}" if isinstance(h['value'], (int,float,np.floating)) else str(h['value'])
                    except Exception:
                        val_fmt = str(h['value'])
                    box(f"**[{h['severity'].upper()}]** `{h['metric']}` = {val_fmt} ‚Üí {h['message']}{ref_note}")

    # ---------------------------- EXPORT ----------------------------
    st.markdown("---")
    colA, colB = st.columns(2)
    with colA:
        params = {"app":"Audit Stats v2.2","time":datetime.now().isoformat(),"file":uploaded.name,
                  "facts":FACTS,"libs":{"numpy":np.__version__,"pandas":pd.__version__}}
        st.download_button("üßæ Download audit log (JSON)",
            data=json.dumps(params, ensure_ascii=False, indent=2).encode("utf-8"),
            file_name=f"audit_log_{int(time.time())}.json")
    with colB:
        try:
            desc_all = df.select_dtypes(include=[np.number]).describe().T.reset_index()
            buf = io.BytesIO()
            with pd.ExcelWriter(buf, engine="openpyxl") as w:
                desc_all.to_excel(w, index=False, sheet_name="Descriptive")
            st.download_button("üíΩ Download Descriptive (Excel)", data=buf.getvalue(), file_name="descriptive.xlsx")
        except Exception:
            pass


if __name__ == "__main__":
    main()
