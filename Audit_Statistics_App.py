from __future__ import annotations
import os, io, re, json, time, hashlib, contextlib, tempfile, warnings
from datetime import datetime
from typing import Optional, List, Callable, Dict, Any
import numpy as np
import pandas as pd
import streamlit as st
import inspect  # added for inspect.signature

def require_full_data(banner='Ch∆∞a c√≥ d·ªØ li·ªáu FULL. H√£y d√πng **Load full data** tr∆∞·ªõc khi ch·∫°y tab n√†y.'):
    df = SS.get('df')
    import pandas as pd
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        st.info(banner); st.stop()
    return df
    
def _first_match(colnames, patterns):
    cols = [c for c in colnames]
    low = {c: str(c).lower() for c in cols}
    for p in patterns:
        p = p.lower()
        for c in cols:
            if p in low[c]:
                return c
    return None

def _decode_bytes_to_str(v):
    if isinstance(v, (bytes, bytearray)):
        for enc in ('utf-8','latin-1','cp1252'):
            try:
                return v.decode(enc, errors='ignore')
            except Exception:
                pass
        try:
            return v.hex()
        except Exception:
            return str(v)
    return v

def sanitize_for_arrow(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame):
        return df
    df = df.copy()
    obj_cols = df.select_dtypes(include=['object']).columns
    for c in obj_cols:
        col = df[c]
        if col.isna().all():
            continue
        # bytes -> str
        if col.map(lambda v: isinstance(v, (bytes, bytearray))).any():
            df[c] = col.map(_decode_bytes_to_str)
            col = df[c]
        try:
            sample = col.dropna().iloc[:1000]
        except Exception:
            sample = col.dropna()
        has_str = any(isinstance(x, str) for x in sample)
        has_num = any(isinstance(x, (int,float,np.integer,np.floating)) for x in sample)
        has_nested = any(isinstance(x, (dict,list,set,tuple)) for x in sample)
        if has_nested or (has_str and has_num):
            df[c] = col.astype(str)
    return df

# ---- Streamlit width compatibility wrappers ----
try:
    _df_params = inspect.signature(st.dataframe).parameters
    _df_supports_width = 'width' in _df_params
except Exception:
    _df_supports_width = False



def st_df(data=None, **kwargs):
    # Normalize width/use_container_width to avoid 'str' passed to Streamlit
    width = kwargs.pop('width', None)
    ucw = kwargs.pop('use_container_width', None)

    # width must be int (pixels); ignore strings like 'stretch'
    if isinstance(width, str):
        try:
            width = int(width)
        except Exception:
            width = None

    if width is not None and _df_supports_width:
        kwargs['width'] = int(width)
        if ucw is not None:
            kwargs['use_container_width'] = bool(ucw)
    else:
        # Fallback to use_container_width
        kwargs['use_container_width'] = True if ucw is None else bool(ucw)

    return st.dataframe(data, **kwargs)  # Kh√¥ng g·ªçi l·∫°i st_df
# ====================== PATCH START: Export Capture Proxies ======================
# ƒê·∫£m b·∫£o SS ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr∆∞·ªõc khi s·ª≠ d·ª•ng
SS = st.session_state
# L∆∞u ‚Äúb·∫£n g·ªëc‚Äù c·ªßa widget Streamlit l·∫ßn ƒë·∫ßu (ƒë·ªÉ tr√°nh wrap l·∫∑p g√¢y ƒë·ªá quy)
if '_orig_plotly_chart' not in SS:
    SS['_orig_plotly_chart'] = st.plotly_chart
if '_orig_dataframe' not in SS:
    SS['_orig_dataframe'] = st.dataframe
if '_orig_table' not in SS:
    SS['_orig_table'] = st.table

# ====================== PATCH END: Export Capture Proxies ======================

from scipy import stats

warnings.filterwarnings('ignore')

# Optional deps
try:
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    HAS_PLOTLY = True
except Exception:
    HAS_PLOTLY = False
try:
    import plotly.io as pio
    HAS_KALEIDO = True
except Exception:
    HAS_KALEIDO = False
try:
    import docx
    from docx.shared import Inches
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False
try:
    import fitz  # PyMuPDF
    HAS_PDF = True
except Exception:
    HAS_PDF = False
try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    HAS_PYARROW = True
except Exception:
    HAS_PYARROW = False
try:
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.metrics import (
        r2_score, mean_squared_error, accuracy_score, roc_auc_score, roc_curve
    )
    HAS_SK = True
except Exception:
    HAS_SK = False

# --------------------------------- App Config ---------------------------------
st.set_page_config(page_title='Audit Statistics', layout='wide', initial_sidebar_state='expanded')
SS = st.session_state

DEFAULTS = {
    'bins': 50,
    'log_scale': False,
    'kde_threshold': 150_000,
    'risk_diff_threshold': 0.05,
    'advanced_visuals': False,
    'use_parquet_cache': False,
    'pv_n': 100,
    'df': None,
 'last_good_df': None,
    'df_preview': None,
 'last_good_preview': None,
    'file_bytes': None,
 'ingest_ready': False,
    'sha12': '',
    'uploaded_name': '',
        'xlsx_sheet': '',
    'header_row': 1,
    'skip_top': 0,
 'col_whitelist': None,
}
for k, v in DEFAULTS.items():
    SS.setdefault(k, v)
SS.setdefault('_export_registry', {})
# ------------------------------- Small Utilities ------------------------------
SS = st.session_state
if not isinstance(SS.get('_plt_seq'), int):
    SS['_plt_seq'] = 0
def file_sha12(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()[:12]

def st_plotly(fig, **kwargs):
    # ƒê·∫£m b·∫£o b·ªô ƒë·∫øm lu√¥n l√† int
    seq = SS.get('_plt_seq')
    if not isinstance(seq, int):
        seq = 0
    seq += 1
    SS['_plt_seq'] = seq

    kwargs.setdefault('use_container_width', True)
    kwargs.setdefault('config', {'displaylogo': False})
    kwargs.setdefault('key', f'plt_{seq}')

    # N·∫øu plotly s·∫µn s√†ng th√¨ v·∫Ω; n·∫øu kh√¥ng, th√¥ng b√°o nh·∫π nh√†ng
    try:
        return st.plotly_chart(fig, **kwargs)
    except Exception as e:
        st.warning(f"Kh√¥ng render ƒë∆∞·ª£c Plotly chart: {e}")
        # (Tu·ª≥ ch·ªçn) c√≥ th·ªÉ th√™m fallback matplotlib ·ªü ƒë√¢y n·∫øu b·∫°n mu·ªën
        return None

@st.cache_data(ttl=900, show_spinner=False, max_entries=64)
def corr_cached(df: pd.DataFrame, cols: List[str], method: str = 'pearson') -> pd.DataFrame:
    if not cols: return pd.DataFrame()
    sub = df[cols].apply(pd.to_numeric, errors='coerce')
    sub = sub.dropna(axis=1, how='all')
    nunique = sub.nunique(dropna=True)
    keep = [c for c in sub.columns if nunique.get(c, 0) > 1]
    sub = sub[keep]
    if sub.shape[1] < 2: return pd.DataFrame()
    return sub.corr(method=method)

def is_datetime_like(colname: str, s: pd.Series) -> bool:
    return pd.api.types.is_datetime64_any_dtype(s) or bool(re.search(r'(date|time)', str(colname), re.I))

def _downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:
    for c in df.select_dtypes(include=['float64']).columns:
        df[c] = pd.to_numeric(df[c], downcast='float')
    for c in df.select_dtypes(include=['int64']).columns:
        df[c] = pd.to_numeric(df[c], downcast='integer')
    return df

def to_float(x) -> Optional[float]:
    from numbers import Real
    try:
        if isinstance(x, Real): return float(x)
        if x is None: return None
        return float(str(x).strip().replace(',', ''))
    except Exception:
        return None

# ------------------------------- Disk Cache I/O --------------------------------
def _parquet_cache_path(sha: str, key: str) -> str:
    return os.path.join(tempfile.gettempdir(), f'astats_cache_{sha}_{key}.parquet')

@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=24)
def write_parquet_cache(df: pd.DataFrame, sha: str, key: str) -> str:
    if not HAS_PYARROW: return ''
    try:
        df = sanitize_for_arrow(df)
        table = pa.Table.from_pandas(df)
        path = _parquet_cache_path(sha, key)
        pq.write_table(table, path)
        return path
    except Exception:
        return ''

def read_parquet_cache(sha: str, key: str) -> Optional[pd.DataFrame]:
    if not HAS_PYARROW: return None
    path = _parquet_cache_path(sha, key)
    if os.path.exists(path):
        try:
            return pq.read_table(path).to_pandas()
        except Exception:
            return None
    return None

# ------------------------------- Fast Readers ---------------------------------
@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=16)
def list_sheets_xlsx(file_bytes: bytes) -> List[str]:
    from openpyxl import load_workbook
    wb = load_workbook(io.BytesIO(file_bytes), read_only=True, data_only=True)
    try:
        return wb.sheetnames
    finally:
        wb.close()

@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=16)
def read_csv_fast(file_bytes: bytes, usecols=None) -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    try:
        df = pd.read_csv(bio, usecols=usecols, engine='pyarrow')
    except Exception:
        bio.seek(0)
        df = pd.read_csv(bio, usecols=usecols, low_memory=False, memory_map=True)
    return _downcast_numeric(df)

@st.cache_data(ttl=6*3600, show_spinner=False, max_entries=16)
def read_xlsx_fast(file_bytes: bytes, sheet: str, usecols=None,
                   header_row: int = 1, skip_top: int = 0, dtype_map=None) -> pd.DataFrame:
    # --- sanitize input ƒë·ªÉ tr√°nh None g√¢y l·ªói so s√°nh/s·ªë h·ªçc ---
    header_row = 1 if header_row in (None, 0, '', False) else int(header_row)
    skip_top   = 0 if skip_top   in (None, '', False)   else int(skip_top)

    # --- FAST PATH: DuckDB excel extension (r·∫•t nhanh n·∫øu s·∫µn c√≥) ---
    if 'HAS_DUCKDB' in globals() and HAS_DUCKDB:
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx")
        try:
            tmp.write(file_bytes); tmp.flush(); tmp.close()
            con = duckdb.connect()
            # excel ext c√≥ th·ªÉ ƒë√£ c√≥ s·∫µn; n·∫øu kh√¥ng, nh·∫£y qua except v√† fallback
            try:
                con.execute("INSTALL excel; LOAD excel;")
            except Exception:
                pass

            # ƒê·ªçc to√†n sheet v·ªÅ Arrow/Pandas
            q = f"SELECT * FROM read_excel('{tmp.name}', sheet='{sheet}')"
            try:
                # ∆∞u ti√™n l·∫•y Arrow table n·∫øu pyarrow s·∫µn ƒë·ªÉ convert nhanh
                import pyarrow as pa
                table = con.execute(q).arrow()
                pdf = table.to_pandas(types_mapper=pd.ArrowDtype) if hasattr(pd, 'ArrowDtype') else table.to_pandas()
            except Exception:
                pdf = con.execute(q).df()

            # M√¥ ph·ªèng h√†nh vi header/skip nh∆∞ pandas.read_excel
            header_idx = max(header_row - 1, 0)
            # b·∫£o v·ªá khi sheet tr·ªëng ho·∫∑c ch·ªâ c√≥ v√†i d√≤ng
            if len(pdf) == 0:
                return _downcast_numeric(pdf)

            pdf.columns = pdf.iloc[header_idx].astype(str).tolist()
            start_data = header_idx + 1 + max(skip_top, 0)
            pdf = pdf.iloc[start_data:].reset_index(drop=True)

            # Ch·ªçn c·ªôt n·∫øu usecols ƒë∆∞·ª£c truy·ªÅn
            if usecols:
                cols_sel = [c for c in usecols if c in pdf.columns]
                if cols_sel:
                    pdf = pdf[cols_sel]

            # √Åp dtype_map n·∫øu c√≥
            if dtype_map:
                for c, t in dtype_map.items():
                    if c in pdf.columns:
                        with contextlib.suppress(Exception):
                            pdf[c] = pdf[c].astype(t)

            return _downcast_numeric(pdf)
        finally:
            with contextlib.suppress(Exception):
                os.unlink(tmp.name)

    # --- FALLBACK: pandas + openpyxl (nh∆∞ c≈©) ---
    skiprows = list(range(header_row, header_row + skip_top)) if skip_top > 0 else None
    bio = io.BytesIO(file_bytes)
    df = pd.read_excel(
        bio, sheet_name=sheet, usecols=usecols,
        header=header_row - 1, skiprows=skiprows,
        dtype=dtype_map, engine='openpyxl'
    )
    return _downcast_numeric(df)


# ----------------------------- Cached Basic Stats -----------------------------
@st.cache_data(ttl=1800, show_spinner=False, max_entries=64)
def numeric_profile_stats(series: pd.Series):
    s = pd.to_numeric(series, errors='coerce').replace([np.inf, -np.inf], np.nan).dropna()
    desc = s.describe(percentiles=[0.01,0.05,0.10,0.25,0.5,0.75,0.90,0.95,0.99])
    skew = float(stats.skew(s)) if len(s) > 2 else np.nan
    kurt = float(stats.kurtosis(s, fisher=True)) if len(s) > 3 else np.nan
    try:
        p_norm = float(stats.normaltest(s)[1]) if len(s) > 7 else np.nan
    except Exception:
        p_norm = np.nan
    p95 = s.quantile(0.95) if len(s)>1 else np.nan
    p99 = s.quantile(0.99) if len(s)>1 else np.nan
    zero_ratio = float((s==0).mean()) if len(s)>0 else np.nan
    return desc.to_dict(), skew, kurt, p_norm, float(p95), float(p99), zero_ratio

@st.cache_data(ttl=1800, show_spinner=False, max_entries=64)
def cat_freq(series: pd.Series) -> pd.DataFrame:
    s = series.dropna().astype(str)
    vc = s.value_counts(dropna=True)
    out = pd.DataFrame({'category': vc.index, 'count': vc.values})
    out['share'] = out['count']/out['count'].sum()
    return out


# ------------------------------ Benford Helpers -------------------------------
@st.cache_data(ttl=3600, show_spinner=False, max_entries=64)
def _benford_1d(series: pd.Series):
    s = pd.to_numeric(series, errors='coerce').replace([np.inf, -np.inf], np.nan).dropna().abs()
    if s.empty: return None
    def _digits(x):
        xs = ("%.15g" % float(x))
        return re.sub(r"[^0-9]", "", xs).lstrip("0")
    d1 = s.apply(lambda v: int(_digits(v)[0]) if len(_digits(v))>=1 else np.nan).dropna()
    d1 = d1[(d1>=1)&(d1<=9)]
    if d1.empty: return None
    obs = d1.value_counts().sort_index().reindex(range(1,10), fill_value=0).astype(float)
    n=obs.sum(); obs_p=obs/n
    idx=np.arange(1,10); exp_p=np.log10(1+1/idx); exp=exp_p*n
    with np.errstate(divide='ignore', invalid='ignore'):
        chi2=np.nansum((obs-exp)**2/exp)
        pval=1-stats.chi2.cdf(chi2, len(idx)-1)
        mad=float(np.mean(np.abs(obs_p-exp_p)))
        var_tbl=pd.DataFrame({'digit':idx,'expected':exp,'observed':obs.values})
        var_tbl['diff']=var_tbl['observed']-var_tbl['expected']
        var_tbl['diff_pct']=(var_tbl['observed']-var_tbl['expected'])/var_tbl['expected']
        table=pd.DataFrame({'digit':idx,'observed_p':obs_p.values,'expected_p':exp_p})
    return {'table':table, 'variance':var_tbl, 'n':int(n), 'chi2':float(chi2), 'p':float(pval), 'MAD':float(mad)}

@st.cache_data(ttl=3600, show_spinner=False, max_entries=64)
def _benford_2d(series: pd.Series):
    s = pd.to_numeric(series, errors='coerce').replace([np.inf, -np.inf], np.nan).dropna().abs()
    if s.empty: return None
    def _digits(x):
        xs = ("%.15g" % float(x))
        return re.sub(r"[^0-9]", "", xs).lstrip("0")
    def _first2(v):
        ds = _digits(v)
        if len(ds)>=2: return int(ds[:2])
        if len(ds)==1 and ds!="0": return int(ds)
        return np.nan
    d2 = s.apply(_first2).dropna(); d2=d2[(d2>=10)&(d2<=99)]
    if d2.empty: return None
    obs = d2.value_counts().sort_index().reindex(range(10,100), fill_value=0).astype(float)
    n=obs.sum(); obs_p=obs/n
    idx=np.arange(10,100); exp_p=np.log10(1+1/idx); exp=exp_p*n
    with np.errstate(divide='ignore', invalid='ignore'):
        chi2=np.nansum((obs-exp)**2/exp)
        pval=1-stats.chi2.cdf(chi2, len(idx)-1)
    mad=float(np.mean(np.abs(obs_p-exp_p)))
    var_tbl=pd.DataFrame({'digit':idx,'expected':exp,'observed':obs.values})
    var_tbl['diff']=var_tbl['observed']-var_tbl['expected']
    var_tbl['diff_pct']=(var_tbl['observed']-var_tbl['expected'])/var_tbl['expected']
    table=pd.DataFrame({'digit':idx,'observed_p':obs_p.values,'expected_p':exp_p})
    return {'table':table, 'variance':var_tbl, 'n':int(n), 'chi2':float(chi2), 'p':float(pval), 'MAD':float(mad)}

def _benford_ready(series: pd.Series) -> tuple[bool, str]:
    s = pd.to_numeric(series, errors='coerce')
    n_nz = int((s != 0).sum())  # nh·∫≠n c·∫£ s·ªë √¢m, ch·ªâ lo·∫°i 0
    if n_nz < 1:
        return False, f"Kh√¥ng c√≥ gi√° tr·ªã ‚â† 0 ƒë·ªÉ ch·∫°y Benford (hi·ªán {n_nz}, c·∫ßn ‚â•300)."
    s_non = s.dropna()
    if s_non.shape[0] > 0:
        ratio_unique = s_non.nunique()/s_non.shape[0]
        if ratio_unique > 0.95:
            return False, "T·ªâ l·ªá unique qu√° cao (kh·∫£ nƒÉng ID/Code) ‚Äî tr√°nh Benford."
    return True, ''

def _plot(fig):
    try:
        st_plotly(fig)
    except Exception:
        st.plotly_chart(fig, use_container_width=True)

def guess_datetime_cols(df, check=3000):
    import numpy as np, pandas as pd
    sample = df.head(check)
    cols = []
    for c in df.columns:
        try:
            if np.issubdtype(df[c].dtype, np.datetime64):
                cols.append(c); continue
            if df[c].dtype == 'object':
                s = pd.to_datetime(sample[c], errors='coerce')
                if s.notna().mean() >= 0.5:
                    cols.append(c)
        except Exception:
            pass
    return cols

# -------------------------- Sidebar: Workflow & perf ---------------------------
up = st.file_uploader('Upload file (.csv, .xlsx)', type=['csv','xlsx'], key='ingest')
if up is not None:
    fb = up.read()  # c√≥ th·ªÉ d√πng up.getvalue() c≈©ng ƒë∆∞·ª£c
    new_sha = file_sha12(fb)
    same_file = (SS.get('sha12') == new_sha) and (SS.get('uploaded_name') == up.name)

    # lu√¥n c·∫≠p nh·∫≠t metadata/bytes ƒë·ªÉ c√°c b∆∞·ªõc sau d√πng
    SS['file_bytes'] = fb
    SS['uploaded_name'] = up.name
    SS['sha12'] = new_sha

    # üîí CH·ªà khi ƒë·ªïi file m·ªõi reset preview/full
    if not same_file:
        SS['df'] = None
        SS['df_preview'] = None

    st.caption(f"ƒê√£ nh·∫≠n file: {up.name} ‚Ä¢ SHA12={SS['sha12']}")

    if st.button('Clear file', key='btn_clear_file'):
        base_keys = ['file_bytes','uploaded_name','sha12','df','df_preview','col_whitelist']
        result_keys = [
            'bf1_res','bf2_res','bf1_col','bf2_col','t4_results','last_corr','last_linear',
            'last_logistic','last_numeric_profile','last_gof','fraud_flags','spearman_recommended',
            '_plt_seq','col_filter','dtype_choice','xlsx_sheet','header_row','skip_top',
            'ingest_ready','last_good_df','last_good_preview'
        ]
        # ƒë·∫∑t t√™n bi·∫øn kh√°c nhau ƒë·ªÉ tr√°nh ƒë√® 'k'
        for bk in base_keys:
            SS[bk] = DEFAULTS.get(bk, None)

        for rk in result_keys:
            if rk in SS:
                SS[rk] = None

        st.rerun()
with st.sidebar.expander('3) Cache', expanded=False):
    if not HAS_PYARROW:
        try:
            import duckdb
            HAS_DUCKDB = True
        except Exception:
            HAS_DUCKDB = False
        st.caption('‚ö†Ô∏è PyArrow ch∆∞a s·∫µn s√†ng ‚Äî Disk cache (Parquet) s·∫Ω b·ªã t·∫Øt.')
        SS['use_parquet_cache'] = False
    SS['use_parquet_cache'] = st.checkbox('Disk cache (Parquet) for faster reloads', value=SS.get('use_parquet_cache', False) and HAS_PYARROW)
    if st.button('üßπ Clear cache'):
        st.cache_data.clear(); st.toast('Cache cleared', icon='üßπ')

# ---------------------------------- Main Gate ---------------------------------
st.title('üìä Audit Statistics')
if SS['file_bytes'] is None:
    st.info('Upload a file ƒë·ªÉ b·∫Øt ƒë·∫ßu.'); st.stop()

fname=SS['uploaded_name']; fb=SS['file_bytes']; sha=SS['sha12']
colL, colR = st.columns([3,2])
with colL:
    st.text_input('File', value=fname or '', disabled=True)
with colR:
    SS['pv_n'] = st.slider('Preview rows', 50, 500, SS.get('pv_n',100), 50)
    do_preview = st.button('üîé Quick preview', key='btn_prev')

# Ingest flow
if fname.lower().endswith('.csv'):
    if do_preview or SS['df_preview'] is None:
        try:
            SS['df_preview'] = sanitize_for_arrow(read_csv_fast(fb).head(SS['pv_n']))
            SS['last_good_preview'] = SS['df_preview']; SS['ingest_ready']=True
        except Exception as e:
            st.error(f'L·ªói ƒë·ªçc CSV: {e}'); SS['df_preview']=None
    if SS['df_preview'] is not None:
        st_df(SS['df_preview'], use_container_width=True, height=260)
        headers=list(SS['df_preview'].columns)
        selected = st.multiselect('Columns to load', headers, default=headers)
        SS['col_whitelist'] = selected if selected else headers
        if st.button('üì• Load full CSV with selected columns', key='btn_load_csv'):
            sel_key=';'.join(selected) if selected else 'ALL'
            key=f"csv_{hashlib.sha1(sel_key.encode()).hexdigest()[:10]}"
            df_cached = read_parquet_cache(sha, key) if SS['use_parquet_cache'] else None
            if df_cached is None:
                df_full = sanitize_for_arrow(read_csv_fast(fb, usecols=(selected or None)))
                if SS['use_parquet_cache']: write_parquet_cache(df_full, sha, key)
            else:
                df_full = df_cached
            SS['df']=df_full; SS['last_good_df']=df_full; SS['ingest_ready']=True; SS['col_whitelist']=list(df_full.columns)
            st.success(f"Loaded: {len(SS['df']):,} rows √ó {len(SS['df'].columns)} cols ‚Ä¢ SHA12={sha}")
else:
    sheets = list_sheets_xlsx(fb)
    with st.expander('üìÅ Select sheet & header (XLSX)', expanded=True):
        c1,c2,c3 = st.columns([2,1,1])
        idx=0 if sheets else 0
        SS['xlsx_sheet'] = c1.selectbox('Sheet', sheets, index=idx)
        SS['header_row'] = c2.number_input('Header row (1‚Äëbased)', 1, 100, SS['header_row'])
        SS['skip_top'] = c3.number_input('Skip N rows after header', 0, 1000, SS['skip_top'])
        SS['dtype_choice'] = st.text_area('dtype mapping (JSON, optional)', SS.get('dtype_choice',''), height=68)
        dtype_map = None
        if (SS.get('dtype_choice') or '').strip():
            try:
                dtype_map = json.loads(SS['dtype_choice'])
            except Exception as e:
                st.warning(f"Dtype mapping JSON kh√¥ng h·ª£p l·ªá: {e}")
        try:
            prev = sanitize_for_arrow(read_xlsx_fast(fb, SS['xlsx_sheet'], usecols=None, header_row=SS['header_row'], skip_top=SS['skip_top'], dtype_map=dtype_map).head(SS['pv_n']))
            SS['df_preview']=prev; SS['last_good_preview']=prev; SS['ingest_ready']=True
        except Exception as e:
            st.error(f'L·ªói ƒë·ªçc XLSX: {e}'); prev=pd.DataFrame()
        st_df(prev, use_container_width=True, height=260)
        headers=list(prev.columns)
        st.caption(f'Columns: {len(headers)} ‚Ä¢ SHA12={sha}')
        SS['col_filter'] = st.text_input('üîé Filter columns', SS.get('col_filter',''))
        filtered = [h for h in headers if SS['col_filter'].lower() in h.lower()] if SS['col_filter'] else headers
        selected = st.multiselect('üßÆ Columns to load', filtered if filtered else headers, default=filtered if filtered else headers)
        if st.button('üì• Load full data', key='btn_load_xlsx'):
            key_tuple=(SS['xlsx_sheet'], SS['header_row'], SS['skip_top'], tuple(selected) if selected else ('ALL',))
            key=f"xlsx_{hashlib.sha1(str(key_tuple).encode()).hexdigest()[:10]}"
            df_cached = read_parquet_cache(sha, key) if SS['use_parquet_cache'] else None
            if df_cached is None:
                df_full = sanitize_for_arrow(read_xlsx_fast(fb, SS['xlsx_sheet'], usecols=(selected or None), header_row=SS['header_row'], skip_top=SS['skip_top'], dtype_map=dtype_map))
                if SS['use_parquet_cache']: write_parquet_cache(df_full, sha, key)
            else:
                df_full = df_cached
            SS['df']=df_full; SS['last_good_df']=df_full; SS['ingest_ready']=True; SS['col_whitelist']=list(df_full.columns)
            st.success(f"Loaded: {len(SS['df']):,} rows √ó {len(SS['df'].columns)} cols ‚Ä¢ SHA12={sha}")

if SS['df'] is None and SS['df_preview'] is None:
    st.stop()

# Source & typing
DF_FULL = require_full_data('Ch∆∞a c√≥ d·ªØ li·ªáu. H√£y d√πng **Load full data**.')
DF_VIEW = DF_FULL  # alias ƒë·ªÉ kh√¥ng ph√° code c≈©

ALL_COLS = list(DF_FULL.columns)
DT_COLS  = [c for c in ALL_COLS if is_datetime_like(c, DF_FULL[c])]
NUM_COLS = DF_FULL[ALL_COLS].select_dtypes(include=[np.number]).columns.tolist()
CAT_COLS = DF_FULL[ALL_COLS].select_dtypes(include=['object','category','bool']).columns.tolist()
VIEW_COLS = [c for c in DF_FULL.columns if (not SS.get('col_whitelist') or c in SS['col_whitelist'])]


@st.cache_data(ttl=900, show_spinner=False, max_entries=64)
def spearman_flag(df: pd.DataFrame, cols: List[str]) -> bool:
    try:
        if df is None or not isinstance(df, pd.DataFrame):
            return False
    except Exception:
        return False

    for c in (cols or [])[:20]:
        if c not in df.columns:
            continue

        s = pd.to_numeric(df[c], errors='coerce').replace([np.inf, -np.inf], np.nan).dropna()
        if len(s) < 50:
            continue

        sk, ku, tail, p_norm = 0.0, 0.0, 0.0, 1.0  # defaults

        try:
            if len(s) > 2:
                sk = float(stats.skew(s))
        except Exception:
            pass

        try:
            if len(s) > 3:
                ku = float(stats.kurtosis(s, fisher=True))
        except Exception:
            pass

        try:
            p99 = s.quantile(0.99)
            if pd.notna(p99):
                tail = float((s > p99).mean())
        except Exception:
            pass

        try:
            if len(s) > 20:
                p_norm = float(stats.normaltest(s)[1])
        except Exception:
            pass

        if (abs(sk) > 1) or (abs(ku) > 3) or (tail > 0.02) or (p_norm < 0.05):
            return True
    return False
# ------------------------------ Rule Engine Core ------------------------------
class Rule:
    def __init__(self, id: str, name: str, scope: str, severity: str,
                 condition: Callable[[Dict[str,Any]], bool],
                 action: str, rationale: str):
        self.id=id; self.name=name; self.scope=scope; self.severity=severity
        self.condition=condition; self.action=action; self.rationale=rationale

    def eval(self, ctx: Dict[str,Any]) -> Optional[Dict[str,Any]]:
        try:
            if self.condition(ctx):
                return {
                    'id': self.id,
                    'name': self.name,
                    'scope': self.scope,
                    'severity': self.severity,
                    'action': self.action,
                    'rationale': self.rationale,
                }
        except Exception:
            return None
        return None

SEV_ORDER = {'High':3,'Medium':2,'Low':1,'Info':0}

def _get(ctx: Dict[str,Any], *keys, default=None):
    cur = ctx
    for k in keys:
        if cur is None: return default
        cur = cur.get(k) if isinstance(cur, dict) else None
    return cur if cur is not None else default

def build_rule_context() -> Dict[str,Any]:
    ctx = {
        'thr': {
            'benford_diff': SS.get('risk_diff_threshold', 0.05),
            'zero_ratio': 0.30,
            'tail_p99': 0.02,
            'off_hours': 0.15,
            'weekend': 0.25,
            'corr_high': 0.9,
            'gap_p95_hours': 12.0,
            'hhi': 0.2,
        },
        'last_numeric': SS.get('last_numeric_profile'),
        'gof': SS.get('last_gof'),
        'benford': {
            'r1': SS.get('bf1_res'),
            'r2': SS.get('bf2_res')
        },
        't4': SS.get('t4_results'),
        'corr': SS.get('last_corr'),
        'regression': {
            'linear': SS.get('last_linear'),
            'logistic': SS.get('last_logistic'),
        },
        'flags': SS.get('fraud_flags') or [],
    }
    # convenience derivations
    r1 = ctx['benford'].get('r1')
    if r1 and isinstance(r1, dict) and 'variance' in r1:
        try:
            ctx['benford']['r1_maxdiff'] = float(r1['variance']['diff_pct'].abs().max())
        except Exception:
            ctx['benford']['r1_maxdiff'] = None
    r2 = ctx['benford'].get('r2')
    if r2 and isinstance(r2, dict) and 'variance' in r2:
        try:
            ctx['benford']['r2_maxdiff'] = float(r2['variance']['diff_pct'].abs().max())
        except Exception:
            ctx['benford']['r2_maxdiff'] = None
    return ctx


# ----------------------------------- TABS -------------------------------------
TAB0, TAB1, TAB2, TAB3, TAB4, TAB5, TAB6 = st.tabs([ '0) Data Quality', '1) Overview (Sales activity)', '2) Profiling/Distribution', '3) Correlation & Trend', '4) Benford', '5) ANOVA & Nonparametric', '6) Regression'])
# ---- TAB 0: Data Quality (FULL) ----

with TAB0:
    st.subheader('üß™ Data Quality')
    if SS.get('df') is None:
        st.info('H√£y **Load full data** ƒë·ªÉ xem Data Quality Tab.')
    else:
        @st.cache_data(ttl=900, show_spinner=False, max_entries=16)
        def data_quality_table(df_in):
            import pandas as pd
            rows = []
            n = len(df_in)
            for c in df_in.columns:
                s = df_in[c]
                is_num = pd.api.types.is_numeric_dtype(s)
                is_dt  = pd.api.types.is_datetime64_any_dtype(s) or is_datetime_like(c, s)
                is_bool= pd.api.types.is_bool_dtype(s)
                is_cat = pd.api.types.is_categorical_dtype(s)
                base_type = 'Numeric' if is_num else ('Datetime' if is_dt else ('Boolean' if is_bool else ('Categorical' if is_cat else 'Text')))
                n_nonnull = int(s.notna().sum())
                n_nan = int(n - n_nonnull)
                n_unique = int(s.nunique(dropna=True))
                mem_mb = float(s.memory_usage(deep=True)) / 1048576.0
                blank = None; blank_pct = None
                zero = None; zero_pct = None
                if base_type in ('Text','Categorical'):
                    s_txt = s[s.notna()].astype(str).str.strip()
                    blank = int((s_txt == '').sum())
                    blank_pct = round(blank / n, 4) if n else None
                if base_type == 'Numeric':
                    s_num = pd.to_numeric(s, errors='coerce')
                    zero = int(s_num.eq(0).sum())
                    zero_pct = round(zero / n, 4) if n else None
                valid = n_nonnull - (blank or 0) if base_type in ('Text','Categorical') else n_nonnull
                rows.append({
                    'column': c,
                    'type': base_type,
                    'rows': n,
                    'valid': int(valid),
                    'valid%': round(valid / n, 4) if n else None,
                    'nan': n_nan,
                    'nan%': round(n_nan / n, 4) if n else None,
                    'blank': blank,
                    'blank%': blank_pct,
                    'zero': zero,
                    'zero%': zero_pct,
                    'unique': n_unique,
                    'memory_MB': round(mem_mb, 3),
                })
            cols_order = ['column','type','rows','valid','valid%','nan','nan%','blank','blank%','zero','zero%','unique','memory_MB']
            dq = pd.DataFrame(rows)
            dq = dq[cols_order]
            return dq.sort_values(['type','column']).reset_index(drop=True)
        try:
            dq = data_quality_table(SS['df'] if SS.get('df') is not None else DF_VIEW)
            st_df(dq, use_container_width=True, height=min(520, 60 + 24*min(len(dq), 18)))
        except Exception as e:
            st.error(f'L·ªói Data Quality: {e}')

# ============================== TAB 1 ‚Äî OVERVIEW (Sales Activities) ==============================
with TAB1:
    import numpy as np, pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go
    import textwrap # C·∫ßn import textwrap

    st.subheader("üìà Overview ‚Äî Sales Activities")

    # ---------- helpers ----------
    RULE = {"Month":"MS","Quarter":"QS","Year":"YS"}
    P2   = {"MS":"M","QS":"Q","YS":"Y"}
    YOY  = {"MS":12,"QS":4,"YS":1}

    def _clean_time(ts, min_year=1900, max_year=2100):
        t = pd.to_datetime(ts, errors="coerce")
        bad = t.notna() & ((t.dt.year < min_year) | (t.dt.year > max_year))
        return t.mask(bad)

    @st.cache_data(ttl=900, show_spinner="ƒêang t·ªïng h·ª£p...")
    def _agg_by_period(series_time, values, rule_code):
        t = _clean_time(series_time)
        m = t.notna()
        p = t.dt.to_period({"MS":"M","QS":"Q","YS":"Y"}[rule_code]).dt.start_time
        return (pd.DataFrame({"p": p[m], "v": values[m]})
                .groupby("p", dropna=False)["v"].sum().sort_index())

    def _wrap_label(lbl, width=16):
        s = "" if lbl is None else str(lbl)
        return "<br>".join(textwrap.wrap(s, width=width)) if len(s) > width else s

    def _pie_with_smart_labels(labels, values, colors, height=460):
        labels_wrapped = [_wrap_label(l, 16) for l in labels]
        max_len = max((len(str(l)) for l in labels), default=0)
        side_margin = 80 if max_len <= 14 else 120 if max_len <= 22 else 160
        show_text = len(labels) <= 14
        fig = go.Figure(go.Pie(
            labels=labels_wrapped, values=values, hole=0.35, sort=False,
            textposition="outside",
            texttemplate="%{label}<br>%{percent:.1%}" if show_text else "%{percent:.1%}",
            hovertemplate="%{label}<br>%{percent:.1%} (%{value:,.0f})<extra></extra>"
        ))
        fig.update_traces(marker=dict(colors=colors), automargin=True)
        fig.update_layout(
            margin=dict(l=side_margin, r=side_margin, t=20, b=20),
            showlegend=False, height=height,
            uniformtext_minsize=11, uniformtext_mode="hide"
        )
        return fig

    def _pick(col, label, key, help_=None):
        df_full = SS.get('df') 
        if df_full is None:
            return col.selectbox(label, ["‚Äî"], index=0, key=key, help=help_)
        
        v = col.selectbox(label, ["‚Äî"] + list(df_full.columns), index=0, key=key, help=help_)
        return None if v == "‚Äî" else v

    def _norm_period_value(p_text):
        s = str(p_text).lower() if p_text else "month"
        if s.startswith(("m","th√°ng")): return "Month"
        if s.startswith(("q","qu√Ω")):   return "Quarter"
        if s.startswith(("y","nƒÉm")):   return "Year"
        return "Month"

    def _norm_ser(s: pd.Series) -> pd.Series:
        return s.astype(str).str.strip().str.replace(r"\s+", " ", regex=True).str.lower()

    def _norm_list(vals):
        if not vals: return set()
        return set(pd.Series(list(vals)).astype(str).str.strip().str.replace(r"\s+", " ", regex=True).str.lower())

    # === (KH√îI PH·ª§C) Drill-down per chart (ƒë√∫ng nh∆∞ code g·ªëc) ===
    def _chart_drilldown_mask(prefix: str,
                              dfin: pd.DataFrame,
                              tv: pd.Series, rule_code: str,
                              region_col: str | None,
                              channel_col: str | None,
                              prod_col: str | None,
                              cust_col: str | None,
                              time_col_present: bool = True):
        def _top_values_local(df_local, col, k=200):
            if not col or col not in dfin.columns: return []
            return dfin[col].astype(str).value_counts(dropna=False).head(k).index.tolist()

        with st.expander("üéØ Drill-down filter ‚Äî Khoanh v√πng d·ªØ li·ªáu (bi·ªÉu ƒë·ªì n√†y)", expanded=False):
            ckR, ckC, ckP, ckU, ckT = st.columns([1,1,1,1,1])
            useR = ckR.checkbox("Region",  key=f"{prefix}_useR")
            useC = ckC.checkbox("Channel", key=f"{prefix}_useC")
            useP = ckP.checkbox("Product", key=f"{prefix}_useP")
            useU = ckU.checkbox("Customer", key=f"{prefix}_useU")
            useT = ckT.checkbox("Time",    key=f"{prefix}_useT") if time_col_present else False

            m1, m2 = st.columns([1.1, 2.2])
            selR = m1.multiselect("Region",  _top_values_local(dfin, region_col),  key=f"{prefix}_valR") if (useR and region_col and region_col in dfin.columns) else []
            selC = m1.multiselect("Channel", _top_values_local(dfin, channel_col), key=f"{prefix}_valC") if (useC and channel_col and channel_col in dfin.columns) else []
            selP = m2.multiselect("Product",  _top_values_local(dfin, prod_col),   key=f"{prefix}_valP") if (useP and prod_col and prod_col in dfin.columns) else []
            selU = m2.multiselect("Customer", _top_values_local(dfin, cust_col),   key=f"{prefix}_valU") if (useU and cust_col and cust_col in dfin.columns) else []

            if useT and time_col_present and tv is not None and not tv.isna().all():
                per_lbl = {"MS":"Month","QS":"Quarter","YS":"Year"}[rule_code]
                per_str = tv.dt.to_period({"MS":"M","QS":"Q","YS":"Y"}[rule_code]).astype(str)
                uniq_periods = sorted(pd.Series(per_str.loc[dfin.index]).dropna().unique().tolist())
                selT = m2.multiselect(f"K·ª≥ theo {per_lbl}", uniq_periods, key=f"{prefix}_valT")
            else:
                selT = []

        mask = pd.Series(True, index=dfin.index)
        if useR and region_col and selR and region_col in dfin.columns: mask &= dfin[region_col].astype(str).isin(selR)
        if useC and channel_col and selC and channel_col in dfin.columns: mask &= dfin[channel_col].astype(str).isin(selC)
        if useP and prod_col and selP and prod_col in dfin.columns:    mask &= dfin[prod_col].astype(str).isin(selP)
        if useU and cust_col and selU and cust_col in dfin.columns:    mask &= dfin[cust_col].astype(str).isin(selU)
        if useT and time_col_present and selT and tv is not None:
            per_now = tv.dt.to_period({"MS":"M","QS":"Q","YS":"Y"}[rule_code]).astype(str)
            mask &= per_now.loc[dfin.index].isin(set(selT))
        return mask

    def _sparse_line_labels(y_vals, fmt=lambda v: f"{v:.1f}%", min_dy_ratio=0.08, max_points=22):
        y = np.array([np.nan if v is None else v for v in y_vals], dtype=float)
        if len(y) == 0 or len(y) > max_points:
            return None
        vmin, vmax = np.nanmin(y), np.nanmax(y)
        rng = (vmax - vmin) if np.isfinite(vmax - vmin) and (vmax - vmin)!=0 else 1.0
        out, last = [], None
        for v in y:
            if np.isnan(v):
                out.append("")
                continue
            if (last is None) or (abs(v - last) >= min_dy_ratio * rng):
                out.append(fmt(v)); last = v
            else:
                out.append("")
        return out

    def _bar_text(values, fmt=lambda v: f"{v:,.0f}", max_labels=12):
        if len(values) <= max_labels:
            return [fmt(v) for v in values]
        return None
        
    @st.cache_data(ttl=900, show_spinner="ƒêang t√≠nh to√°n chi·∫øt kh·∫•u...")
    def get_discount_analysis(df_source, group_col, revenue_series, discount_series):
        if not group_col or group_col not in df_source.columns:
            return pd.DataFrame(columns=["Group", "Discount_Rate", "Total_Discount", "Total_Revenue"])
        
        g_disc = (pd.DataFrame({
            "Group": df_source[group_col].astype(str).fillna("(NA)"),
            "SalesB": revenue_series,
            "DiscB":  discount_series
        }).groupby("Group").sum(numeric_only=True))

        g_disc = g_disc[g_disc["SalesB"] > 0] 
        if g_disc.empty:
            return pd.DataFrame(columns=["Group", "Discount_Rate", "Total_Discount", "Total_Revenue"])
            
        g_disc["Discount_Rate"] = (-g_disc["DiscB"] / g_disc["SalesB"]) * 100.0
        g_disc = g_disc.sort_values("Discount_Rate", ascending=False)
        
        g_disc.columns = ["Total_Revenue", "Total_Discount", "Discount_Rate"]
        g_disc = g_disc.reset_index() 
        return g_disc[['Group', 'Discount_Rate', 'Total_Discount', 'Total_Revenue']]

    # ---- Data / guard
    df = SS.get("df") 
    if df is None or df.empty:
        st.info("H√£y n·∫°p d·ªØ li·ªáu tr∆∞·ªõc."); st.stop()

    # ====================== 0) Import Input Data ‚Äî (ƒê√É C·∫¨P NH·∫¨T) ======================
    st.markdown("### ‚öôÔ∏è Import Input Data ‚Äî (Required)")
    with st.container(border=True):
        c1, c2, c3, c4, c5 = st.columns(5)
        time_col    = _pick(c1, "üïí Time", "ov_time", help_="Datetime ƒë·ªÉ resample Month/Quarter/Year.")
        cust_col    = _pick(c2, "üë§ Customer", "ov_cust")
        prod_col    = _pick(c3, "üì¶ Product", "ov_prod")
        region_col  = _pick(c4, "üåç Region", "ov_region")
        channel_col = _pick(c5, "üõí Channel", "ov_channel")

        r1, r2, r3, r4 = st.columns(4) 
        rev_col    = _pick(r1, "üí∞ Revenue", "ov_rev", help_="Doanh thu cho bi·ªÉu ƒë·ªì/b·∫£ng.")
        weight_vol_col = _pick(r2, "‚öñÔ∏è Weight (Amount)", "ov_weight_vol", 
                               help_="D√πng cho Avg Price, %Sales(A), v√† chart Revenue vs Weight.")
        
        map_a = _pick(r3, "üè∑Ô∏è Mapping A ‚Äî Transaction", "ov_map_a",
                      help_="Ph√¢n Sales (External) vs Transfer (Internal) ‚Äî theo VOLUME/WEIGHT.")
        map_b = _pick(r4, "üè∑Ô∏è Mapping B ‚Äî Value Type", "ov_map_b",
                      help_="Ph√¢n Sales(B) vs Discount(B) ‚Äî theo REVENUE.")
        if map_a and map_b and map_a == map_b:
            st.warning("Mapping A v√† Mapping B ƒëang d√πng **c√πng c·ªôt**. H√£y ch·ªçn c·ªôt kh√°c nhau.")

        uniq_a = sorted(df[map_a].astype(str).unique().tolist()) if map_a and map_a in df.columns else []
        uniq_b = sorted(df[map_b].astype(str).unique().tolist()) if map_b and map_b in df.columns else []
        
        with st.expander("Mapping chi ti·∫øt", expanded=False):
            a1, a2 = st.columns(2)
            mv_a_sales = a1.multiselect("Sales (External) ‚Äî A", uniq_a, key="mv_a_sales")
            mv_a_trans = a2.multiselect("Transfer (Internal) ‚Äî A", uniq_a, key="mv_a_transfer")
            b1, b2 = st.columns(2)
            mv_b_sales = b1.multiselect("Sales (B)", uniq_b, key="mv_b_sales")
            mv_b_disc  = b2.multiselect("Discount (B)", uniq_b, key="mv_b_disc")

    if not rev_col or rev_col not in df.columns:
        st.info("C·∫ßn ch·ªçn **Revenue** ƒë·ªÉ xem Overview."); 
    else:

        # ====================== 1) Display ======================
        st.markdown("### üß≠ Display")
        d1, d2, d3 = st.columns([1,1,1])
        period_raw = d1.segmented_control("Period", ["Month","Quarter","Year"])
        compare    = d2.segmented_control("Compare", ["Prev","YoY"])
        period     = _norm_period_value(period_raw)
        rule       = RULE[period]

        if time_col and time_col in df.columns:
            all_years = sorted(pd.to_datetime(df[time_col], errors="coerce").dropna().dt.year.unique())
            year_scope = d3.selectbox("Year scope (KPI/Trend)", ["All"]+[str(y) for y in all_years], index=len(all_years))
        else:
            year_scope = "All"
            
        # ====================== 2) L·ªçc scope nƒÉm ======================
        t_all = _clean_time(df[time_col]) if time_col and time_col in df.columns else pd.Series(pd.NaT, index=df.index)
        mask_scope = (t_all.dt.year == int(year_scope)) if (time_col and year_scope!="All") else pd.Series(True, index=df.index)
        
        dfv = df.loc[mask_scope].copy() # dfv l√† DataFrame ch·ªâ l·ªçc theo nƒÉm
        tv  = t_all.loc[mask_scope] if time_col else pd.Series(pd.NaT, index=dfv.index)
        
        if dfv.empty:
            st.info("Kh√¥ng c√≥ d·ªØ li·ªáu trong ph·∫°m vi ƒë√£ ch·ªçn."); st.stop()
        # =========================================================================

        # series c∆° b·∫£n
        rev = pd.to_numeric(dfv[rev_col], errors="coerce").fillna(0.0)
        vol_wgt = pd.to_numeric(dfv[weight_vol_col], errors="coerce").fillna(0.0) if weight_vol_col and weight_vol_col in dfv.columns else pd.Series(0.0, index=dfv.index)

        # Mapping A
        if map_a and map_a in dfv.columns:
            A_norm = _norm_ser(dfv[map_a])
            m_salesA    = A_norm.isin(_norm_list(SS.get("mv_a_sales", [])))
            m_transferA = A_norm.isin(_norm_list(SS.get("mv_a_transfer", [])))
        else:
            m_salesA    = pd.Series(False, index=dfv.index)
            m_transferA = pd.Series(False, index=dfv.index)

        salesA_vol    = vol_wgt.where(m_salesA, 0.0)
        transferA_vol = vol_wgt.where(m_transferA, 0.0)
        baseA = float(salesA_vol.abs().sum() + transferA_vol.abs().sum())
        pct_salesA    = (float(salesA_vol.abs().sum())/baseA*100) if baseA>0 else np.nan
        pct_transferA = (float(transferA_vol.abs().sum())/baseA*100) if baseA>0 else np.nan

        # Mapping B
        if map_b and map_b in dfv.columns:
            B_norm = _norm_ser(dfv[map_b])
            is_salesB = B_norm.isin(_norm_list(SS.get("mv_b_sales", [])))
            is_discB  = B_norm.isin(_norm_list(SS.get("mv_b_disc",  [])))
            salesB_rev = rev.where(is_salesB, 0.0)
            discB_rev  = rev.where(is_discB,  0.0)
        else:
            salesB_rev = rev.copy() 
            discB_rev  = pd.Series(0.0, index=dfv.index) 

        # Discount% (T√≠nh to√°n s·ªõm ƒë·ªÉ KPI d√πng)
        disc_avg_month = np.nan; disc_year_pct = np.nan
        mon_sales_gt_0 = pd.DataFrame() 
        
        if time_col and not salesB_rev.empty and not discB_rev.empty:
            mon = (pd.DataFrame({"m": tv.dt.to_period("M").dt.start_time,
                                "SalesB": salesB_rev, "DiscB": discB_rev})
                .groupby("m").sum(numeric_only=True))
            mon_sales_gt_0 = mon[mon["SalesB"] > 0].copy() 
            
            if not mon_sales_gt_0.empty:
                mon_sales_gt_0["Discount%"] = (-mon_sales_gt_0["DiscB"] / mon_sales_gt_0["SalesB"]) * 100.0
                
                y_opts = sorted(mon_sales_gt_0.index.year.unique())
                if y_opts:
                    yr = int(year_scope) if (year_scope!="All" and int(year_scope) in y_opts) else int(y_opts[-1])
                    mon_y = mon_sales_gt_0[mon_sales_gt_0.index.year==yr]
                    
                    if not mon_y.empty:
                        disc_avg_month = float(mon_y["Discount%"].mean())
                        disc_year_pct  = float((-mon_y["DiscB"].sum() / mon_y["SalesB"].sum()) * 100.0)

        revenue_for_charts = salesB_rev if (map_b and map_b in dfv.columns and SS.get("mv_b_sales")) else rev

        # ====================== 3) KPI (ƒê√É C·∫¨P NH·∫¨T) ======================
        orders_total = len(dfv)
        prod_total   = (dfv.loc[revenue_for_charts>0, prod_col].nunique()
                        if (prod_col and prod_col in dfv.columns) else np.nan)
        revenue_total = float(revenue_for_charts.sum())

        k1, k2, k3, k4 = st.columns(4)
        k1.metric("Revenue (for charts)", f"{revenue_total:,.0f}")
        k2.metric("Total Transactions", f"{orders_total:,.0f}") 
        k3.metric("Total product", f"{prod_total:,.0f}" if not np.isnan(prod_total) else "‚Äî")
        k4.metric("%Sales (A) by Weight/Vol", f"{pct_salesA:.1f}%" if not np.isnan(pct_salesA) else "‚Äî") 

        k5, k6, k7, k8 = st.columns(4)
        k5.metric("%Transfer (A) by Weight/Vol", f"{pct_transferA:.1f}%" if not np.isnan(pct_transferA) else "‚Äî") 
        k6.metric("Discount% avg monthly (B)", f"{disc_avg_month:.1f}%" if not np.isnan(disc_avg_month) else "‚Äî")
        k7.metric("Discount% (YTD, B)", f"{disc_year_pct:.1f}%" if not np.isnan(disc_year_pct) else "‚Äî")
        k8.metric("Scope year", year_scope)

        # =============== 4) Trend ‚Äî Revenue + %Œî ==================
        with st.expander("üìä 1. Trend ‚Äî Revenue & %Œî", expanded=True):
            tmask = _chart_drilldown_mask("ov_trend", dfv, tv, rule, region_col, channel_col, prod_col, cust_col, bool(time_col))
            
            with st.expander("üé® M√†u & nh√£n ‚Äî Trend", expanded=False):
                ccol1, ccol2, ccol3 = st.columns([1,1,1])
                color_bar_trend   = ccol1.color_picker("M√†u c·ªôt (Revenue)", "#74b9ff", key="clr_tr_bar")
                color_line_trend  = ccol2.color_picker("M√†u line (%Œî)",    "#1f77b4", key="clr_tr_line")
                color_text_common = ccol3.color_picker("M√†u s·ªë li·ªáu (labels)", "#cccccc", key="clr_tr_txt")
                show_all_line_lbl = st.checkbox("Hi·ªán t·∫•t c·∫£ nh√£n line", value=True, key="tr_show_all")

            if time_col:
                g_rev = _agg_by_period(tv.loc[tmask], revenue_for_charts.loc[tmask], rule)
                base  = g_rev.shift(1) if compare=="Prev" else g_rev.shift(YOY[rule])
                pct   = np.where(base!=0, (g_rev/base-1.0)*100.0, np.nan)

                bar_text = _bar_text(g_rev.values)
                line_text = [f"{v:.1f}%" if (v is not None and not np.isnan(v)) else "" for v in pct] if show_all_line_lbl else _sparse_line_labels(pct, fmt=lambda v: f"{v:.1f}%")

                y_pad = max(g_rev.max() * 0.15, 1.0) if not g_rev.empty else 1.0
                fig = go.Figure()
                fig.add_bar(x=g_rev.index, y=g_rev.values, name="Revenue",
                            marker_color=color_bar_trend,
                            text=bar_text, textposition="outside",
                            textfont=dict(color=color_text_common), cliponaxis=False)
                fig.add_scatter(x=g_rev.index, y=pct, yaxis="y2", mode="lines+markers+text", name="%Œî",
                                line=dict(color=color_line_trend),
                                text=line_text, textposition="top center",
                                textfont=dict(color=color_text_common))
                fig.update_layout(
                    xaxis_title=period,
                    yaxis=dict(title="Revenue", range=[0, float(g_rev.max()+y_pad)] if not g_rev.empty else [0,1], title_standoff=8),
                    yaxis2=dict(title="%Œî", overlaying="y", side="right", showgrid=False, title_standoff=14),
                    margin=dict(l=10,r=90,t=10,b=10), showlegend=True, height=430,
                    uniformtext_minsize=10, uniformtext_mode="hide"
                )
                st.plotly_chart(fig, use_container_width=True, config={"displayModeBar": False})

                with st.expander("üìÑ Trend data (table)"):
                    show = pd.DataFrame({
                        period: g_rev.index, "Revenue": g_rev.values,
                        "Base": base.values, "%Œî": pct
                    })
                    # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                    show_styled = show.copy()
                    show_styled["Revenue"] = show_styled["Revenue"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                    show_styled["Base"] = show_styled["Base"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                    show_styled["%Œî"] = show_styled["%Œî"].map(lambda x: f"{x:.1f}%" if pd.notna(x) else "‚Äî")
                    st.dataframe(show_styled, use_container_width=True, hide_index=True)
            else:
                st.info("C·∫ßn ch·ªçn **Time** ƒë·ªÉ xem Trend.")

        # =============== (M·ªöI) T√ÅCH PH√ÇN T√çCH CHI·∫æT KH·∫§U RA RI√äNG =================
        with st.expander("üìâ 2. Ph√¢n t√≠ch Chi·∫øt kh·∫•u (Discount Analysis)", expanded=False):
            if not time_col or not map_b or not SS.get("mv_b_sales"):
                st.info("C·∫ßn ch·ªçn **Time** v√† **Mapping B (Value Type)** (bao g·ªìm Sales v√† Discount) ƒë·ªÉ xem ph√¢n t√≠ch Chi·∫øt kh·∫•u.")
            else:
                dmask = _chart_drilldown_mask("ov_disc", dfv, tv, rule, region_col, channel_col, prod_col, cust_col, bool(time_col))
                
                dfv_disc = dfv.loc[dmask]
                tv_disc = tv.loc[dmask]
                salesB_rev_disc = salesB_rev.loc[dmask]
                discB_rev_disc = discB_rev.loc[dmask]

                st.markdown("#### T·ª∑ l·ªá chi·∫øt kh·∫•u h√†ng th√°ng")
                
                mon_disc = (pd.DataFrame({
                    "m": tv_disc.dt.to_period("M").dt.start_time,
                    "SalesB": salesB_rev_disc, 
                    "DiscB":  discB_rev_disc
                }).groupby("m").sum(numeric_only=True))
                mon_disc_gt_0 = mon_disc[mon_disc["SalesB"] > 0].copy() 
                
                if mon_disc_gt_0.empty:
                    st.info("Ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≠nh Monthly Discount (sau khi drill-down).")
                else:
                    mon_disc_gt_0["Discount%"] = (-mon_disc_gt_0["DiscB"] / mon_disc_gt_0["SalesB"]) * 100.0
                    y_opts = sorted(mon_disc_gt_0.index.year.unique())
                    if not y_opts:
                        st.warning("Kh√¥ng t√¨m th·∫•y nƒÉm h·ª£p l·ªá.")
                    else:
                        default_year_idx = y_opts.index(int(year_scope)) if (year_scope!="All" and int(year_scope) in y_opts) else len(y_opts)-1
                        yr = st.selectbox("Year", y_opts, index=default_year_idx, key="trend_disc_year")

                        show = mon_disc_gt_0[mon_disc_gt_0.index.year == int(yr)].copy()
                        show.index = show.index.strftime("%b %Y")
                        
                        # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                        styled = show.copy()
                        styled["SalesB"]    = styled["SalesB"].map(lambda x: f"{x:,.0f}")
                        styled["DiscB"]     = styled["DiscB"].map(lambda x: f"{x:,.0f}")
                        styled["Discount%"] = styled["Discount%"].map(lambda x: f"{x:.1f}%")
                        st.dataframe(styled, use_container_width=True, height=340)
                        
                        st.markdown("#### Top Chi·∫øt kh·∫•u (trong nƒÉm ƒë√£ ch·ªçn & ƒë√£ drill-down)")
                        
                        dfv_year_disc = dfv_disc[tv_disc.dt.year == int(yr)] if time_col else dfv_disc
                        salesB_rev_year_disc = salesB_rev.loc[dfv_year_disc.index]
                        discB_rev_year_disc = discB_rev.loc[dfv_year_disc.index]
                        
                        tabD1, tabD2, tabD3 = st.tabs(["Theo C·ª≠a h√†ng (Region)", "Theo S·∫£n ph·∫©m", "Theo Kh√°ch h√†ng"])
                        
                        # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                        def format_disc_table(df_in):
                            df_out = df_in.head(15).copy()
                            df_out["Discount_Rate"] = df_out["Discount_Rate"].map(lambda x: f"{x:.2f}%" if pd.notna(x) else "‚Äî")
                            df_out["Total_Discount"] = df_out["Total_Discount"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                            df_out["Total_Revenue"] = df_out["Total_Revenue"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                            return df_out
                        
                        with tabD1:
                            df_disc_region = get_discount_analysis(dfv_year_disc, region_col, salesB_rev_year_disc, discB_rev_year_disc)
                            st.dataframe(format_disc_table(df_disc_region), 
                                         use_container_width=True, hide_index=True)
                        with tabD2:
                            df_disc_prod = get_discount_analysis(dfv_year_disc, prod_col, salesB_rev_year_disc, discB_rev_year_disc)
                            st.dataframe(format_disc_table(df_disc_prod), 
                                         use_container_width=True, hide_index=True)
                        with tabD3:
                            df_disc_cust = get_discount_analysis(dfv_year_disc, cust_col, salesB_rev_year_disc, discB_rev_year_disc)
                            st.dataframe(format_disc_table(df_disc_cust), 
                                         use_container_width=True, hide_index=True)
            
        # ============ 4b) Sales Revenue vs Sales Weight ============
        with st.expander("üíπ 3. Sales Revenue vs Sales Weight", expanded=False):
            rw_mask = _chart_drilldown_mask("ov_rw", dfv, tv, rule, region_col, channel_col, prod_col, cust_col, bool(time_col))
            
            with st.expander("üé® M√†u & nh√£n ‚Äî Revenue vs Weight", expanded=False):
                c3, c4, c5 = st.columns([1,1,1])
                color_bar_rw   = c3.color_picker("M√†u c·ªôt (Revenue)", "#74b9ff", key="clr_rw_bar")
                color_line_rw  = c4.color_picker("M√†u line (Weight)", "#2ca02c", key="clr_rw_line")
                color_text_rw  = c5.color_picker("M√†u s·ªë li·ªáu (labels)", "#cccccc", key="clr_rw_txt")
                show_all_line_lbl_rw = st.checkbox("Hi·ªán t·∫•t c·∫£ nh√£n line", value=True, key="rw_show_all")

            if time_col and weight_vol_col and weight_vol_col in df.columns:
                g_rev2 = _agg_by_period(tv.loc[rw_mask], revenue_for_charts.loc[rw_mask], rule)
                g_wgt2 = _agg_by_period(tv.loc[rw_mask], vol_wgt.loc[rw_mask].where(vol_wgt.loc[rw_mask]>0, 0.0), rule)
                idx = g_rev2.index.union(g_wgt2.index)
                g_rev2 = g_rev2.reindex(idx, fill_value=0)
                g_wgt2 = g_wgt2.reindex(idx, fill_value=0)

                bar_text = _bar_text(g_rev2.values)
                line_text = [f"{v:,.0f}" if not pd.isna(v) else "" for v in g_wgt2.values] if show_all_line_lbl_rw else _sparse_line_labels(g_wgt2.values, fmt=lambda v: f"{v:,.0f}")

                y_pad = max(g_rev2.max() * 0.15, 1.0) if not g_rev2.empty else 1.0
                fig2 = go.Figure()
                fig2.add_bar(x=idx, y=g_rev2.values, name="Sales Revenue",
                            marker_color=color_bar_rw,
                            text=bar_text, textposition="outside",
                            textfont=dict(color=color_text_rw),
                            cliponaxis=False)
                fig2.add_scatter(x=idx, y=g_wgt2.values, yaxis="y2", mode="lines+markers+text", name="Sales Weight",
                                line=dict(color=color_line_rw),
                                text=line_text, textposition="top center",
                                textfont=dict(color=color_text_rw))
                fig2.update_layout(
                    xaxis_title=period,
                    yaxis=dict(title="Sales Revenue", range=[0, float(g_rev2.max()+y_pad)] if not g_rev2.empty else [0,1], title_standoff=8),
                    yaxis2=dict(title="Sales Weight", overlaying="y", side="right", showgrid=False, title_standoff=14),
                    margin=dict(l=10,r=90,t=10,b=10), showlegend=True, height=430,
                    uniformtext_minsize=10, uniformtext_mode="hide"
                )
                st.plotly_chart(fig2, use_container_width=True, config={"displayModeBar": False})

                with st.expander("üìÑ Revenue vs Weight ‚Äî monthly (table)"):
                    show = pd.DataFrame({period: idx, "Revenue": g_rev2.reindex(idx).values,
                                        "Weight": g_wgt2.reindex(idx).values})
                    # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                    show_styled = show.copy()
                    show_styled["Revenue"] = show_styled["Revenue"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                    show_styled["Weight"] = show_styled["Weight"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                    st.dataframe(show_styled, use_container_width=True, hide_index=True)
            else:
                st.info("C·∫ßn ch·ªçn **Time** v√† **Weight (Amount)** ƒë·ªÉ xem bi·ªÉu ƒë·ªì n√†y.")

        # ====================== 5) Top Contribution  |  Pie ======================
        with st.expander("üß± 4. Top Contribution  |  ü•ß Pie", expanded=False):
            tc1, tc2, tc3 = st.columns([2,1,1])
            dim_col = tc1.selectbox("üìä Dimension (X)", ["‚Äî"] + list(dfv.columns), index=0, key="ov_dim_topc")
            topN    = tc2.slider("Top-N", 3, 50, 10, key="ov_topn_topc")
            as_share= tc3.checkbox("Chu·∫©n h√≥a % (share)", value=False, key="ov_share")

            pal_opts = { "Plotly": px.colors.qualitative.Plotly, "Bold": px.colors.qualitative.Bold, "Pastel": px.colors.qualitative.Pastel, "Set3": px.colors.qualitative.Set3 }
            with st.expander("üé® M√†u & nh√£n ‚Äî Top Contribution", expanded=False):
                cpl, cln = st.columns([1,1])
                pal_name = cpl.selectbox("B·∫£ng m√†u Bar/Pie", list(pal_opts.keys()), index=0, key="ov_tc_palette")
                color_line_cum = cln.color_picker("M√†u line (Cumulative %)", "#636EFA", key="clr_tc_line")
                color_text_tc  = st.color_picker("M√†u s·ªë li·ªáu (labels)", "#cccccc", key="clr_tc_txt")
                show_all_line_lbl_tc = st.checkbox("Hi·ªán t·∫•t c·∫£ nh√£n line", value=True, key="tc_show_all")

            if (not dim_col) or (dim_col=="‚Äî") or (dim_col not in dfv.columns):
                st.info("Ch·ªçn Dimension (X) ƒë·ªÉ xem Top Contribution.")
            else:
                tc_mask = _chart_drilldown_mask("ov_tc", dfv, tv, rule, region_col, channel_col, prod_col, cust_col, bool(time_col))
                
                dim_vals = dfv.loc[tc_mask, dim_col].astype(str).fillna("(NA)")
                g = (pd.DataFrame({"d": dim_vals, "v": revenue_for_charts.loc[tc_mask]})
                    .groupby("d", dropna=False)["v"].sum().sort_values(ascending=False))

                total_sel = float(g.sum()) if len(g) else 0.0
                g_top = g.head(topN)
                cum   = (g_top.cumsum()/total_sel*100.0) if total_sel>0 else pd.Series(np.nan, index=g_top.index)
                yvals = (g_top/total_sel*100.0) if (as_share and total_sel>0) else g_top

                palette = pal_opts[pal_name]
                colors_for = {cat: palette[i % len(palette)] for i, cat in enumerate(g_top.index)}

                cL, cR = st.columns([0.7, 0.3])
                with cL:
                    bar_text = _bar_text(yvals.values, fmt=(lambda v: f"{v:.1f}%") if as_share else (lambda v: f"{v:,.0f}"))
                    line_text = [f"{v:.1f}%" if not pd.isna(v) else "" for v in cum.values] if show_all_line_lbl_tc else _sparse_line_labels(cum.values, fmt=lambda v: f"{v:.1f}%")
                    fig_t = go.Figure()
                    fig_t.add_bar(
                        x=g_top.index, y=yvals.values, name="Top-N",
                        marker_color=[colors_for[c] for c in g_top.index],
                        text=bar_text, textposition="outside",
                        textfont=dict(color=color_text_tc), cliponaxis=False
                    )
                    fig_t.add_scatter(
                        x=g_top.index, y=cum.values, yaxis="y2", mode="lines+markers+text", name="Cumulative %",
                        line=dict(color=color_line_cum),
                        text=line_text, textposition="top center",
                        textfont=dict(color=color_text_tc)
                    )
                    fig_t.update_layout(
                        xaxis_title=dim_col,
                        yaxis_title=("Share %" if as_share else "Revenue"),
                        yaxis2=dict(title="Cumulative %", overlaying="y", side="right", showgrid=False, title_standoff=14),
                        margin=dict(l=10,r=90,t=10,b=10), showlegend=True, height=460,
                        uniformtext_minsize=10, uniformtext_mode="hide"
                    )
                    st.plotly_chart(fig_t, use_container_width=True, config={"displayModeBar": False})

                with cR:
                    other_val = max(0.0, total_sel - float(g_top.sum()))
                    labels = list(g_top.index) + (["Other"] if other_val > 0 else [])
                    values = list(g_top.values) + ([other_val] if other_val > 0 else [])
                    colors = [colors_for[c] for c in g_top.index] + (["#BDBDBD"] if other_val > 0 else [])
                    fig_p = _pie_with_smart_labels(labels, values, colors, height=460)
                    st.plotly_chart(fig_p, use_container_width=True, config={"displayModeBar": False})

                with st.expander("üìÑ Top contribution (table)"):
                    tbl = (pd.DataFrame({"Label": g_top.index, "Value": g_top.values})
                        .assign(Share=lambda d: d["Value"]/d["Value"].sum()*100 if d["Value"].sum()!=0 else np.nan))
                    # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                    tbl_styled = tbl.copy()
                    tbl_styled["Value"] = tbl_styled["Value"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                    tbl_styled["Share"] = tbl_styled["Share"].map(lambda x: f"{x:.1f}%" if pd.notna(x) else "‚Äî")
                    st.dataframe(tbl_styled, use_container_width=True, hide_index=True)

        # =============== 6) Avg Price vs Revenue =================
        with st.expander("üíπ 5. Avg Price vs Revenue", expanded=False):
            pr_mask = _chart_drilldown_mask("ov_avg", dfv, tv, rule, region_col, channel_col, prod_col, cust_col, bool(time_col))
            
            with st.expander("üé® M√†u & nh√£n ‚Äî Avg Price vs Revenue", expanded=False):
                c5, c6, c7 = st.columns([1,1,1])
                color_bar_avg  = c5.color_picker("M√†u c·ªôt (Revenue)", "#74b9ff", key="clr_avg_bar")
                color_line_avg = c6.color_picker("M√†u line (Avg Price)", "#e377c2", key="clr_avg_line")
                color_text_avg = c7.color_picker("M√†u s·ªë li·ªáu (labels)", "#cccccc", key="clr_avg_txt")
                show_all_line_lbl_avg = st.checkbox("Hi·ªán t·∫•t c·∫£ nh√£n line", value=True, key="avg_show_all")

            if time_col and weight_vol_col and weight_vol_col in df.columns:
                grpM = tv.loc[pr_mask].dt.to_period("M").dt.start_time
                rev_bar = pd.DataFrame({"m": grpM, "v": revenue_for_charts.loc[pr_mask]}).groupby("m")["v"].sum()
                mask_w = vol_wgt.loc[pr_mask] > 0
                num = pd.DataFrame({"m": grpM, "num": revenue_for_charts.loc[pr_mask].where(mask_w, 0.0)}).groupby("m")["num"].sum()
                den = pd.DataFrame({"m": grpM, "den": vol_wgt.loc[pr_mask].where(mask_w, 0.0)}).groupby("m")["den"].sum().replace(0, np.nan)
                avg_price = (num/den).reindex(rev_bar.index)

                bar_text  = _bar_text(rev_bar.values)
                line_text = [f"{v:,.0f}" if not pd.isna(v) else "" for v in avg_price.values] if show_all_line_lbl_avg else _sparse_line_labels(avg_price.values, fmt=lambda v: f"{v:,.0f}")

                y_pad = max(rev_bar.max() * 0.15, 1.0) if not rev_bar.empty else 1.0
                figp = go.Figure()
                figp.add_bar(x=rev_bar.index, y=rev_bar.values, name="Revenue",
                            marker_color=color_bar_avg,
                            text=bar_text, textposition="outside",
                            textfont=dict(color=color_text_avg),
                            cliponaxis=False)
                figp.add_scatter(x=rev_bar.index, y=avg_price.values, yaxis="y2", mode="lines+markers+text", name="Avg Price",
                                line=dict(color=color_line_avg),
                                text=line_text, textposition="top center",
                                textfont=dict(color=color_text_avg))
                figp.update_layout(
                    xaxis_title="Month",
                    yaxis=dict(title="Revenue", range=[0, float(rev_bar.max()+y_pad)] if not rev_bar.empty else [0,1], title_standoff=8),
                    yaxis2=dict(title="Avg Price", overlaying="y", side="right", showgrid=False, title_standoff=14),
                    margin=dict(l=10,r=90,t=10,b=10), showlegend=True, height=430,
                    uniformtext_minsize=10, uniformtext_mode="hide"
                )
                st.plotly_chart(figp, use_container_width=True, config={"displayModeBar": False})

                with st.expander("üìÑ Avg Price vs Revenue ‚Äî monthly (table)"):
                    show = pd.DataFrame({
                        "Month": rev_bar.index, "Revenue": rev_bar.values,
                        "Avg Price": avg_price.values
                    })
                    # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                    show_styled = show.copy()
                    show_styled["Revenue"] = show_styled["Revenue"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                    show_styled["Avg Price"] = show_styled["Avg Price"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                    st.dataframe(show_styled, use_container_width=True, hide_index=True)
            else:
                st.info("C·∫ßn ch·ªçn **Time** v√† **Weight (Amount)** ƒë·ªÉ xem Avg Price vs Revenue.")

        # =============== 7) Distribution ‚Äî Region √ó Channel (stacked) ===============
        with st.expander("üó∫Ô∏è 6. Distribution ‚Äî Region √ó Channel (stacked)", expanded=False):
            ds_mask = _chart_drilldown_mask("ov_dist", dfv, tv, rule, region_col, channel_col, prod_col, cust_col, bool(time_col))

            with st.expander("üé® M√†u ‚Äî Distribution", expanded=False):
                pal_name2 = st.selectbox("B·∫£ng m√†u (stacked)", ["Plotly","Bold","Pastel","Set3"], index=0, key="ov_dist_pal")
            pal2 = {"Plotly": px.colors.qualitative.Plotly, "Bold": px.colors.qualitative.Bold, "Pastel": px.colors.qualitative.Pastel, "Set3": px.colors.qualitative.Set3}[pal_name2]

            if region_col and channel_col and region_col in dfv.columns and channel_col in dfv.columns:
                ddf = dfv.loc[ds_mask].copy()
                srev= revenue_for_charts.loc[ds_mask]

                topn_ch = st.slider("Top-N Channel (stacked)", 3, 20, 6, key="ov_dist_topn")
                ch_sum = (pd.DataFrame({"ch": ddf[channel_col].astype(str), "v": srev})
                        .groupby("ch")["v"].sum().sort_values(ascending=False))
                keep = set(ch_sum.head(topn_ch).index)
                ch = ddf[channel_col].astype(str).where(ddf[channel_col].astype(str).isin(keep), other="Other")

                g = (pd.DataFrame({"Region": ddf[region_col].astype(str), "Channel": ch, "v": srev})
                    .groupby(["Region","Channel"])["v"].sum().reset_index())
                piv = g.pivot(index="Region", columns="Channel", values="v").fillna(0.0)
                color_map = {c: pal2[i % len(pal2)] for i, c in enumerate(piv.columns)}

                row_tot = piv.sum(axis=1).replace(0, np.nan)
                share   = piv.div(row_tot, axis=0) * 100.0
                piv     = piv.loc[row_tot.sort_values().index]; share = share.loc[piv.index]

                fig = go.Figure(); thr = 8.0
                for col in piv.columns:
                    vals = piv[col].values
                    pct  = share[col].values
                    text = [f"{v:.1f}%" if pd.notna(v) else "" for v in pct]
                    pos  = ["inside" if (isinstance(p, (int,float)) and p >= thr) else "outside" for p in pct]
                    fig.add_bar(x=piv.index, y=vals, name=str(col),
                                marker_color=color_map[str(col)], text=text, textposition=pos, cliponaxis=False)
                fig.update_layout(
                    barmode="stack", xaxis_title="Region", yaxis_title="Revenue",
                    margin=dict(l=10,r=90,t=10,b=10), showlegend=True, height=460,
                    uniformtext_minsize=10, uniformtext_mode="hide"
                )
                st.plotly_chart(fig, use_container_width=True, config={"displayModeBar": False})

                with st.expander("üìÑ Region √ó Channel (pivot table)"):
                    # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                    st.dataframe(piv.style.format("{:,.0f}"), use_container_width=True)
            else:
                st.info("C·∫ßn ch·ªçn **Region** v√† **Channel**.")

        # ====================== 8) ‚ú® Bi·ªÉu ƒë·ªì tu·ª≥ ch·ªânh (pivot-like) ======================
        with st.expander("‚ú® 7. Bi·ªÉu ƒë·ªì tu·ª≥ ch·ªânh (X/Y/Z nh∆∞ pivot)", expanded=False):
            with st.container(border=True):
                c0, c1, c2, c3 = st.columns([1.2,1,1,1])
                x_col = c0.selectbox("X (Datetime/Categorical)", ["‚Äî"] + list(df.columns), index=0, key="pv_x")
                y_col = c1.selectbox("Y (Numeric)", ["‚Äî"] + list(df.select_dtypes(include=[np.number]).columns), index=0, key="pv_y")
                z_mode= c2.selectbox("Z (Line)", ["None","% share of Y","Secondary numeric"], index=0, key="pv_zmode")
                chart = c3.selectbox("Chart type", ["Bar","Line","Bar + Line"], index=0, key="pv_chart")

                pv_mask = _chart_drilldown_mask("ov_pv", dfv, tv, rule, region_col, channel_col, prod_col, cust_col, bool(time_col))

                with st.expander("üé® M√†u & nh√£n ‚Äî Custom chart", expanded=False):
                    c7, c8, c9 = st.columns([1,1,1])
                    color_pv_bar  = c7.color_picker("M√†u c·ªôt (Y)", "#74b9ff", key="clr_pv_bar")
                    color_pv_line = c8.color_picker("M√†u line (Z)", "#ff7f0e", key="clr_pv_line")
                    color_pv_txt  = c9.color_picker("M√†u s·ªë li·ªáu (labels)", "#cccccc", key="clr_pv_txt")
                    show_all_line_lbl_pv = st.checkbox("Hi·ªán t·∫•t c·∫£ nh√£n line", value=True, key="pv_show_all")

                agg = st.radio("Aggregation for Y", ["sum","mean","median","count"], horizontal=True, key="pv_agg")

                if z_mode == "Secondary numeric":
                    z_col = st.selectbox("Z (Numeric for line)", ["‚Äî"] + list(df.select_dtypes(include=[np.number]).columns), index=0, key="pv_zcol")
                    z_agg= st.radio("Aggregation for Z", ["sum","mean","median","count"], horizontal=True, key="pv_zagg")
                else:
                    z_col = None; z_agg = None

                if (not x_col) or x_col=="‚Äî" or (not y_col) or y_col=="‚Äî" or x_col not in df.columns or y_col not in df.columns:
                    st.info("Ch·ªçn X v√† Y ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì tu·ª≥ ch·ªânh.")
                else:
                    base = dfv.loc[pv_mask].copy()

                    x_series = base[x_col]
                    if pd.api.types.is_datetime64_any_dtype(df[x_col]) or 'date' in str(x_col).lower() or 'time' in str(x_col).lower():
                        x_series = pd.to_datetime(x_series, errors="coerce").dt.to_period(P2[rule]).dt.start_time
                        x_title  = f"{x_col} ({period})"
                    else:
                        x_series = x_series.astype(str); x_title = f"{x_col} (category)"

                    y_ser = pd.to_numeric(base[y_col], errors="coerce")
                    agg_map = {"sum":"sum","mean":"mean","median":"median","count":"count"}
                    if agg == "count":
                        dfY = pd.DataFrame({"x": x_series, "y": 1}).groupby("x")["y"].count()
                    else:
                        dfY = pd.DataFrame({"x": x_series, "y": y_ser}).groupby("x")["y"].agg(agg_map[agg])

                    dfZ = None
                    if z_mode == "Secondary numeric" and z_col and z_col != "‚Äî" and z_col in df.columns:
                        z_ser = pd.to_numeric(base[z_col], errors="coerce")
                        if z_agg == "count":
                            dfZ = pd.DataFrame({"x": x_series, "z": 1}).groupby("x")["z"].count()
                        else:
                            dfZ = pd.DataFrame({"x": x_series, "z": z_ser}).groupby("x")["z"].agg(agg_map[z_agg])
                        dfZ = dfZ.reindex(dfY.index)
                    elif z_mode == "% share of Y":
                        total_y = float(dfY.sum()) if dfY.notna().any() else 0.0
                        dfZ = (dfY/total_y*100.0) if total_y>0 else dfY*0+np.nan

                    figc = go.Figure()
                    bar_text = _bar_text(dfY.values)
                    y_pad = max(dfY.max() * 0.15, 1.0) if not dfY.empty else 1.0
                    
                    if chart in ("Bar","Bar + Line"):
                        figc.add_bar(x=dfY.index, y=dfY.values, name=f"Y ({agg})",
                                    marker_color=color_pv_bar,
                                    text=bar_text, textposition="outside",
                                    textfont=dict(color=color_pv_txt),
                                    cliponaxis=False)
                    if chart in ("Line","Bar + Line") and dfZ is not None:
                        line_text = [f"{v:.1f}%" if z_mode=="% share of Y" else (f"{v:,.0f}" if not pd.isna(v) else "") for v in dfZ.values] if show_all_line_lbl_pv else _sparse_line_labels(dfZ.values, fmt=(lambda v: f"{v:.1f}%") if z_mode=="% share of Y" else (lambda v: f"{v:,.0f}"))
                        figc.add_scatter(x=dfY.index, y=dfZ.values, yaxis="y2",
                                        mode="lines+markers+text", name="Z (line)",
                                        line=dict(color=color_pv_line),
                                        text=line_text, textposition="top center",
                                        textfont=dict(color=color_pv_txt))
                    figc.update_layout(
                        xaxis_title=x_title,
                        yaxis=dict(title=f"Y = {y_col} [{agg}]", range=[0, float(dfY.max()+y_pad)] if not dfY.empty else [0,1], title_standoff=8),
                        yaxis2=dict(title=("Z = % share" if z_mode=="% share of Y" else f"Z = {z_col} [{z_agg}]"),
                                    overlaying="y", side="right", showgrid=False, title_standoff=14) if (chart!="Bar" and dfZ is not None) else None,
                        margin=dict(l=10,r=90,t=10,b=10), showlegend=True, height=460,
                        uniformtext_minsize=10, uniformtext_mode="hide"
                    )
                    st.plotly_chart(figc, use_container_width=True, config={"displayModeBar": False})

                    with st.expander("üìÑ Custom chart (table)"):
                        out_tbl = pd.DataFrame({"X": dfY.index, "Y": dfY.values})
                        
                        # (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG)
                        out_tbl_styled = out_tbl.copy()
                        out_tbl_styled["Y"] = out_tbl_styled["Y"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                        
                        if dfZ is not None:
                            out_tbl_styled["Z"] = out_tbl["Z"]
                            if z_mode == "% share of Y":
                                out_tbl_styled["Z"] = out_tbl_styled["Z"].map(lambda x: f"{x:.1f}%" if pd.notna(x) else "‚Äî")
                            elif z_agg == "count" or (z_col and z_col in df.columns and ("quantity" in z_col.lower() or "weight" in z_col.lower() or "amount" in z_col.lower())):
                                out_tbl_styled["Z"] = out_tbl_styled["Z"].map(lambda x: f"{x:,.0f}" if pd.notna(x) else "‚Äî")
                            else:
                                out_tbl_styled["Z"] = out_tbl_styled["Z"].map(lambda x: f"{x:,.2f}" if pd.notna(x) else "‚Äî")
                                
                        st.dataframe(out_tbl_styled, use_container_width=True, hide_index=True)

# ============================== TAB 2 ‚Äî PROFILING / DISTRIBUTION ==============================
with TAB2:
    import numpy as np
    import pandas as pd
    import plotly.graph_objects as go
    import streamlit as st

    # SciPy (n·∫øu c√≥) ƒë·ªÉ ki·ªÉm ƒë·ªãnh Normality / skew-kurtosis chu·∫©n h∆°n
    try:
        from scipy import stats
        _HAS_SCIPY = True
    except Exception:
        _HAS_SCIPY = False
    MAX_POINTS_PROFILE = 500_000
    st.subheader("üìä Profiling / Distribution")

    df = st.session_state.get("df")
    if df is None or df.empty:
        st.info("H√£y n·∫°p d·ªØ li·ªáu tr∆∞·ªõc.")
        st.stop()

    # ------------------------- Helpers -------------------------
    MAX_TIME_OPTIONS = {"M": 240, "Q": 80, "Y": 40}  # gi·ªõi h·∫°n s·ªë k·ª≥ hi·ªÉn th·ªã ƒë·ªÉ UI m∆∞·ª£t

    def _fmt_safe(x, fmt=".3f", na="‚Äî"):
        """Format s·ªë an to√†n; NaN/None/¬±inf ‚Üí na."""
        try:
            xv = float(x)
            if not np.isfinite(xv):
                return na
            return format(xv, fmt)
        except Exception:
            return na

    def _clean_time(ts, min_year=1900, max_year=2100):
        t = pd.to_datetime(ts, errors="coerce")
        bad = t.notna() & ((t.dt.year < min_year) | (t.dt.year > max_year))
        return t.mask(bad)

    def _top_values(df_local, col, k=200):
        if not col or col not in df_local.columns:
            return []
        return df_local[col].astype(str).value_counts(dropna=False).head(k).index.tolist()

    # m√†u c·ªë ƒë·ªãnh cho c√°c m·ªëc
    MARK_COLORS = {
        "Min":    "#7f8c8d",
        "Q1":     "#8e44ad",
        "Median": "#e84393",
        "Mean":   "#f1c40f",
        "Q3":     "#27ae60",
        "Max":    "#2d3436",
    }

    def _add_vlines_with_legend(
        fig, marks, y_max, dash="dot", annotate=True, label_font_size=11
    ):
        """
        V·∫Ω vline cho c√°c m·ªëc; label hi·ªÉn th·ªã ·ªü legend b√™n ph·∫£i.
        ƒê·ªìng th·ªùi g·∫Øn nh√£n tr√™n bi·ªÉu ƒë·ªì (so le theo chi·ªÅu cao ƒë·ªÉ tr√°nh ch·ªìng nhau).
        """
        n = len(marks)
        levels = np.linspace(0.92, 0.72, num=n) if n > 1 else [0.90]
        for (lab, xv), frac in zip(marks.items(), levels):
            if xv is None:
                continue
            try:
                xfloat = float(xv)
                if not np.isfinite(xfloat):
                    continue
            except Exception:
                continue

            col = MARK_COLORS.get(lab, "#888")

            # ƒë∆∞·ªùng d·ªçc + legend
            fig.add_scatter(
                x=[xfloat, xfloat],
                y=[0.0, float(y_max)],
                mode="lines",
                name=str(lab),
                line=dict(color=col, dash=dash, width=1.5),
                hovertemplate=f"{lab}: %{{x:,.4g}}<extra></extra>",
                showlegend=True,
            )
            # nh√£n ngay tr√™n chart (so le ƒë·ªÉ kh√¥ng ch·ªìng)
            if annotate:
                fig.add_annotation(
                    x=xfloat, y=float(y_max) * float(frac),
                    xref="x", yref="y",
                    text=str(lab),
                    showarrow=False,
                    font=dict(size=label_font_size, color=col),
                    bordercolor="rgba(0,0,0,0)",
                    bgcolor="rgba(0,0,0,0)",
                    xanchor="center", yanchor="bottom",
                    align="center"
                )

    # ---------- Drill-down ƒë√∫ng UI ----------
    def drilldown_panel_distribution(df: pd.DataFrame, prefix="pr"):
        st.markdown("### üîé Drill-down filter ‚Äî Khoanh v√πng d·ªØ li·ªáu")
        ckR, ckC, ckP, ckU, ckT = st.columns([1, 1, 1, 1, 1])
        useR = ckR.checkbox("Region", key=f"{prefix}_useR")
        useC = ckC.checkbox("Channel", key=f"{prefix}_useC")
        useP = ckP.checkbox("Product", key=f"{prefix}_useP")
        useU = ckU.checkbox("Customer", key=f"{prefix}_useU")
        useT = ckT.checkbox("Time", key=f"{prefix}_useT", value=True)

        time_col = None
        per_rule = "M"
        sel_periods = []
        region_col = channel_col = prod_col = cust_col = None
        selR = selC = selP = selU = []

        if useT:
            st.caption("C·ªôt th·ªùi gian")
            time_col = st.selectbox(
                " ", ["‚Äî"] + list(df.columns),
                index=0, key=f"{prefix}_timecol", label_visibility="collapsed"
            )
            st.caption("Granularity")
            per_txt = st.radio(
                " ", ["Month", "Quarter", "Year"],
                horizontal=True, key=f"{prefix}_gran", label_visibility="collapsed"
            )
            per_rule = {"Month": "M", "Quarter": "Q", "Year": "Y"}[per_txt]
            if time_col and time_col != "‚Äî":
                t = _clean_time(df[time_col])
                periods = t.dt.to_period(per_rule).astype(str).dropna()
                uniq = sorted(periods.unique().tolist())
                cap = MAX_TIME_OPTIONS[per_rule]
                if len(uniq) > cap:
                    uniq = uniq[-cap:]
                st.caption("Kho·∫£ng th·ªùi gian")
                sel_periods = st.multiselect(
                    " ", uniq, default=uniq[-1:] if uniq else [],
                    key=f"{prefix}_selT", label_visibility="collapsed"
                )

        if useR:
            region_col = st.selectbox("C·ªôt Region", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colR")
            if region_col and region_col != "‚Äî":
                selR = st.multiselect("Region (top 200)", _top_values(df, region_col), key=f"{prefix}_valR")
        if useC:
            channel_col = st.selectbox("C·ªôt Channel", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colC")
            if channel_col and channel_col != "‚Äî":
                selC = st.multiselect("Channel (top 200)", _top_values(df, channel_col), key=f"{prefix}_valC")
        if useP:
            prod_col = st.selectbox("C·ªôt Product", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colP")
            if prod_col and prod_col != "‚Äî":
                selP = st.multiselect("Product (top 200)", _top_values(df, prod_col), key=f"{prefix}_valP")
        if useU:
            cust_col = st.selectbox("C·ªôt Customer", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colU")
            if cust_col and cust_col != "‚Äî":
                selU = st.multiselect("Customer (top 200)", _top_values(df, cust_col), key=f"{prefix}_valU")

        # mask
        mask = pd.Series(True, index=df.index)
        if useT and time_col and time_col != "‚Äî" and sel_periods:
            cur = _clean_time(df[time_col]).dt.to_period(per_rule).astype(str)
            mask &= cur.isin(set(sel_periods))
        if useR and region_col and region_col != "‚Äî" and selR:
            mask &= df[region_col].astype(str).isin(selR)
        if useC and channel_col and channel_col != "‚Äî" and selC:
            mask &= df[channel_col].astype(str).isin(selC)
        if useP and prod_col and prod_col != "‚Äî" and selP:
            mask &= df[prod_col].astype(str).isin(selP)
        if useU and cust_col and cust_col != "‚Äî" and selU:
            mask &= df[cust_col].astype(str).isin(selU)

        return (time_col if time_col != "‚Äî" else None), per_rule, region_col, channel_col, prod_col, cust_col, mask

    # ---- d√πng drilldown m·ªõi ----
    time_col, per_rule, region_col, channel_col, prod_col, cust_col, mask = drilldown_panel_distribution(df, "pr")
    dfx = df.loc[mask].copy()
    if dfx.empty:
        st.warning("Kh√¥ng c√≤n d·ªØ li·ªáu sau khi khoanh v√πng.")
        st.stop()

    # ---------- ch·ªçn bi·∫øn numeric ----------
    NUMS = dfx.select_dtypes(include=[np.number]).columns.tolist()
    st.markdown("### üßÆ Ch·ªçn bi·∫øn numeric")
    ncol = st.selectbox("Metric (numeric)", NUMS or ["‚Äî"], key="pr_num_sel")
    if (not ncol) or (ncol not in dfx.columns):
        st.info("Ch∆∞a ch·ªçn bi·∫øn numeric h·ª£p l·ªá.")
        st.stop()

    # ---------- l√†m s·∫°ch (log10/>0, b·ªè =0, b·ªè <0) ----------
    st.markdown("### üßπ L√†m s·∫°ch & tu·ª≥ ch·ªçn")
    o1, o2, o3, o4 = st.columns([1, 1, 1, 1])
    use_log = o1.checkbox("log10 (ch·ªâ >0)", value=False, key="pr_log")
    drop_eq0 = o2.checkbox("B·ªè = 0", value=False, key="pr_eq0")
    drop_lt0 = o3.checkbox("B·ªè < 0", value=False, key="pr_lt0")
    show_points_ecdf = o4.checkbox("ECDF points", value=False, key="pr_ecdf_pts")

    s_raw = pd.to_numeric(dfx[ncol], errors="coerce").replace([np.inf, -np.inf], np.nan)
    if drop_lt0:
        s_raw = s_raw[s_raw >= 0]
    if drop_eq0:
        s_raw = s_raw[s_raw != 0]
    if use_log:
        s = s_raw[s_raw > 0].copy()
        s = np.log10(s)
        x_title = f"log10({ncol})"
        log_note = " (log10)"
    else:
        s = s_raw.copy()
        x_title = ncol
        log_note = ""

    s = s.dropna()
    if s.empty:
        st.warning("D·ªØ li·ªáu r·ªóng sau khi √°p ƒëi·ªÅu ki·ªán. H√£y n·ªõi b·ªô l·ªçc.")
        st.stop()
    if len(s) > MAX_POINTS_PROFILE:
        s_sampled_charts = s.sample(MAX_POINTS_PROFILE, random_state=42)
    else:
        s_sampled_charts = s

    MAX_STATS_SAMPLE = 500_000
    if len(s) > MAX_STATS_SAMPLE:
        s_stats_sample = s.sample(MAX_STATS_SAMPLE, random_state=42)
    else:
        s_stats_sample = s

    # ---------- Metric t·ªïng h·ª£p (2 c·ªôt; gi·∫£i th√≠ch ƒë∆∞a xu·ªëng ph·∫ßn Nh·∫≠n ƒë·ªãnh) ----------
    desc = s.describe(percentiles=[.05, .25, .5, .75, .95]).to_dict()
    mean_v, median_v = float(s.mean()), float(s.median())
    try:
        mode_v = float(pd.Series(s).mode(dropna=True).iloc[0])
    except Exception:
        mode_v = np.nan
    std_v = float(s.std(ddof=1)) if len(s) > 1 else np.nan
    iqr_v = float(s.quantile(.75) - s.quantile(.25))
    cv_v = float(std_v / mean_v * 100) if (mean_v != 0 and np.isfinite(mean_v) and np.isfinite(std_v)) else np.nan
    if _HAS_SCIPY and len(s) > 2:
        skew_v = float(stats.skew(s_stats_sample))
    else:
        skew_v = float(pd.Series(s).skew()) if len(s) > 2 else np.nan
    if _HAS_SCIPY and len(s) > 3:
        kurt_v = float(stats.kurtosis(s_stats_sample, fisher=True))
    else:
        kurt_v = float(pd.Series(s).kurt()) if len(s) > 3 else np.nan
    if _HAS_SCIPY and len(s) > 7:
        try:
            p_norm = float(stats.normaltest(s_stats_sample)[1])  # D‚ÄôAgostino K^2
        except Exception:
            p_norm = np.nan
    else:
        p_norm = np.nan

    miss = int(dfx[ncol].isna().sum())
    zero_cnt = int((pd.to_numeric(dfx[ncol], errors="coerce") == 0).sum())
    q1, q3 = float(s.quantile(.25)), float(s.quantile(.75))
    lf, uf = q1 - 1.5 * (q3 - q1), q3 + 1.5 * (q3 - q1)
    out_mask = (s < lf) | (s > uf)
    out_cnt = int(out_mask.sum())
    out_pct = (out_cnt / len(s) * 100.0) if len(s) else 0.0
    range_val = float(s.max() - s.min())

    skew_dir = (
        "l·ªách ph·∫£i (ƒëu√¥i ph·∫£i)" if (np.isfinite(skew_v) and skew_v > 0.5)
        else ("l·ªách tr√°i (ƒëu√¥i tr√°i)" if (np.isfinite(skew_v) and skew_v < -0.5) else "g·∫ßn ƒë·ªëi x·ª©ng")
    )
    tail_txt = (
        "ƒëu√¥i d√†y h∆°n chu·∫©n (leptokurtic)" if (np.isfinite(kurt_v) and kurt_v > 0)
        else ("ƒëu√¥i m·ªèng (platykurtic)" if (np.isfinite(kurt_v) and kurt_v < 0) else "g·∫ßn chu·∫©n (mesokurtic)")
    )
    if p_norm == p_norm:
        normal_txt = "kh√¥ng b√°c b·ªè gi·∫£ thuy·∫øt **chu·∫©n**" if p_norm >= 0.05 else "b√°c b·ªè gi·∫£ thuy·∫øt **chu·∫©n**"
    else:
        normal_txt = "kh√¥ng ki·ªÉm ƒë·ªãnh do n nh·ªè / thi·∫øu SciPy"
    c_tend = "Mean > Median" if mean_v > median_v else ("Mean < Median" if mean_v < median_v else "Mean ‚âà Median")
    spread_g = "ph√¢n t√°n r·ªông" if (cv_v == cv_v and cv_v > 50) else ("trung b√¨nh" if (cv_v == cv_v and cv_v > 20) else "kh√° ch·∫∑t")

    st.markdown("### üìã Metric t·ªïng h·ª£p (Shape ‚Ä¢ Spread ‚Ä¢ Central tendency)")
    metric_tbl = pd.DataFrame(
        [
            ("Count", f"{len(s):,}", "S·ªë m·∫´u sau l√†m s·∫°ch"),
            ("Missing", f"{miss:,}", "Gi√° tr·ªã thi·∫øu (tr∆∞·ªõc log/lo·∫°i)"),
            ("Zero (=0)", f"{zero_cnt:,}", "S·ªë gi√° tr·ªã b·∫±ng 0 (tr∆∞·ªõc log)"),
            ("Min", _fmt_safe(desc.get("min")), "Nh·ªè nh·∫•t"),
            ("P5", _fmt_safe(desc.get("5%")), "5th percentile"),
            ("Q1", _fmt_safe(q1), "25th percentile"),
            ("Median", _fmt_safe(median_v), "Trung v·ªã (50%)"),
            ("Mean", _fmt_safe(mean_v), "Trung b√¨nh"),
            ("Mode", _fmt_safe(mode_v), "Gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t"),
            ("Q3", _fmt_safe(q3), "75th percentile"),
            ("P95", _fmt_safe(desc.get("95%")), "95th percentile"),
            ("Max", _fmt_safe(desc.get("max")), "L·ªõn nh·∫•t"),
            ("Std (œÉ)", _fmt_safe(std_v), "ƒê·ªô l·ªách chu·∫©n"),
            ("IQR", _fmt_safe(iqr_v), "Q3 ‚àí Q1 (ph·∫ßn gi·ªØa)"),
            ("CV (%)", _fmt_safe(cv_v, ".2f"), "ƒê·ªô bi·∫øn thi√™n t∆∞∆°ng ƒë·ªëi"),
            ("Skewness", _fmt_safe(skew_v), "ƒê·ªô l·ªách tr√°i/ph·∫£i"),
            ("Kurtosis (excess)", _fmt_safe(kurt_v), "ƒê·ªô nh·ªçn so v·ªõi chu·∫©n (0 = chu·∫©n)"),
            ("Range", _fmt_safe(range_val), "Kho·∫£ng tr·∫£i r·ªông (max ‚àí min)"),
            ("Lower fence", _fmt_safe(lf), "Q1 ‚àí 1.5√óIQR (m·ªëc outlier)"),
            ("Upper fence", _fmt_safe(uf), "Q3 + 1.5√óIQR (m·ªëc outlier)"),
            ("Outliers (count, %)", f"{out_cnt:,} ({_fmt_safe(out_pct, '.2f')}%)", "S·ªë l∆∞·ª£ng/ t·ª∑ l·ªá ƒëi·ªÉm v∆∞·ª£t fence"),
            ("Normality p-value", _fmt_safe(p_norm), "p‚â•0.05 ‚Üí d·ªØ li·ªáu c√≥ th·ªÉ coi l√† g·∫ßn chu·∫©n"),
        ],
        columns=["Metric", "Value", "Gi·∫£i th√≠ch"],
    )
    st.dataframe(
        metric_tbl, use_container_width=True, hide_index=True,
        height=min(520, 34 * (len(metric_tbl) + 1)),
    )

    # ---------- Nh·∫≠n ƒë·ªãnh g·ªôp to√†n b·ªô gi·∫£i th√≠ch ----------
    st.markdown(
        f"""
### üß† Nh·∫≠n ƒë·ªãnh t·ª´ d·ªØ li·ªáu hi·ªán t·∫°i{log_note}

**H√¨nh d·∫°ng (Shape) & Chu·∫©n ho√°**  
‚Ä¢ **Skewness** = `{_fmt_safe(skew_v,'.3f')}` ‚áí **{skew_dir}** (d∆∞∆°ng: ƒëu√¥i ph·∫£i; √¢m: ƒëu√¥i tr√°i; ‚âà0: g·∫ßn ƒë·ªëi x·ª©ng).  
‚Ä¢ **Kurtosis (excess)** = `{_fmt_safe(kurt_v,'.3f')}` ‚áí **{tail_txt}** (0 ‚âà Gaussian).  
‚Ä¢ **Normality (D‚ÄôAgostino K¬≤)**: p-value = `{_fmt_safe(p_norm,'.3f')}` ‚áí {normal_txt}. *(Quy ∆∞·ªõc: p‚â•0.05 ‚áí ch∆∞a c√≥ b·∫±ng ch·ª©ng l·ªách chu·∫©n).*

**Xu h∆∞·ªõng trung t√¢m**  
‚Ä¢ **Mean** = `{_fmt_safe(mean_v,'.4g')}`, **Median** = `{_fmt_safe(median_v,'.4g')}`, **Mode** = `{_fmt_safe(mode_v,'.4g')}` ‚Üí **{c_tend}**.

**ƒê·ªô ph√¢n t√°n (Spread)**  
‚Ä¢ **œÉ (Std)** = `{_fmt_safe(std_v,'.4g')}`, **IQR** = `{_fmt_safe(iqr_v,'.4g')}`, **CV** = `{_fmt_safe(cv_v,'.2f')}%` ‚áí m·ª©c ph√¢n t√°n **{spread_g}**.  
‚Ä¢ **Range** = `max ‚àí min = {_fmt_safe(range_val,'.4g')}` cho bi·∫øt ƒë·ªô tr·∫£i r·ªông t·ªïng th·ªÉ.

**Kho·∫£ng ki·ªÉm so√°t & Outlier (Tukey fence)**  
‚Ä¢ **Fence**: **[{_fmt_safe(lf,'.4g')} ; {_fmt_safe(uf,'.4g')}]** (Q1‚àí1.5√óIQR ; Q3+1.5√óIQR).  
‚Ä¢ **Outliers** v∆∞·ª£t fence: **{out_cnt:,}** ƒëi·ªÉm (**{_fmt_safe(out_pct,'.2f')}%**) ‚Üí m·ª©c ·∫£nh h∆∞·ªüng {("ƒë√°ng k·ªÉ" if out_pct>5 else ("v·ª´a" if out_pct>1 else "th·∫•p"))}.
"""
    )

    # ---------- m√†u & bins cho ph·∫ßn bi·ªÉu ƒë·ªì ch√≠nh ----------
    with st.expander("üé® T√πy bi·∫øn hi·ªÉn th·ªã (m√†u/bins)", expanded=False):
        c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
        clr_hist = c1.color_picker("Histogram (bars)", "#74b9ff")
        clr_bell = c2.color_picker("Bell (Normal)", "#e67e22")
        clr_ecdf = c3.color_picker("ECDF", "#1abc9c")
        clr_box  = c4.color_picker("Box/Violin", "#a29bfe")
        bins = st.slider("S·ªë bins (Histogram)", 20, 120, 50, 2, key="pr_bins")

    # ---------- Charts ----------
    st.markdown("### üìà Ph√¢n ph·ªëi (Histogram) & ECDF")
    gL, gR = st.columns(2)

    # Histogram + Bell v·ªõi legend m·ªëc ·ªü b√™n ph·∫£i + nh√£n ngay tr√™n ƒë·ªì th·ªã
    with gL:
        xs = np.linspace(float(s.min()), float(s.max()), 400)
        sigma = float(s.std(ddof=1)) if len(s) > 1 else np.nan
        mu = float(s.mean())

        if np.isfinite(sigma) and sigma > 0:
            bell = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((xs - mu) / sigma) ** 2)
            binw = (s.max() - s.min()) / bins if (np.ptp(s) > 0 and bins > 0) else 1.0
            bell_scaled = bell * len(s) * binw
        else:
            bell_scaled = np.zeros_like(xs)

        # l·∫•y y_max ƒë·ªÉ k√©o vlines t·ªõi ƒë·ªânh plot
        counts, _ = np.histogram(s, bins=bins)
        y_max = max(
            float(counts.max()) if len(counts) else 0.0,
            float(bell_scaled.max() if len(bell_scaled) else 0.0)
        ) * 1.05

        figH = go.Figure()
        # 1. Bin d·ªØ li·ªáu b·∫±ng NumPy (c·ª±c nhanh)
        counts, bin_edges = np.histogram(s, bins=bins)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2 # L·∫•y ƒëi·ªÉm gi·ªØa c·ªßa m·ªói bin

        # 2. L·∫•y y_max t·ª´ counts (ƒë√£ bin)
        y_max = float(counts.max()) if len(counts) > 0 else 0.0

        figH = go.Figure()

        # 3. D√πng go.Bar ƒë·ªÉ v·∫Ω c√°c bin (ch·ªâ g·ª≠i 'bins' (50) ƒëi·ªÉm d·ªØ li·ªáu)
        figH.add_trace(go.Bar(
            x=bin_centers,
            y=counts,
            name="Frequency",
            marker_color=clr_hist,
            width=(bin_edges[1]-bin_edges[0]) # ƒê·ªô r·ªông c·ªßa bin
        ))

        # 4. T√≠nh to√°n Bell curve (nh∆∞ c≈©)
        xs = np.linspace(float(s.min()), float(s.max()), 400)
        sigma = float(s.std(ddof=1)) if len(s) > 1 else np.nan
        mu = float(s.mean())

        if np.isfinite(sigma) and sigma > 0:
            bell = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((xs - mu) / sigma) ** 2)
            # S·ª≠a 'binw' ƒë·ªÉ kh·ªõp v·ªõi t√≠nh to√°n c·ªßa NumPy
            binw = (bin_edges[1]-bin_edges[0]) if len(bin_edges)>1 else 1.0
            bell_scaled = bell * len(s) * binw
        else:
            bell_scaled = np.zeros_like(xs)

        # 5. Th√™m bell curve v√† vlines (nh∆∞ c≈©)
        figH.add_scatter(
            x=xs, y=bell_scaled, mode="lines",
            name="Normal bell (scaled)", line=dict(color=clr_bell, width=2)
        )

        # C·∫≠p nh·∫≠t y_max ƒë·ªÉ bao g·ªìm c·∫£ bell curve
        y_max = max(y_max, float(bell_scaled.max() if len(bell_scaled) else 0.0)) * 1.05

        marks = {
            "Min": float(s.min()),
            "Q1": q1,
            "Median": median_v,
            "Mean": mean_v,
            "Q3": q3,
            "Max": float(s.max()),
        }
        _add_vlines_with_legend(figH, marks, y_max, annotate=True)
        st.plotly_chart(figH, use_container_width=True, config={"displayModeBar": False})
        st.caption("C·ªôt th·ªÉ hi·ªán t·∫ßn su·∫•t; ƒë∆∞·ªùng *bell* cho bi·∫øt m·ª©c ƒë·ªô g·∫ßn chu·∫©n. C√°c m·ªëc Min/Q1/Median/Mean/Q3/Max hi·ªÉn th·ªã r√µ tr√™n bi·ªÉu ƒë·ªì v√† ·ªü legend b√™n ph·∫£i.")

    # ECDF v·ªõi legend m·ªëc ·ªü b√™n ph·∫£i + nh√£n tr√™n ƒë·ªì th·ªã
    with gR:
        s_sorted = np.sort(s_sampled_charts.values)
        y_ecdf = np.arange(1, len(s_sorted) + 1) / len(s_sorted)
        figE = go.Figure()
        figE.add_scatter(
            x=s_sorted, y=y_ecdf,
            mode="lines" if not show_points_ecdf else "lines+markers",
            name="ECDF", line=dict(color=clr_ecdf),
            hovertemplate="x=%{x:,.4g}<br>P=%{y:.3f}<extra></extra>"
        )

        ecdf_marks = {"Q1": q1, "Median": median_v, "Mean": mean_v, "Q3": q3}
        _add_vlines_with_legend(figE, ecdf_marks, 1.0, annotate=True)

        figE.update_layout(
            title=f"ECDF ‚Äî Cumulative Distribution of {x_title}",
            xaxis_title=x_title, yaxis_title="Probability",
            height=430,
            legend=dict(orientation="v", y=1, x=1.02, yanchor="top", xanchor="left"),
            margin=dict(l=10, r=160, t=40, b=10),
        )
        st.plotly_chart(figE, use_container_width=True, config={"displayModeBar": False})
        st.caption("ECDF cho bi·∫øt x√°c su·∫•t t√≠ch lu·ªπ P(X ‚â§ x). C√°c m·ªëc Q1/Median/Mean/Q3 ƒë∆∞·ª£c g·∫Øn m√†u ri√™ng v√† nh√£n tr·ª±c ti·∫øp tr√™n ƒë·ªì th·ªã.")

    # Box & Violin (Spread)
    st.markdown("### üß∑ Spread ‚Äî Box & Violin")
    b1, b2 = st.columns(2)
    with b1:
        figB = go.Figure()
        figB.add_box(y=s_sampled_charts, name=x_title, boxpoints="outliers", marker_color=clr_box)
        figB.update_layout(
            title="Box Plot", yaxis_title=x_title, height=400,
            margin=dict(l=10, r=10, t=40, b=10)
        )
        st.plotly_chart(figB, use_container_width=True, config={"displayModeBar": False})
        st.caption("H·ªôp gi·ªØa Q1‚ÄìQ3; ƒë∆∞·ªùng gi·ªØa l√† Median; ƒëi·ªÉm v∆∞·ª£t *fence* l√† outlier ti·ªÅm nƒÉng.")
    with b2:
        figV = go.Figure()
        figV.add_violin(y=s_sampled_charts, name=x_title, line_color=clr_box,
                        fillcolor="rgba(162,155,254,0.25)", meanline_visible=True)
        figV.update_layout(
            title="Violin Plot", yaxis_title=x_title, height=400,
            margin=dict(l=10, r=10, t=40, b=10), showlegend=False
        )
        st.plotly_chart(figV, use_container_width=True, config={"displayModeBar": False})
        st.caption("H√¨nh *violin* bi·ªÉu th·ªã m·∫≠t ƒë·ªô ph√¢n ph·ªëi; meanline hi·ªÉn th·ªã Mean/Median.")

# ============================== TAB 3 ‚Äî CORRELATION (BUSINESS-ORIENTED) ==============================
with TAB3:
    import numpy as np
    import pandas as pd
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    import streamlit as st

    # ===== Optional SciPy (p-value, Spearman nhanh, quantile Z) =====
    try:
        from scipy import stats
        _HAS_SCIPY = True
        _Z975 = stats.norm.ppf(0.975)
    except Exception:
        _HAS_SCIPY = False
        _Z975 = 1.96

    # ============================ Helpers ============================
    MAX_TIME_OPTIONS = {"M": 240, "Q": 80, "Y": 40}  # gi·ªõi h·∫°n #k·ª≥ cho UI m∆∞·ª£t

    def _fmt(x, fmt=".3f", na="‚Äî"):
        try:
            xv = float(x)
            if not np.isfinite(xv):
                return na
            return format(xv, fmt)
        except Exception:
            return na

    def _clean_time(ts, min_year=1900, max_year=2100):
        t = pd.to_datetime(ts, errors="coerce")
        bad = t.notna() & ((t.dt.year < min_year) | (t.dt.year > max_year))
        return t.mask(bad)

    def _top_values(df_local, col, k=200):
        if not col or col not in df_local.columns:
            return []
        return df_local[col].astype(str).value_counts(dropna=False).head(k).index.tolist()

    def strength_label(r_abs: float) -> str:
        if r_abs < 0.1: return "r·∫•t y·∫øu"
        if r_abs < 0.3: return "y·∫øu"
        if r_abs < 0.5: return "trung b√¨nh"
        if r_abs < 0.7: return "m·∫°nh"
        if r_abs < 0.9: return "r·∫•t m·∫°nh"
        return "g·∫ßn ho√†n h·∫£o"

    def fisher_ci(r: float, n: int):
        """CI 95% cho Pearson r (Fisher z)."""
        try:
            if not np.isfinite(r) or n is None or n <= 3 or abs(r) >= 0.9999:
                return (np.nan, np.nan)
            z = np.arctanh(r)
            se = 1.0 / np.sqrt(n - 3)
            zlo, zhi = z - _Z975*se, z + _Z975*se
            return (np.tanh(zlo), np.tanh(zhi))
        except Exception:
            return (np.nan, np.nan)

    def corr_one(x: pd.Series, y: pd.Series, method="pearson"):
        """Tr·∫£ v·ªÅ r, p, n (fallback n·∫øu kh√¥ng c√≥ SciPy)."""
        s = pd.concat([x, y], axis=1).dropna()
        s.columns = ["x", "y"]
        n = len(s)
        if n < 3:
            return np.nan, np.nan, n
        if _HAS_SCIPY:
            if method.lower() == "pearson":
                r, p = stats.pearsonr(s["x"], s["y"])
            elif method.lower() == "spearman":
                r, p = stats.spearmanr(s["x"], s["y"])
            else:
                r, p = stats.kendalltau(s["x"], s["y"])
        else:
            r = np.corrcoef(s["x"], s["y"])[0, 1]
            p = np.nan
        return r, p, n

    def prepare_xy(xs: pd.Series, ys: pd.Series, drop_lt0, drop_eq0, use_log):
        s = pd.concat([xs, ys], axis=1).rename(columns={xs.name: "x", ys.name: "y"})
        s = s.replace([np.inf, -np.inf], np.nan).dropna()
        if drop_lt0:
            s = s[(s["x"] >= 0) & (s["y"] >= 0)]
        if drop_eq0:
            s = s[(s["x"] != 0) & (s["y"] != 0)]
        if use_log:
            s = s[(s["x"] > 0) & (s["y"] > 0)]
            s["x"] = np.log10(s["x"])
            s["y"] = np.log10(s["y"])
        return s["x"], s["y"]

    def guard(ok: bool, msg: str) -> bool:
        if ok: return True
        st.info(msg); st.write("---")
        return False

    # ============================ Data ============================
    df = st.session_state.get("df")
    if df is None or df.empty:
        st.info("H√£y n·∫°p d·ªØ li·ªáu tr∆∞·ªõc.")
        st.stop()

    st.subheader("üîó Correlation")

    # ====================== Drill-down filter ======================
    def drilldown_panel_corr(df: pd.DataFrame, prefix="corr"):
        st.markdown("### üîé Drill-down filter ‚Äî Khoanh v√πng d·ªØ li·ªáu")
        ckR, ckC, ckP, ckU, ckT = st.columns([1, 1, 1, 1, 1])
        useR = ckR.checkbox("Region", key=f"{prefix}_useR")
        useC = ckC.checkbox("Channel", key=f"{prefix}_useC")
        useP = ckP.checkbox("Product", key=f"{prefix}_useP")
        useU = ckU.checkbox("Customer", key=f"{prefix}_useU")
        useT = ckT.checkbox("Time", key=f"{prefix}_useT", value=True)

        time_col = None
        per_rule = "M"
        sel_periods = []
        region_col = channel_col = prod_col = cust_col = None
        selR = selC = selP = selU = []

        if useT:
            st.caption("C·ªôt th·ªùi gian")
            time_col = st.selectbox(
                " ", ["‚Äî"] + list(df.columns),
                index=0, key=f"{prefix}_timecol", label_visibility="collapsed"
            )
            st.caption("Granularity")
            per_txt = st.radio(
                " ", ["Month", "Quarter", "Year"],
                horizontal=True, key=f"{prefix}_gran", label_visibility="collapsed"
            )
            per_rule = {"Month": "M", "Quarter": "Q", "Year": "Y"}[per_txt]
            if time_col and time_col != "‚Äî":
                t = _clean_time(df[time_col])
                periods = t.dt.to_period(per_rule).astype(str).dropna()
                uniq = sorted(periods.unique().tolist())
                cap = MAX_TIME_OPTIONS[per_rule]
                if len(uniq) > cap:
                    uniq = uniq[-cap:]
                st.caption("Kho·∫£ng th·ªùi gian")
                sel_periods = st.multiselect(
                    " ", uniq, default=uniq[-1:] if uniq else [],
                    key=f"{prefix}_selT", label_visibility="collapsed"
                )

        if useR:
            region_col = st.selectbox("C·ªôt Region", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colR")
            if region_col and region_col != "‚Äî":
                selR = st.multiselect("Region (top 200)", _top_values(df, region_col), key=f"{prefix}_valR")
        if useC:
            channel_col = st.selectbox("C·ªôt Channel", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colC")
            if channel_col and channel_col != "‚Äî":
                selC = st.multiselect("Channel (top 200)", _top_values(df, channel_col), key=f"{prefix}_valC")
        if useP:
            prod_col = st.selectbox("C·ªôt Product", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colP")
            if prod_col and prod_col != "‚Äî":
                selP = st.multiselect("Product (top 200)", _top_values(df, prod_col), key=f"{prefix}_valP")
        if useU:
            cust_col = st.selectbox("C·ªôt Customer", ["‚Äî"] + list(df.columns), index=0, key=f"{prefix}_colU")
            if cust_col and cust_col != "‚Äî":
                selU = st.multiselect("Customer (top 200)", _top_values(df, cust_col), key=f"{prefix}_valU")

        # mask
        mask = pd.Series(True, index=df.index)
        if useT and time_col and time_col != "‚Äî" and sel_periods:
            cur = _clean_time(df[time_col]).dt.to_period(per_rule).astype(str)
            mask &= cur.isin(set(sel_periods))
        if useR and region_col and region_col != "‚Äî" and selR:
            mask &= df[region_col].astype(str).isin(selR)
        if useC and channel_col and channel_col != "‚Äî" and selC:
            mask &= df[channel_col].astype(str).isin(selC)
        if useP and prod_col and prod_col != "‚Äî" and selP:
            mask &= df[prod_col].astype(str).isin(selP)
        if useU and cust_col and cust_col != "‚Äî" and selU:
            mask &= df[cust_col].astype(str).isin(selU)

        return (time_col if time_col != "‚Äî" else None), per_rule, region_col, channel_col, prod_col, cust_col, mask

    time_col, per_rule, region_col, channel_col, prod_col, cust_col, mask = drilldown_panel_corr(df, "corr")
    dfx = df.loc[mask].copy()
    if dfx.empty:
        st.warning("Kh√¥ng c√≤n d·ªØ li·ªáu sau khi khoanh v√πng.")
        st.stop()

    # ====================== Ch·ªçn bi·∫øn ======================
    st.markdown("### üéØ Ch·ªçn bi·∫øn (Target Y & Drivers X)")
    NUMS = dfx.select_dtypes(include=[np.number]).columns.tolist()
    if not NUMS:
        st.info("Kh√¥ng c√≥ c·ªôt numeric ƒë·ªÉ t√≠nh t∆∞∆°ng quan.")
        st.stop()

    c1, c2 = st.columns([1, 2])
    y_col = c1.selectbox("Target (numeric Y)", NUMS, index=0)

    numeric_wo_y = [c for c in NUMS if c != y_col]
    var_rank = dfx[numeric_wo_y].var(numeric_only=True).sort_values(ascending=False)
    x_default = var_rank.head(min(10, len(var_rank))).index.tolist()
    x_cols = c2.multiselect("Drivers X (numeric, multi-select)", numeric_wo_y, default=x_default)

    if not x_cols:
        st.info("Ch·ªçn √≠t nh·∫•t 1 bi·∫øn X ƒë·ªÉ t√≠nh t∆∞∆°ng quan.")
        st.stop()

    # ==================== L√†m s·∫°ch & tu·ª≥ ch·ªçn ====================
    st.markdown("### üßπ L√†m s·∫°ch & tu·ª≥ ch·ªçn")
    o1, o2, o3, o4 = st.columns([1, 1, 1, 1])
    drop_eq0 = o1.checkbox("B·ªè = 0", value=False)
    drop_lt0 = o2.checkbox("B·ªè < 0", value=False)
    use_log  = o3.checkbox("log10 (√°p d·ª•ng v·ªõi bi·∫øn >0)", value=False)
    method   = o4.radio("Ph∆∞∆°ng ph√°p", ["Pearson", "Spearman"], horizontal=True)

    # ================== ‚è± Trend theo th·ªùi gian (FIX) ==================
    st.markdown("### ‚è± Trend theo th·ªùi gian (Y & 1 driver)")
    if guard(time_col is not None, "B·∫≠t **Time** trong Drill-down ƒë·ªÉ xem Trend."):
        drv_for_trend = st.selectbox("Ch·ªçn 1 driver", x_cols, index=0, key="corr_trend_x")

        c1t, c2t, c3t = st.columns([1, 1, 1])
        aggY = c1t.radio("G·ªôp Y theo", ["sum", "mean"], horizontal=True, key="corr_aggY")
        aggX = c2t.radio("G·ªôp X theo", ["sum", "mean"], horizontal=True, key="corr_aggX")
        use_index = c3t.checkbox("Chu·∫©n ho√° Index = 100 (k·ª≥ ƒë·∫ßu)", value=True, key="corr_use_index")
        win = st.slider("C·ª≠a s·ªï Rolling-corr (s·ªë k·ª≥)", 3, 24, 6, key="corr_roll_win")

        # D√πng datetime th·ª±c cho tr·ª•c th·ªùi gian
        tdt = _clean_time(dfx[time_col])
        tmp = (pd.DataFrame({"t": tdt, "Y": dfx[y_col], "X": dfx[drv_for_trend]})
                 .replace([np.inf, -np.inf], np.nan).dropna())
        if drop_lt0: tmp = tmp[(tmp["Y"] >= 0) & (tmp["X"] >= 0)]
        if drop_eq0: tmp = tmp[(tmp["Y"] != 0) & (tmp["X"] != 0)]
        if use_log:
            tmp = tmp[(tmp["Y"] > 0) & (tmp["X"] > 0)]
            tmp["Y"] = np.log10(tmp["Y"]); tmp["X"] = np.log10(tmp["X"])

        def _agg_by(freq_code):
            p = tmp["t"].dt.to_period(freq_code)
            g = tmp.groupby(p).agg(
                Y=("Y", "sum" if aggY == "sum" else "mean"),
                X=("X", "sum" if aggX == "sum" else "mean"),
            ).sort_index()
            g.index = g.index.to_timestamp(how="start")  # datetime th·∫≠t
            return g

        try_order = [per_rule] + [f for f in ("Q", "M") if f != per_rule]  # n·∫øu √≠t k·ª≥ ‚Üí fallback
        g, used_freq = None, per_rule
        for f in try_order:
            gg = _agg_by(f)
            if len(gg) >= 2:
                g, used_freq = gg, f
                break

        if g is None:
            st.info("Ch·ªâ c√≥ 1 k·ª≥ sau khi nh√≥m theo th·ªùi gian. H√£y m·ªü r·ªông kho·∫£ng th·ªùi gian ho·∫∑c ch·ªçn granularity nh·ªè h∆°n.")
        else:
            g_plot = (g / g.iloc[0] * 100.0) if use_index else g
            y_left_title  = "Y (Index=100)" if use_index else y_col
            y_right_title = "X (Index=100)" if use_index else drv_for_trend

            figT = make_subplots(specs=[[{"secondary_y": True}]])
            figT.add_bar(
                x=g_plot.index, y=g_plot["Y"], name=y_col,
                marker_color="#74b9ff", opacity=0.9,
                hovertemplate="%{x|%Y-%m-%d}<br>Y=%{y:,.4g}<extra></extra>",
                secondary_y=False
            )
            figT.add_scatter(
                x=g_plot.index, y=g_plot["X"], name=drv_for_trend, mode="lines+markers",
                line=dict(color="#e84393", width=2), marker=dict(size=5),
                hovertemplate="%{x|%Y-%m-%d}<br>X=%{y:,.4g}<extra></extra>",
                secondary_y=True
            )
            figT.update_layout(
                height=420, bargap=0.35, hovermode="x unified",
                legend=dict(orientation="h", y=1.1, x=0),
                margin=dict(l=10, r=10, t=10, b=10),
                xaxis=dict(type="date")
            )
            if used_freq == "M":   figT.update_xaxes(dtick="M1",  tickformat="%b %Y")
            elif used_freq == "Q": figT.update_xaxes(dtick="M3",  tickformat="%b %Y")
            else:                   figT.update_xaxes(dtick="M12", tickformat="%Y")

            figT.update_yaxes(title_text=y_left_title, secondary_y=False)
            figT.update_yaxes(title_text=y_right_title, secondary_y=True, showgrid=False)
            st.plotly_chart(figT, use_container_width=True, config={"displayModeBar": False})

            # Rolling-corr
            if len(g) >= win:
                r_roll = g["Y"].rolling(win).corr(g["X"])
                figR = go.Figure()
                figR.add_scatter(
                    x=g.index, y=r_roll, mode="lines+markers",
                    name=f"Pearson-r rolling ({win})", line=dict(color="#2ecc71"),
                    marker=dict(size=5),
                    hovertemplate="%{x|%Y-%m-%d}<br>r=%{y:.3f}<extra></extra>"
                )
                figR.add_hline(y=0, line=dict(color="#95a5a6", dash="dot"))
                figR.update_layout(
                    height=300, margin=dict(l=10, r=10, t=10, b=10),
                    hovermode="x unified", yaxis=dict(range=[-1, 1])
                )
                figR.update_xaxes(type="date")
                st.plotly_chart(figR, use_container_width=True, config={"displayModeBar": False})

                last_r = r_roll.dropna().iloc[-1] if r_roll.notna().any() else np.nan
                delta_y = (g.iloc[-1, 0] / g.iloc[0, 0] - 1) * 100 if len(g) >= 2 else np.nan
                delta_x = (g.iloc[-1, 1] / g.iloc[0, 1] - 1) * 100 if len(g) >= 2 else np.nan
                lbl = "tƒÉng c√πng chi·ªÅu" if last_r == last_r and last_r > 0 else ("gi·∫£m ng∆∞·ª£c chi·ªÅu" if last_r == last_r and last_r < 0 else "kh√¥ng r√µ chi·ªÅu")
                st.markdown(
                    f"- **Di·ªÖn bi·∫øn**: Y `{_fmt(delta_y,'.1f')}%`, X `{_fmt(delta_x,'.1f')}%` t·ª´ k·ª≥ ƒë·∫ßu ‚Üí k·ª≥ cu·ªëi.  "
                    f"- **Rolling-r (g·∫ßn nh·∫•t)**: r={_fmt(last_r,'.3f')} ‚áí **{lbl}** trong c·ª≠a s·ªï {win} k·ª≥."
                )

    # ==================== T√≠nh t∆∞∆°ng quan X~Y ====================
    rows = []
    for col in x_cols:
        xx, yy = prepare_xy(dfx[col], dfx[y_col], drop_lt0, drop_eq0, use_log)
        r, p, n = corr_one(xx, yy, method=method)
        lo, hi = fisher_ci(r, n) if method.lower() == "pearson" else (np.nan, np.nan)
        rows.append({
            "X": col, "N": n, "r": r, "p_value": p, "CI_low": lo, "CI_high": hi,
            "abs_r": abs(r), "direction": "d∆∞∆°ng (+)" if r == r and r > 0 else ("√¢m (‚àí)" if r == r and r < 0 else "‚Äî"),
            "strength": strength_label(abs(r)) if r == r else "‚Äî",
        })
    corr_tbl = pd.DataFrame(rows).sort_values("abs_r", ascending=False).reset_index(drop=True)

    # ======================== KPIs nhanh ========================
    cA, cB, cC, cD = st.columns(4)
    cA.metric("S·ªë bi·∫øn X", f"{len(x_cols)}")
    cB.metric("n t·ªëi ƒëa", f"{int(corr_tbl['N'].max() if len(corr_tbl) else 0):,}")
    best = corr_tbl.iloc[0] if len(corr_tbl) else None
    cC.metric("M·∫°nh nh·∫•t (|r|)", f"{_fmt(best['abs_r'],'.3f') if best is not None else '‚Äî'}")
    sig_rate = (corr_tbl["p_value"] < 0.05).mean() if _HAS_SCIPY and len(corr_tbl) else np.nan
    cD.metric("% quan h·ªá p<.05", f"{_fmt(100*sig_rate,'.1f') if sig_rate==sig_rate else '‚Äî'}%")

    # ==================== Bar r + CI (Pearson) ====================
    st.markdown("### üìä Correlation v·ªõi Target (c√≥ kho·∫£ng tin c·∫≠y)")
    topN = st.slider("Hi·ªÉn th·ªã Top-N theo |r|", 3, min(30, len(corr_tbl)), min(10, len(corr_tbl)))
    view_df = corr_tbl.head(topN).copy()
    colors = np.where(view_df["r"] >= 0, "#27ae60", "#e74c3c")

    fig_bar = go.Figure()
    fig_bar.add_bar(
        x=view_df["X"], y=view_df["r"], marker_color=colors,
        text=[_fmt(v, ".3f") for v in view_df["r"]], textposition="outside", cliponaxis=False,
        error_y=dict(
            type="data",
            array=(view_df["CI_high"] - view_df["r"]).astype(float),
            arrayminus=(view_df["r"] - view_df["CI_low"]).astype(float),
            visible=True if method.lower() == "pearson" else False
        ),
        customdata=np.stack([
            view_df["CI_low"].fillna(np.nan),
            view_df["CI_high"].fillna(np.nan),
            view_df["N"].fillna(0),
            view_df["p_value"].fillna(np.nan)
        ], axis=1),
        hovertemplate=(
            "X=%{x}<br>r=%{y:.3f}"
            + ("<br>95% CI=[%{customdata[0]:.3f}; %{customdata[1]:.3f}]" if method.lower()=="pearson" else "")
            + "<br>n=%{customdata[2]:,}"
            + ("<br>p=%{customdata[3]:.4f}" if _HAS_SCIPY else "")
            + "<extra></extra>"
        )
    )
    fig_bar.update_layout(
        height=440, xaxis_title="Bi·∫øn X", yaxis_title=f"r ({method})",
        margin=dict(l=10, r=10, t=10, b=50),
        yaxis=dict(range=[min(-1, float(view_df["r"].min()) - 0.05), max(1, float(view_df["r"].max()) + 0.05)])
    )
    st.plotly_chart(fig_bar, use_container_width=True, config={"displayModeBar": False})

    # ==================== B·∫£ng chi ti·∫øt ====================
    st.markdown("### üìã B·∫£ng chi ti·∫øt (r, CI, p, n, d·∫•u, m·ª©c ƒë·ªô)")
    show_cols = ["X", "N", "r", "CI_low", "CI_high", "p_value", "direction", "strength"]
    st.dataframe(
        corr_tbl[show_cols].rename(columns={
            "X": "Bi·∫øn X", "N": "n", "r": "r", "CI_low": "CI th·∫•p", "CI_high": "CI cao",
            "p_value": "p-value", "direction": "D·∫•u", "strength": "M·ª©c ƒë·ªô"
        }),
        use_container_width=True, hide_index=True,
        height=min(480, 32*(len(corr_tbl)+1))
    )

    # ==================== Heatmap (gi·ªõi h·∫°n) ====================
    st.markdown("### üå°Ô∏è Heatmap t∆∞∆°ng quan")
    cmax1, cmax2 = st.columns([2, 1])
    max_h_cols = cmax2.slider("T·ªëi ƒëa s·ªë bi·∫øn", 4, min(20, len(x_cols) + 1), min(12, len(x_cols) + 1))
    top_vars = corr_tbl.head(max_h_cols - 1)["X"].tolist()
    hm_cols = [y_col] + top_vars
    sub = dfx[hm_cols].replace([np.inf, -np.inf], np.nan).dropna()
    if len(sub) >= 3:
        corrM = sub.corr(method="pearson" if method.lower()=="pearson" else "spearman")
        fig_hm = go.Figure(data=go.Heatmap(
            z=corrM.values, x=corrM.columns, y=corrM.index,
            zmin=-1, zmax=1, colorscale="RdBu", reversescale=True,
            colorbar=dict(title="r")
        ))
        fig_hm.update_layout(height=420, margin=dict(l=10, r=10, t=10, b=10))
        st.plotly_chart(fig_hm, use_container_width=True, config={"displayModeBar": False})
    else:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu s·∫°ch ƒë·ªÉ v·∫Ω heatmap.")

    # ==================== Scatter (top m·ªëi quan h·ªá) ====================
    st.markdown("### üîç Scatter (Top quan h·ªá)")
    top_scatter = corr_tbl.head(min(4, len(corr_tbl)))["X"].tolist()
    max_points = st.slider("Gi·ªõi h·∫°n s·ªë ƒëi·ªÉm/bi·ªÉu ƒë·ªì (sampling)", 500, 10000, 3000, 500)
    if top_scatter:
        rows = int(np.ceil(len(top_scatter) / 2))
        fig_sc = make_subplots(rows=rows, cols=2, subplot_titles=[f"{x} vs {y_col}" for x in top_scatter])
        r_idx, c_idx = 1, 1
        for x in top_scatter:
            s = pd.concat([dfx[x], dfx[y_col]], axis=1).replace([np.inf, -np.inf], np.nan).dropna()
            if drop_lt0: s = s[(s.iloc[:, 0] >= 0) & (s.iloc[:, 1] >= 0)]
            if drop_eq0: s = s[(s.iloc[:, 0] != 0) & (s.iloc[:, 1] != 0)]
            if use_log:
                s = s[(s.iloc[:, 0] > 0) & (s.iloc[:, 1] > 0)]
                s = np.log10(s)

            if len(s) > max_points:
                s = s.sample(max_points, random_state=42)

            r_sc, p_sc, n_sc = corr_one(s.iloc[:, 0], s.iloc[:, 1], method=method)
            fig_sc.add_trace(
                go.Scattergl(x=s.iloc[:, 0], y=s.iloc[:, 1], mode="markers",
                             marker=dict(size=4, opacity=0.6),
                             hovertemplate=f"{x}=%{{x:,.4g}}<br>{y_col}=%{{y:,.4g}}<extra></extra>",
                             showlegend=False),
                row=r_idx, col=c_idx
            )
            # ƒë∆∞·ªùng fit tuy·∫øn t√≠nh s∆° b·ªô
            if len(s) >= 2 and np.ptp(s.iloc[:, 0]) > 0:
                coefs = np.polyfit(s.iloc[:, 0], s.iloc[:, 1], deg=1)
                xs = np.linspace(s.iloc[:, 0].min(), s.iloc[:, 0].max(), 100)
                ys = coefs[0] * xs + coefs[1]
                fig_sc.add_trace(go.Scatter(x=xs, y=ys, mode="lines", line=dict(color="#d35400"),
                                            showlegend=False), row=r_idx, col=c_idx)

            fig_sc.update_xaxes(title_text=x, row=r_idx, col=c_idx)
            fig_sc.update_yaxes(title_text=y_col, row=r_idx, col=c_idx)

            # Annotation r/p/n ‚Äî FIX xref/yref cho subplot ƒë·∫ßu ti√™n
            axis_num = (r_idx - 1) * 2 + c_idx
            xref = "x domain" if axis_num == 1 else f"x{axis_num} domain"
            yref = "y domain" if axis_num == 1 else f"y{axis_num} domain"
            fig_sc.add_annotation(
                xref=xref, yref=yref, x=1.0, y=1.04, xanchor="right",
                text=f"r={_fmt(r_sc,'.3f')}, n={n_sc}{'' if not _HAS_SCIPY else f', p={_fmt(p_sc,'.3g')}'}",
                showarrow=False, font=dict(size=12)
            )

            c_idx += 1
            if c_idx > 2: c_idx = 1; r_idx += 1

        fig_sc.update_layout(height=300*rows, margin=dict(l=10, r=10, t=30, b=10))
        st.plotly_chart(fig_sc, use_container_width=True, config={"displayModeBar": False})
    else:
        st.info("Ch·ªçn th√™m bi·∫øn X ƒë·ªÉ xem scatter.")

    # ==================== Collinearity gi·ªØa c√°c X ====================
    st.markdown("### üßØ C·∫£nh b√°o collinearity gi·ªØa c√°c X")
    if len(x_cols) >= 2:
        subx = dfx[x_cols].replace([np.inf, -np.inf], np.nan).dropna()
        if len(subx) >= 5:
            cxx = subx.corr(method="pearson" if method.lower()=="pearson" else "spearman").abs()
            pairs, cols = [], cxx.columns.tolist()
            for i in range(len(cols)):
                for j in range(i+1, len(cols)):
                    pairs.append((cols[i], cols[j], cxx.iloc[i, j]))
            col_warn = pd.DataFrame(pairs, columns=["X1", "X2", "|r|"]).sort_values("|r|", ascending=False)
            st.dataframe(col_warn.head(10), use_container_width=True, hide_index=True)
            if (col_warn["|r|"] > 0.8).any():
                st.warning("C√≥ c·∫∑p X t∆∞∆°ng quan cao (|r|>0.8). C√¢n nh·∫Øc ch·ªçn b·ªõt ƒë·ªÉ tr√°nh tr√πng th√¥ng tin.")
        else:
            st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu s·∫°ch ƒë·ªÉ ki·ªÉm tra collinearity.")
    else:
        st.caption("C·∫ßn ‚â•2 bi·∫øn X ƒë·ªÉ ki·ªÉm tra collinearity.")

    # ==================== Nh·∫≠n ƒë·ªãnh t·ª´ d·ªØ li·ªáu hi·ªán t·∫°i ====================
    st.markdown("### üß† Nh·∫≠n ƒë·ªãnh t·ª´ d·ªØ li·ªáu hi·ªán t·∫°i")
    bullets = []

    pos = corr_tbl[corr_tbl["r"] > 0].head(3)
    neg = corr_tbl[corr_tbl["r"] < 0].head(3)

    if not pos.empty:
        s_txt = "; ".join(
            [f"{r.X} (r={_fmt(r.r,'.3f')}, {r.strength}, {r.direction}"
             + (f", CI[{_fmt(r.CI_low,'.3f')};{_fmt(r.CI_high,'.3f')}]" if method.lower()=="pearson" else "")
             + (f", p={_fmt(r.p_value,'.3g')}" if _HAS_SCIPY else "")
             + f", n={int(r.N)})"
             for _, r in pos.iterrows()]
        )
        bullets.append(f"**TƒÉng c√πng chi·ªÅu v·ªõi {y_col}**: {s_txt}.")

    if not neg.empty:
        s_txt = "; ".join(
            [f"{r.X} (r={_fmt(r.r,'.3f')}, {r.strength}, {r.direction}"
             + (f", CI[{_fmt(r.CI_low,'.3f')};{_fmt(r.CI_high,'.3f')}]" if method.lower()=="pearson" else "")
             + (f", p={_fmt(r.p_value,'.3g')}" if _HAS_SCIPY else "")
             + f", n={int(r.N)})"
             for _, r in neg.iterrows()]
        )
        bullets.append(f"**Gi·∫£m ng∆∞·ª£c chi·ªÅu v·ªõi {y_col}**: {s_txt}.")

    if method.lower() == "pearson":
        unstable = corr_tbl[(corr_tbl["CI_low"] < 0) & (corr_tbl["CI_high"] > 0)].head(5)
        if len(unstable):
            bullets.append("**Kh√¥ng ch·∫Øc ch·∫Øn (CI c·∫Øt 0)**: " + ", ".join([f"{r.X}" for _, r in unstable.iterrows()]))

    if _HAS_SCIPY:
        weak_sig = corr_tbl[(corr_tbl["abs_r"] < 0.3) & (corr_tbl["p_value"] < 0.05)].head(5)
        if len(weak_sig):
            bullets.append("**p<.05 nh∆∞ng hi·ªáu ·ª©ng nh·ªè**: " + ", ".join([f"{r.X}" for _, r in weak_sig.iterrows()]))

    bullets.append("**G·ª£i √Ω**: ∆∞u ti√™n bi·∫øn **|r|‚â•0.5** (m·∫°nh). N·∫øu **Spearman ‚â´ Pearson** ‚Üí quan h·ªá c√≥ th·ªÉ **phi tuy·∫øn**; n√™n xem scatter & c√¢n nh·∫Øc bi·∫øn ƒë·ªïi.")
    st.markdown("\n".join([f"- {b}" for b in bullets]) if bullets else "Ch∆∞a ƒë·ªß th√¥ng tin ƒë·ªÉ nh·∫≠n ƒë·ªãnh.")

# ------------------------------- TAB 3: Benford -------------------------------
with TAB4:
    for k in ['bf1_res','bf2_res','bf1_col','bf2_col']:
        if k not in SS: SS[k]=None
    st.subheader('üî¢ Benford Law ‚Äî 1D & 2D')
    base_df = DF_FULL
    if not NUM_COLS:
        st.info('Kh√¥ng c√≥ c·ªôt numeric ƒë·ªÉ ch·∫°y Benford.')
    else:
        run_on_full = True
        data_for_benford = DF_FULL
        # info removed
        c1,c2 = st.columns(2)
        with c1:
            amt1 = st.selectbox('Amount (1D)', NUM_COLS, key='bf1_col')
            if st.button('Run Benford 1D', key='btn_bf1'):
                ok,msg = _benford_ready(data_for_benford[amt1])
                if not ok: st.warning(msg)
                else: SS['bf1_res']=_benford_1d(data_for_benford[amt1])
        with c2:
            default_idx = 1 if len(NUM_COLS)>1 else 0
            amt2 = st.selectbox('Amount (2D)', NUM_COLS, index=default_idx, key='bf2_col')
            if st.button('Run Benford 2D', key='btn_bf2'):
                ok,msg = _benford_ready(data_for_benford[amt2])
                if not ok: st.warning(msg)
                else: SS['bf2_res']=_benford_2d(data_for_benford[amt2])
        g1,g2 = st.columns(2)
        with g1:
            if SS.get('bf1_res'):
                r=SS['bf1_res']; tb, var, p, MAD = r['table'], r['variance'], r['p'], r['MAD']
                if HAS_PLOTLY:
                    fig1 = go.Figure(); fig1.add_trace(go.Bar(x=tb['digit'], y=tb['observed_p'], name='Observed'))
                    fig1.add_trace(go.Scatter(x=tb['digit'], y=tb['expected_p'], name='Expected', mode='lines', line=dict(color='#F6AE2D')))
                    src_tag = 'FULL' if (SS['df'] is not None and SS.get('bf_use_full')) else 'SAMPLE'
                    fig1.update_layout(title=f'Benford 1D ‚Äî Obs vs Exp ({SS.get("bf1_col")}, {src_tag})', height=340)
                    st_plotly(fig1)
                # --- Data quality ‚Äî c·ªôt 1D ƒë√£ ch·ªçn ---
                    _raw1 = data_for_benford[SS.get('bf1_col')]
                    _num1 = pd.to_numeric(_raw1, errors='coerce')
                    _total1 = len(_raw1)
                    _none_like1 = _raw1.astype('string').str.strip().str.lower().isin(['none','null']).sum()
                    _n_nan1  = _num1.isna().sum()
                    _n_zero1 = (_num1 == 0).sum()
                    _n_pos1  = (_num1 > 0).sum()
                    _n_neg1  = (_num1 < 0).sum()
                    _used1   = (_num1 != 0).sum()            
                    _base_clean1 = max(_total1 - _n_nan1 - _n_zero1, 0)
                    
                    qdf1 = pd.DataFrame({
                        'type': ['Total rows','NaN (numeric)','None/Null (text)','Zero (==0)',
                                 'Positive (>0)','Negative (<0)','Used for Benford (‚â†0)'],
                        'count': [int(_total1), int(_n_nan1), int(_none_like1), int(_n_zero1),
                                  int(_n_pos1), int(_n_neg1), int(_used1)]
                    })
                    qdf1['% vs total'] = (qdf1['count'] / _total1 * 100.0).round(2) if _total1>0 else 0.0
                    qdf1['% vs non-missing&non-zero'] = (
                        (qdf1['count'] / _base_clean1 * 100.0).round(2) if _base_clean1>0 else 0.0
                    )
                    st.caption('üìã Data quality ‚Äî c·ªôt 1D ƒë√£ ch·ªçn')
                    st_df(qdf1, use_container_width=True, height=180)
                    # --- B·∫£ng % 1D (expected% / observed%) & diff% = observed% - expected% ---
                    color_thr_pct = 5.0  # drill-down theo chu·∫©n 5%
                    
                    t1 = pd.DataFrame({
                        'digit': tb['digit'].astype(int),
                        'expected_%': tb['expected_p'] * 100.0,
                        'observed_%': tb['observed_p'] * 100.0,
                    })
                    t1['diff_%'] = t1['observed_%'] - t1['expected_%']
                    
                    def _hl_percent1(v):
                        try:
                            return 'color: #d32f2f' if abs(float(v)) >= color_thr_pct else ''
                        except Exception:
                            return ''
                    
                    sty1 = (
                        t1.round(2)
                          .style
                          .format({'expected_%': '{:.2f}%', 'observed_%': '{:.2f}%', 'diff_%': '{:.2f}%'})
                          .applymap(_hl_percent1, subset=['diff_%'])
                    )
                    st_df(sty1, use_container_width=True, height=220)
                    
                    # --- Drill-down 1D cho nh·ªØng digit l·ªách ‚â•5% (t√≠nh theo diff_% ·ªü tr√™n) ---
                    bad_digits_1d = t1.loc[t1['diff_%'].abs() >= color_thr_pct, 'digit'].astype(int).tolist()
                    if bad_digits_1d:
                        with st.expander('üîé Drill-down 1D: c√°c ch·ªØ s·ªë l·ªách (|diff%| ‚â• 5%)', expanded=False):
                            mode1 = st.radio('Ch·∫ø ƒë·ªô hi·ªÉn th·ªã', ['Ng·∫Øn g·ªçn','X·ªï h·∫øt'], index=0,
                                             horizontal=True, key='bf1_drill_mode')
                    
                            import re as _re_local
                            def _digits_str(x):
                                xs = ("%.15g" % float(x))
                                return _re_local.sub(r"[^0-9]", "", xs).lstrip("0")
                            def _first1(v):
                                ds = _digits_str(v)
                                return int(ds[0]) if len(ds) >= 1 else np.nan
                    
                            s1_num = pd.to_numeric(data_for_benford[SS['bf1_col']], errors='coerce') \
                                       .replace([np.inf, -np.inf], np.nan).dropna().abs()
                            d1 = s1_num.apply(_first1).dropna()
                    
                            for dg in bad_digits_1d:
                                idx = d1[d1 == dg].index
                                st.markdown(f'**Digit {dg}** ‚Äî {len(idx):,} rows')
                                if len(idx) == 0:
                                    continue
                                if mode1 == 'X·ªï h·∫øt':
                                    st_df(data_for_benford.loc[idx].head(2000), use_container_width=True, height=260)
                                else:
                                    st_df(data_for_benford.loc[idx, [SS.get("bf1_col")]].head(200),
                                          use_container_width=True, height=220)
                    
                    # --- Th√¥ng ƒëi·ªáp tr·∫°ng th√°i d√πng ng∆∞·ª°ng slider (so s√°nh theo t·ª∑ l·ªá, kh√¥ng ph·∫£i % point) ---
                    thr = SS.get('risk_diff_threshold', 0.05)               # v√≠ d·ª• 0.05 = 5%
                    maxdiff_pp = float(t1['diff_%'].abs().max())            # % point
                    maxdiff_ratio = maxdiff_pp / 100.0                      # ƒë·ªïi v·ªÅ t·ª∑ l·ªá ƒë·ªÉ so v·ªõi thr
                    
                    msg = 'üü¢ Green'
                    if maxdiff_ratio >= 2*thr:
                        msg = 'üö® Red'
                    elif maxdiff_ratio >= thr:
                        msg = 'üü° Yellow'
                    
                    sev = 'üü¢ Green'
                    if (p < 0.01) or (MAD > 0.015): sev = 'üö® Red'
                    elif (p < 0.05) or (MAD > 0.012): sev = 'üü° Yellow'
                    
                    st.info(f"Diff% status: {msg} ‚Ä¢ p={p:.4f}, MAD={MAD:.4f} ‚áí Benford severity: {sev}")

                    
        with g2:
            if SS.get('bf2_res'):
                r2=SS['bf2_res']; tb2, var2, p2, MAD2 = r2['table'], r2['variance'], r2['p'], r2['MAD']
                if HAS_PLOTLY:
                    fig2 = go.Figure(); fig2.add_trace(go.Bar(x=tb2['digit'], y=tb2['observed_p'], name='Observed'))
                    fig2.add_trace(go.Scatter(x=tb2['digit'], y=tb2['expected_p'], name='Expected', mode='lines', line=dict(color='#F6AE2D')))
                    src_tag = 'FULL' if (SS['df'] is not None and SS.get('bf_use_full')) else 'SAMPLE'
                    fig2.update_layout(title=f'Benford 2D ‚Äî Obs vs Exp ({SS.get("bf2_col")}, {src_tag})', height=340)
                    st_plotly(fig2)
                # --- Data quality ‚Äî c·ªôt 2D ƒë√£ ch·ªçn ---
                    _raw2 = data_for_benford[SS.get('bf2_col')]
                    _num2 = pd.to_numeric(_raw2, errors='coerce')
                    _total2 = len(_raw2)
                    _none_like2 = _raw2.astype('string').str.strip().str.lower().isin(['none','null']).sum()
                    _n_nan2  = _num2.isna().sum()
                    _n_zero2 = (_num2 == 0).sum()
                    _n_pos2  = (_num2 > 0).sum()
                    _n_neg2  = (_num2 < 0).sum()
                    _used2   = (_num2 != 0).sum()            # Used for Benford: > 0 (gi·ªØ ƒë√∫ng logic tab n√†y)
                    _base_clean2 = max(_total2 - _n_nan2 - _n_zero2, 0)
                    
                    qdf2 = pd.DataFrame({
                        'type': ['Total rows','NaN (numeric)','None/Null (text)','Zero (==0)',
                                 'Positive (>0)','Negative (<0)','Used for Benford (‚â†0)'],
                        'count': [int(_total2), int(_n_nan2), int(_none_like2), int(_n_zero2),
                                  int(_n_pos2), int(_n_neg2), int(_used2)]
                    })
                    qdf2['% vs total'] = (qdf2['count'] / _total2 * 100.0).round(2) if _total2>0 else 0.0
                    qdf2['% vs non-missing&non-zero'] = (
                        (qdf2['count'] / _base_clean2 * 100.0).round(2) if _base_clean2>0 else 0.0
                    )
                    st.caption('üìã Data quality ‚Äî c·ªôt 2D ƒë√£ ch·ªçn')
                    st_df(qdf2, use_container_width=True, height=180)
                    
                   # --- B·∫£ng % 2D (expected% / observed%) & diff% = observed% - expected% ---
                    color_thr_pct = 5.0  # drill-down theo chu·∫©n 5%
                    
                    t2 = pd.DataFrame({
                        'digit': tb2['digit'].astype(int),
                        'expected_%': tb2['expected_p'] * 100.0,
                        'observed_%': tb2['observed_p'] * 100.0,
                    })
                    t2['diff_%'] = t2['observed_%'] - t2['expected_%']
                    
                    def _hl_percent2(v):
                        try:
                            return 'color: #d32f2f' if abs(float(v)) >= color_thr_pct else ''
                        except Exception:
                            return ''
                    
                    sty2 = (
                        t2.round(2)
                          .style
                          .format({'expected_%': '{:.2f}%', 'observed_%': '{:.2f}%', 'diff_%': '{:.2f}%'})
                          .applymap(_hl_percent2, subset=['diff_%'])
                    )
                    st_df(sty2, use_container_width=True, height=220)
                    
                    # --- Drill-down 2D cho nh·ªØng digit l·ªách ‚â•5% (t√≠nh theo diff_% ·ªü tr√™n) ---
                    bad_digits_2d = t2.loc[t2['diff_%'].abs() >= color_thr_pct, 'digit'].astype(int).tolist()
                    if bad_digits_2d:
                        with st.expander('üîé Drill-down 2D: c√°c ch·ªØ s·ªë l·ªách (|diff%| ‚â• 5%)', expanded=False):
                            mode2 = st.radio('Ch·∫ø ƒë·ªô hi·ªÉn th·ªã', ['Ng·∫Øn g·ªçn','X·ªï h·∫øt'], index=0,
                                             horizontal=True, key='bf2_drill_mode')
                    
                            import re as _re_local
                            def _digits_str(x):
                                xs = ("%.15g" % float(x))
                                return _re_local.sub(r"[^0-9]", "", xs).lstrip("0")
                            def _first2(v):
                                ds = _digits_str(v)
                                return int(ds[:2]) if len(ds) >= 2 else (int(ds) if len(ds) == 1 and ds != '0' else np.nan)
                    
                            s2_num = pd.to_numeric(data_for_benford[SS['bf2_col']], errors='coerce') \
                                       .replace([np.inf, -np.inf], np.nan).dropna().abs()
                            d2 = s2_num.apply(_first2).dropna()
                    
                            for dg in bad_digits_2d:
                                idx = d2[d2 == dg].index
                                st.markdown(f'**Digit {dg}** ‚Äî {len(idx):,} rows')
                                if len(idx) == 0:
                                    continue
                                if mode2 == 'X·ªï h·∫øt':
                                    st_df(data_for_benford.loc[idx].head(2000), use_container_width=True, height=260)
                                else:
                                    st_df(data_for_benford.loc[idx, [SS.get("bf2_col")]].head(200),
                                          use_container_width=True, height=220)
                    
                    # --- Th√¥ng ƒëi·ªáp tr·∫°ng th√°i d√πng ng∆∞·ª°ng slider (so s√°nh theo t·ª∑ l·ªá, kh√¥ng ph·∫£i % point) ---
                    thr = SS.get('risk_diff_threshold', 0.05)
                    maxdiff2_pp = float(t2['diff_%'].abs().max())
                    maxdiff2_ratio = maxdiff2_pp / 100.0
                    
                    msg2 = 'üü¢ Green'
                    if maxdiff2_ratio >= 2*thr:
                        msg2 = 'üö® Red'
                    elif maxdiff2_ratio >= thr:
                        msg2 = 'üü° Yellow'
                    
                    sev2 = 'üü¢ Green'
                    if (p2 < 0.01) or (MAD2 > 0.015): sev2 = 'üö® Red'
                    elif (p2 < 0.05) or (MAD2 > 0.012): sev2 = 'üü° Yellow'
                    
                    st.info(f"Diff% status: {msg2} ‚Ä¢ p={p2:.4f}, MAD={MAD2:.4f} ‚áí Benford severity: {sev2}")

# ------------------------------ TAB ? : Statistics Test (ANOVA & Nonparametric, balanced UI) ------------------------------
# ------------------------------ TAB 5 : Statistics Test (ANOVA & Nonparametric, balanced UI) ------------------------------
with TAB5:
    import numpy as np, pandas as pd, re
    import plotly.express as px
    import plotly.graph_objects as go
    from scipy import stats
    import streamlit as st

    st.subheader("üìä Hypothesis ‚Äî ANOVA & Nonparametric")

    # ===== Data guard =====
    DF = SS.get('df')
    if DF is None or len(DF) == 0:
        st.info("H√£y n·∫°p d·ªØ li·ªáu tr∆∞·ªõc.")
        st.stop()

    # ===== Type helpers =====
    def is_num(c):
        try: return pd.api.types.is_numeric_dtype(DF[c])
        except: return False
    def is_dt(c):
        if c not in DF.columns: return False
        if pd.api.types.is_datetime64_any_dtype(DF[c]): return True
        return bool(re.search(r'(date|time|ng√†y|th·ªùi gian)', str(c), flags=re.I))
    def is_cat(c):
        return (not is_num(c)) and (not is_dt(c))

    NUM_COLS = [c for c in DF.columns if is_num(c)]
    CAT_COLS = [c for c in DF.columns if is_cat(c)]

    # ===== Small utils =====
    def topn_cat(s: pd.Series, n=10):
        vc = s.astype(str).fillna("NaN").value_counts()
        keep = vc.index[:n].tolist()
        return s.astype(str).where(s.astype(str).isin(keep), "Kh√°c")

    def group_summary(y, g):
        d = pd.DataFrame({"y": y, "g": g}).dropna()
        if d.empty: 
            return pd.DataFrame(columns=["group","n","mean","std","median","se","ci95"])
        agg = d.groupby("g")["y"].agg(n="count", mean="mean", std="std", median="median")
        agg["se"] = agg["std"] / np.sqrt(agg["n"].clip(lower=1))
        agg["ci95"] = 1.96 * agg["se"]
        out = agg.reset_index().rename(columns={"g":"group"})
        return out.replace([np.inf, -np.inf], np.nan).fillna(0.0)

    def holm_bonferroni(pvals, labels):
        p = np.asarray(pvals, dtype=float); m = len(p); order = np.argsort(p)
        adj = np.empty(m, dtype=float); running_max = 0.0
        for r, idx in enumerate(order):
            adj_val = (m - r) * p[idx]
            running_max = max(running_max, adj_val)
            adj[idx] = min(1.0, running_max)
        return pd.DataFrame({"pair": labels, "p_raw": p, "p_adj_holm": adj}).sort_values("p_adj_holm")

    def one_way_anova_fast(y, g):
        d = pd.DataFrame({"y": pd.to_numeric(y, errors="coerce"), "g": g}).dropna()
        if d["g"].nunique() < 2 or len(d) < 3:
            return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan
        try:
            levene_p = stats.levene(*[grp["y"].values for _, grp in d.groupby("g")], center="median").pvalue
        except Exception:
            levene_p = np.nan
        grp = d.groupby("g")["y"].agg(n="count", mean="mean"); ssq = d.assign(y2=d["y"]**2).groupby("g")["y2"].sum()
        grand_mean = float(d["y"].mean()); ssb = float((grp["n"] * (grp["mean"] - grand_mean) ** 2).sum())
        ssw = float((ssq - grp["n"] * (grp["mean"] ** 2)).sum()); sst = float(((d["y"] - grand_mean) ** 2).sum())
        k = int(grp.shape[0]); n = int(d.shape[0]); df1 = k - 1; df2 = max(n - k, 1)
        msb = ssb / max(df1, 1); msw = ssw / max(df2, 1)
        F = (msb / msw) if msw > 0 else np.inf
        p = 1 - stats.f.cdf(F, df1, df2) if np.isfinite(F) else 0.0
        eta2 = (ssb / sst) if sst > 0 else np.nan
        omega2 = ((ssb - df1 * msw) / (sst + msw)) if (sst + msw) > 0 else np.nan
        return float(F), float(p), float(df1), float(df2), float(eta2), float(omega2), float(levene_p)

    def kruskal_eps2(H, k, n):
        return float((H - (k - 1)) / (n - k)) if (n - k) > 0 else np.nan

    def _dtype_name(col):
        if col is None: return "‚Äî"
        try:
            if pd.api.types.is_datetime64_any_dtype(DF[col]): return "datetime"
            if pd.api.types.is_numeric_dtype(DF[col]): return "numeric"
            return "categorical"
        except Exception:
            return "unknown"

    def _type_hint(label, col, expect):
        actual = _dtype_name(col)
        ok = (actual == expect)
        icon = "‚úÖ" if ok else "‚ö†Ô∏è"
        st.caption(f"{icon} {label}: `{col}` ¬∑ {actual} (y√™u c·∫ßu: {expect})")

    # ===== (THAY ƒê·ªîI) ƒê·∫∑t H∆∞·ªõng d·∫´n Popover g·ªçn g√†ng B√äN TR√äN Tabs =====
    st.markdown(
        """
        <style>
            /* Thu nh·ªè popover trigger button */
            button[data-testid="stPopoverTrigger"] {
                padding: 0px 6px;
                font-size: 0.9rem;
                margin-left: 8px;
            }
        </style>
        """, unsafe_allow_html=True
    )

    col_t1, col_t2 = st.columns(2)
    with col_t1:
        with st.popover("‚ìò ANOVA (Parametric)?"):
            st.markdown("**Khi n√†o d√πng:** D·ªØ li·ªáu Y (numeric) c·ªßa b·∫°n *g·∫ßn* tu√¢n theo ph√¢n ph·ªëi chu·∫©n.")
            st.markdown("**Ki·ªÉm tra (·ªü TAB 2):**")
            st.markdown("* `Skewness` (ƒë·ªô l·ªách): L√Ω t∆∞·ªüng l√† `g·∫ßn 0` (v√≠ d·ª•: trong kho·∫£ng -1 ƒë·∫øn 1).")
            st.markdown("* `Kurtosis` (ƒë·ªô nh·ªçn): L√Ω t∆∞·ªüng l√† `g·∫ßn 0` (v√≠ d·ª•: trong kho·∫£ng -2 ƒë·∫øn 2).")
            st.markdown("* `Normality p-value`: > 0.05.")
            st.markdown("---")
            st.markdown("‚ö†Ô∏è *N·∫øu vi ph·∫°m n·∫∑ng (p < 0.05, skew/kurtosis l·ªõn), h√£y ∆∞u ti√™n **Nonparametric**.*")
    with col_t2:
        with st.popover("‚ìò Nonparametric?"):
             st.markdown("**Khi n√†o d√πng:**")
             st.markdown("* D·ªØ li·ªáu Y *l·ªách nhi·ªÅu* ho·∫∑c *kh√¥ng chu·∫©n* (vi ph·∫°m gi·∫£ ƒë·ªãnh Parametric).")
             st.markdown("* D·ªØ li·ªáu c√≥ *nhi·ªÅu outliers* (b·ªã k√©o ƒëu√¥i).")
             st.markdown("* B·∫°n mu·ªën so s√°nh *trung v·ªã (median)* thay v√¨ *trung b√¨nh (mean)*.")

    tab_a, tab_np = st.tabs(["ANOVA (Parametric)", "Nonparametric"])

# ====================== ANOVA (Parametric) ‚Äî (UI Tinh gi·∫£n) ======================
    with tab_a:
        mode_a = st.radio("Testing", ["Independent (between)", "Repeated (within)"], horizontal=True, key="anova_mode")
    
        # ---------- Independent (between) ----------
        if mode_a == "Independent (between)":
            if len(NUM_COLS) == 0 or len(CAT_COLS) == 0:
                st.info("C·∫ßn t·ªëi thi·ªÉu 1 c·ªôt numeric (Y) v√† 1 c·ªôt categorical (factor).")
            else:
                box_top = st.container(border=True)
                with box_top:
                    st.markdown("### ANOVA ‚Äî Independent (between)")
                    with st.popover("‚ìò H∆∞·ªõng d·∫´n"):
                        st.markdown(
                            "**Independent (between):**\n\n"
                            "So s√°nh c√°c nh√≥m *kh√°c bi·ªát, ƒë·ªôc l·∫≠p* v·ªõi nhau.\n\n"
                            "*V√≠ d·ª•: So s√°nh doanh s·ªë gi·ªØa Region A, Region B, v√† Region C.*"
                        )
                    
                    y_col  = st.selectbox("üéØ Dependent (numeric)", NUM_COLS, key="av_y")
                    a_col  = st.selectbox("üè∑Ô∏è Factor A (categorical)", CAT_COLS, key="av_a")
                    
                    use_two = st.toggle("‚ûï Two-way ANOVA (Th√™m Factor B)", value=False, key="av_two")
                    b_col = None
                    if use_two:
                        # *** (THAY ƒê·ªîI) L·ªçc th√¥ng minh: B kh√¥ng th·ªÉ l√† A ***
                        b_choices = [c for c in CAT_COLS if c != a_col]
                        if not b_choices:
                            st.warning("Kh√¥ng ƒë·ªß c·ªôt categorical kh√°c ƒë·ªÉ l√†m Factor B.")
                            use_two = False
                        else:
                            b_col = st.selectbox("üè∑Ô∏è Factor B (categorical)", b_choices, key="av_b")
                    
                    _type_hint("Dependent", y_col, "numeric")
                    _type_hint("Factor A", a_col, "categorical")
                    if use_two and b_col:
                        _type_hint("Factor B", b_col, "categorical")
    
                # Controls (UI Tinh gi·∫£n - ·∫®n C√†i ƒë·∫∑t N√¢ng cao)
                box_ctl = st.container(border=True)
                with box_ctl:
                    st.checkbox("Hi·ªán 95% CI (Bi·ªÉu ƒë·ªì)", value=True, key="av_ci")
                    if not use_two:
                        st.checkbox("Pairwise Post-hoc (So s√°nh c·∫∑p)", value=True, key="av_posthoc")
                    
                    # *** (THAY ƒê·ªîI) C√†i ƒë·∫∑t n√¢ng cao ƒë∆∞·ª£c ·∫©n ƒëi (m·∫∑c ƒë·ªãnh) ***
                    topN_A = 10
                    topN_B = 8
                    max_fit = 300_000
                    
                    run = st.button("‚ñ∂Ô∏è Run", use_container_width=True, key="av_run")
    
                # Compute & report
                if run:
                    if not use_two:
                        # ----- One-way (T·ª∞ ƒê·ªòNG CH·ªåN T-TEST / ANOVA) -----
                        sub = DF[[y_col, a_col]].copy()
                        if len(sub) > max_fit:
                            sub = sub.sample(n=max_fit, random_state=42)
                        sub[a_col] = topn_cat(sub[a_col], n=topN_A)
    
                        y = pd.to_numeric(sub[y_col], errors="coerce")
                        g = sub[a_col].astype(str)
                        ok = (~y.isna()) & (~g.isna())
                        y, g = y[ok], g[ok]
    
                        summ = group_summary(y, g).sort_values("mean", ascending=False)
                        st.dataframe(summ, use_container_width=True, hide_index=True)
                        if summ.shape[0] < 2 or len(y) < 3:
                            st.warning("Kh√¥ng ƒë·ªß nh√≥m/h√†ng ƒë·ªÉ ch·∫°y.")
                            st.stop()
                        
                        k = summ.shape[0] # S·ªë l∆∞·ª£ng nh√≥m

                        if k == 2:
                            # ----- (THAY ƒê·ªîI) CH·∫†Y WELCH T-TEST CHO 2 NH√ìM -----
                            st.markdown("#### K·∫øt qu·∫£ Welch's T-test (cho 2 nh√≥m)")
                            groups = [y[g == lv].values for lv in summ["group"]]
                            tt = stats.ttest_ind(groups[0], groups[1], equal_var=False) # equal_var=False l√† Welch
                            
                            m1, m2, m3 = st.columns(3)
                            m1.metric("t-statistic", f"{tt.statistic:.4f}")
                            m2.metric("p-value", f"{tt.pvalue:.4g}")
                            m3.metric("Groups", f"{summ['group'].iloc[0]} vs {summ['group'].iloc[1]}")
                            
                            # *** (THAY ƒê·ªîI) D√πng Violin Plot cho 2 nh√≥m ***
                            st.markdown("#### Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi (Violin)")
                            fig = px.violin(
                                pd.DataFrame({"group": g, "y": y}), 
                                x="group", y="y", 
                                box=True, points=False, 
                                labels={"group": a_col, "y": y_col}
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            if tt.pvalue < 0.05:
                                st.success(f"**K·∫øt lu·∫≠n:** C√≥ s·ª± kh√°c bi·ªát c√≥ √Ω nghƒ©a th·ªëng k√™ gi·ªØa 2 nh√≥m (p < 0.05).")
                            else:
                                st.info(f"**K·∫øt lu·∫≠n:** Ch∆∞a ƒë·ªß b·∫±ng ch·ª©ng v·ªÅ s·ª± kh√°c bi·ªát gi·ªØa 2 nh√≥m (p >= 0.05).")

                        else:
                            # ----- CH·∫†Y ANOVA (nh∆∞ c≈©, cho 3+ nh√≥m) -----
                            st.markdown(f"#### K·∫øt qu·∫£ One-Way ANOVA (cho {k} nh√≥m)")
                            F, p, df1, df2, eta2, omega2, lev_p = one_way_anova_fast(y, g)
        
                            m1, m2, m3, m4 = st.columns(4)
                            m1.metric("F", f"{F:.3f}")
                            m2.metric("p-value", f"{p:.4g}")
                            m3.metric("Œ∑¬≤", f"{eta2:.3f}" if not np.isnan(eta2) else "‚Äî")
                            m4.metric("œâ¬≤", f"{omega2:.3f}" if not np.isnan(omega2) else "‚Äî")
                            st.caption(f"Levene (ph∆∞∆°ng sai b·∫±ng nhau) p = {lev_p:.4g}")

                            # *** C·∫¢NH B√ÅO LEVENE (ƒê√É TH√äM) ***
                            if lev_p < 0.05:
                                st.warning(
                                    "‚ö†Ô∏è Levene's p < 0.05: Gi·∫£ ƒë·ªãnh v·ªÅ ph∆∞∆°ng sai b·∫±ng nhau ƒë√£ b·ªã vi ph·∫°m. "
                                    "K·∫øt qu·∫£ F-test c·ªßa ANOVA c√≥ th·ªÉ kh√¥ng ƒë√°ng tin c·∫≠y. "
                                    "**G·ª£i √Ω:** H√£y ∆∞u ti√™n k·∫øt qu·∫£ Post-hoc 'Welch t-test' (v√¨ n√≥ robust) "
                                    "ho·∫∑c s·ª≠ d·ª•ng tab 'Nonparametric' (Kruskal-Wallis)."
                                )
        
                            # chart
                            if not SS.get("av_ci"): # B·ªè check 'fast'
                                fig = px.bar(summ, x="group", y="mean", labels={"group": a_col, "mean": f"Mean {y_col}"})
                            else:
                                fig = go.Figure(go.Bar(x=summ["group"], y=summ["mean"],
                                                       error_y=dict(type="data", array=summ["ci95"], visible=True)))
                                fig.update_layout(yaxis_title=f"{y_col} (mean ¬± 95% CI)")
                            st.plotly_chart(fig, use_container_width=True)
        
                            # post-hoc (Welch t-test + Holm)
                            if SS.get("av_posthoc") and summ.shape[0] >= 2:
                                groups_dict = {lv: y[g == lv].values for lv in summ["group"]}
                                pvals, labs = [], []
                                group_names = summ["group"].tolist()
                                for i in range(len(group_names)):
                                    gi = group_names[i]; xi = groups_dict[gi]
                                    for j in range(i+1, len(group_names)):
                                        gj = group_names[j]; xj = groups_dict[gj]
                                        if len(xi) >= 2 and len(xj) >= 2:
                                            tt = stats.ttest_ind(xi, xj, equal_var=False)
                                            pvals.append(float(tt.pvalue)); labs.append(f"{gi} vs {gj}")
                                if pvals:
                                    adj = holm_bonferroni(np.array(pvals), np.array(labs))
                                    diffs = []
                                    for pair in adj["pair"]:
                                        gi, gj = str(pair).split(" vs ")
                                        mi = summ.loc[summ["group"] == gi, "mean"].values[0]
                                        mj = summ.loc[summ["group"] == gj, "mean"].values[0]
                                        diffs.append(mi - mj)
                                    adj["mean_diff"] = diffs
                                    st.dataframe(adj.head(50), use_container_width=True, hide_index=True)
                                    st.caption("Pairwise Welch t-test (Holm-adjusted).")
        
                            # *** (GI·ªÆ NGUY√äN) K·∫øt lu·∫≠n M·∫°nh/Y·∫øu/V·ª´a ***
                            strength = ("y·∫øu" if (np.isnan(eta2) or eta2 < 0.06) else ("v·ª´a" if eta2 < 0.14 else "m·∫°nh"))
                            best = str(summ.iloc[0]["group"]) if len(summ) else "‚Äî"
                            st.success(f"**K·∫øt lu·∫≠n:** Kh√°c bi·ªát gi·ªØa c√°c nh√≥m **{strength}** (Œ∑¬≤={eta2:.2f}). Nh√≥m cao nh·∫•t: **{best}**.")
    
                    else:
                        # ----- Two-way ANOVA (OLS + anova_lm) -----
                        try:
                            import statsmodels.api as sm
                            import statsmodels.formula.api as smf
                        except Exception:
                            st.error("Two-way ANOVA c·∫ßn `statsmodels`. H√£y c√†i ƒë·∫∑t g√≥i n√†y.")
                            st.stop()
    
                        sub = DF[[y_col, a_col, b_col]].dropna().copy()
                        if len(sub) > max_fit:
                            sub = sub.sample(n=max_fit, random_state=42)
                        sub[a_col] = topn_cat(sub[a_col], n=topN_A)
                        sub[b_col] = topn_cat(sub[b_col], n=topN_B)
    
                        d = sub.rename(columns={y_col: "Y", a_col: "A", b_col: "B"})
                        d["Y"] = pd.to_numeric(d["Y"], errors="coerce")
                        d = d.dropna(subset=["Y"])
                        if d["A"].nunique() < 2 or d["B"].nunique() < 2:
                            st.warning("C·∫ßn ‚â•2 m·ª©c cho m·ªói factor sau khi Top-N.")
                            st.stop()
    
                        model = smf.ols("Y ~ C(A) + C(B) + C(A):C(B)", data=d).fit()
                        an_tbl = sm.stats.anova_lm(model, typ=2)
                        st.dataframe(an_tbl, use_container_width=True)
    
                        if "Residual" in an_tbl.index and "sum_sq" in an_tbl.columns:
                            ss_res = float(an_tbl.loc["Residual", "sum_sq"])
                            def peta(row): 
                                ss = float(row["sum_sq"])
                                return ss / (ss + ss_res) if (ss + ss_res) > 0 else np.nan
                            peta_vals = an_tbl.apply(peta, axis=1)
                            pe = peta_vals.to_dict()
                        else:
                            pe = {}
    
                        def card_val(name, col):
                            if name in an_tbl.index:
                                Fv = an_tbl.loc[name, "F"]; pv = an_tbl.loc[name, "PR(>F)"]
                                ev = pe.get(name, np.nan)
                                col.metric(name.replace("C(","").replace(")",""), f"F={Fv:.2f}", f"p={pv:.3g}")
                                if not np.isnan(ev): col.caption(f"partial Œ∑¬≤ ‚âà {ev:.3f}")
                            else:
                                col.metric(name, "‚Äî", "‚Äî")
    
                        c1, c2, c3 = st.columns(3)
                        card_val("C(A)", c1); card_val("C(B)", c2); card_val("C(A):C(B)", c3)
    
                        grp = d.groupby(["A","B"])["Y"].agg(n="count", mean="mean").reset_index()
                        fig = px.bar(grp, x="A", y="mean", color="B", barmode="group",
                                     labels={"A": a_col, "B": b_col, "mean": f"Mean {y_col}"})
                        st.plotly_chart(fig, use_container_width=True)
    
                        pA = float(an_tbl.loc["C(A)", "PR(>F)"]) if "C(A)" in an_tbl.index else np.nan
                        pB = float(an_tbl.loc["C(B)", "PR(>F)"]) if "C(B)" in an_tbl.index else np.nan
                        pI = float(an_tbl.loc["C(A):C(B)", "PR(>F)"]) if "C(A):C(B)" in an_tbl.index else np.nan
                        msg = []
                        if not np.isnan(pI) and pI < 0.05:
                            msg.append("**c√≥ t∆∞∆°ng t√°c A√óB** (p<0.05) ‚Äî n√™n ƒë·ªçc theo t·ª´ng l√°t c·∫Øt.")
                        if not np.isnan(pA) and pA < 0.05:
                            msg.append("Factor **A** c√≥ √Ω nghƒ©a.")
                        if not np.isnan(pB) and pB < 0.05:
                            msg.append("Factor **B** c√≥ √Ω nghƒ©a.")
                        if not msg: msg = ["Ch∆∞a th·∫•y hi·ªáu ·ª©ng c√≥ √Ω nghƒ©a (p‚â•0.05)."]
                        st.success(" ; ".join(msg))
    
        # ---------- Repeated (within) ----------
        else:
            cand_id = [c for c in DF.columns if is_cat(c)]
            cand_factor = [c for c in CAT_COLS]
            if len(NUM_COLS) == 0 or len(cand_id) == 0 or len(cand_factor) == 0:
                st.info("C·∫ßn: 1 numeric (Y), 1 ID (subject), 1 categorical (condition).")
            else:
                box_top_r = st.container(border=True)
                with box_top_r:
                    st.markdown("### ANOVA ‚Äî Repeated (within)")
                    with st.popover("‚ìò H∆∞·ªõng d·∫´n"):
                        st.markdown(
                            "**Repeated (within):**\n\n"
                            "So s√°nh *c√πng m·ªôt ƒë·ªëi t∆∞·ª£ng* qua nhi·ªÅu ƒëi·ªÅu ki·ªán ho·∫∑c th·ªùi gian.\n\n"
                            "*V√≠ d·ª•: So s√°nh doanh s·ªë c·ªßa t·ª´ng C·ª≠a h√†ng v√†o Th√°ng 1, Th√°ng 2, v√† Th√°ng 3.*"
                        )
                    y_col = st.selectbox("üéØ Y (numeric)", NUM_COLS, key="av_rep_y")
                    id_col = st.selectbox("üßë‚Äçü§ù‚Äçüßë ID (subject)", cand_id, key="av_rep_id")
                    
                    # *** (THAY ƒê·ªîI) L·ªçc th√¥ng minh: Condition kh√¥ng th·ªÉ l√† ID ***
                    cond_choices = [c for c in cand_factor if c != id_col]
                    if not cond_choices:
                        st.warning("Kh√¥ng ƒë·ªß c·ªôt categorical kh√°c ƒë·ªÉ l√†m Condition.")
                        st.stop()
                    else:
                        cond_col = st.selectbox("üè∑Ô∏è Condition (within)", cond_choices, key="av_rep_cond")

                    _type_hint("Y", y_col, "numeric")
                    _type_hint("ID", id_col, "categorical")
                    _type_hint("Condition", cond_col, "categorical")
    
                box_ctl_r = st.container(border=True)
                with box_ctl_r:
                    # *** (THAY ƒê·ªîI) ·∫®n c√†i ƒë·∫∑t n√¢ng cao ***
                    max_subj_fit = 5_000
                    plot_subj = 80
                    run = st.button("‚ñ∂Ô∏è Run", use_container_width=True, key="av_rep_run")
    
                if run:
                    try:
                        from statsmodels.stats.anova import AnovaRM
                    except Exception:
                        st.error("RM-ANOVA c·∫ßn `statsmodels`. B·∫°n c√≥ th·ªÉ d√πng tab **Nonparametric ‚Üí Friedman** nh∆∞ m·ªôt thay th·∫ø.")
                        st.stop()
    
                    d0 = DF[[y_col, id_col, cond_col]].dropna().copy()
                    cnt = d0.groupby([id_col, cond_col]).size().unstack(cond_col).dropna()
                    keep_ids = cnt.index
                    d = d0[d0[id_col].isin(keep_ids)]
                    uniq_ids = d[id_col].unique()
                    if len(uniq_ids) > max_subj_fit:
                        keep = pd.Index(uniq_ids).sample(max_subj_fit, random_state=42)
                        d = d[d[id_col].isin(keep)]
    
                    if d.empty or d[cond_col].nunique() < 2:
                        st.warning("Kh√¥ng ƒë·ªß subject/ƒëi·ªÅu ki·ªán ƒë·ªÉ ch·∫°y RM-ANOVA.")
                        st.stop()
    
                    model = AnovaRM(d, depvar=y_col, subject=id_col, within=[cond_col])
                    res = model.fit()
                    # *** (THAY ƒê·ªîI) Hi·ªÉn th·ªã DataFrame s·∫°ch s·∫Ω ***
                    st.dataframe(res.anova_summary, use_container_width=True)
    
                    # Means + spaghetti
                    pivot = d.pivot_table(index=id_col, columns=cond_col, values=y_col, aggfunc="mean")
                    levels = list(pivot.columns)
                    means = pivot.mean().reset_index()
                    means.columns = ["cond","mean"]
                    fig = px.line(means, x="cond", y="mean", markers=True)
                    
                    # *** (THAY ƒê·ªîI) B·ªè bi·ªÉu ƒë·ªì l·∫∑p, ch·ªâ v·∫Ω 1 l·∫ßn ***
                    if plot_subj > 0 and pivot.shape[0] > 0:
                        samp = pivot.sample(min(plot_subj, pivot.shape[0]), random_state=42)
                        for _, row in samp.iterrows():
                            fig.add_trace(go.Scatter(x=levels, y=row.values, mode="lines", opacity=0.25, showlegend=False))
                    st.plotly_chart(fig, use_container_width=True)
    
                    st.success("**K·∫øt lu·∫≠n:** xem p-value c·ªßa within-factor trong b·∫£ng; p<0.05 ‚áí c√≥ kh√°c bi·ªát gi·ªØa c√°c ƒëi·ªÅu ki·ªán.")

    # ====================== NONPARAMETRIC (UI Tinh gi·∫£n) ======================
    with tab_np:
        mode = st.radio("Testing", ["Independent (between)", "Repeated (within)"], horizontal=True, key="np_mode")

        # ---------- Independent (between) ----------
        if mode == "Independent (between)":
            if len(NUM_COLS) == 0 or len(CAT_COLS) == 0:
                st.info("C·∫ßn 1 numeric (Y) v√† 1 categorical (group).")
            else:
                box_top_np = st.container(border=True)
                with box_top_np:
                    st.markdown("### Nonparametric ‚Äî Independent")
                    with st.popover("‚ìò H∆∞·ªõng d·∫´n"):
                        st.markdown(
                            "**Independent (between):**\n\n"
                            "So s√°nh c√°c nh√≥m *kh√°c bi·ªát, ƒë·ªôc l·∫≠p* v·ªõi nhau (d√πng Median).\n\n"
                            "*V√≠ d·ª•: So s√°nh doanh s·ªë gi·ªØa Region A, B, C khi d·ªØ li·ªáu b·ªã l·ªách (skewed).* \n\n"
                            "‚Ä¢ 2 nh√≥m: **Mann‚ÄìWhitney U**.\n\n"
                            "‚Ä¢ 3+ nh√≥m: **Kruskal‚ÄìWallis**."
                        )
                    y_col = st.selectbox("üéØ Y (numeric)", NUM_COLS, key="np_y")
                    g_col = st.selectbox("üè∑Ô∏è Group (categorical)", CAT_COLS, key="np_g")
                    _type_hint("Y", y_col, "numeric")
                    _type_hint("Group", g_col, "categorical")

                box_ctl_np = st.container(border=True)
                with box_ctl_np:
                    # *** (THAY ƒê·ªîI) ·∫®n c√†i ƒë·∫∑t n√¢ng cao ***
                    topN = 10
                    max_fit = 300_000
                    run = st.button("‚ñ∂Ô∏è Run", use_container_width=True, key="np_run")

                if run:
                    sub = DF[[y_col, g_col]].copy()
                    if len(sub) > max_fit:
                        sub = sub.sample(max_fit, random_state=42)
                    sub[g_col] = topn_cat(sub[g_col], n=topN)
                    y = pd.to_numeric(sub[y_col], errors="coerce")
                    g = sub[g_col].astype(str)
                    ok = (~y.isna()) & (~g.isna())
                    y, g = y[ok], g[ok]

                    summ = group_summary(y, g).sort_values("median", ascending=False)
                    st.dataframe(summ, use_container_width=True, hide_index=True)

                    groups = [y[g == lv].values for lv in summ["group"]]
                    k = len(groups)
                    n = int(sum(len(arr) for arr in groups))

                    if k == 2:
                        # Mann‚ÄìWhitney U
                        ures = stats.mannwhitneyu(groups[0], groups[1], alternative="two-sided")
                        p = float(ures.pvalue); U = float(ures.statistic)
                        z = float(stats.norm.isf(p / 2.0)) if p > 0 else np.inf
                        r_eff = z / np.sqrt(n) if n > 0 and np.isfinite(z) else np.nan
                        st.markdown(f"**Mann‚ÄìWhitney U**: U = {U:.3f}, p = {p:.4g}, r ‚âà {r_eff:.3f} (effect size)")

                        fig = px.violin(pd.DataFrame({g_col: g, y_col: y}), x=g_col, y=y_col,
                                        box=True, points=False)
                        st.plotly_chart(fig, use_container_width=True)

                        hi = str(summ.iloc[0]['group']) if len(summ) else "‚Äî"
                        level = ("m·∫°nh" if (not np.isnan(r_eff) and r_eff >= 0.5)
                                 else "v·ª´a" if (not np.isnan(r_eff) and r_eff >= 0.3) else "y·∫øu")
                        st.success(f"**K·∫øt lu·∫≠n:** Kh√°c bi·ªát **{level}** (r‚âà{r_eff:.2f}). Nh√≥m median cao nh·∫•t: **{hi}**.")
                    
                    elif k > 2:
                        # Kruskal‚ÄìWallis
                        H, p = stats.kruskal(*groups)
                        eps2 = kruskal_eps2(H, k, n)
                        st.markdown(f"**Kruskal‚ÄìWallis**: H = {H:.3f}, p = {p:.4g}, Œµ¬≤ = {eps2:.3f} (effect size)")

                        # (Bi·ªÉu ƒë·ªì bar cho 3+ nh√≥m l√† ph√π h·ª£p)
                        fig = go.Figure(go.Bar(x=summ["group"], y=summ["median"],
                                               error_y=dict(array=summ["ci95"], visible=True)))
                        fig.update_layout(yaxis_title=f"{y_col} (median ¬± 95% CI‚âà)")
                        st.plotly_chart(fig, use_container_width=True)

                        # Post-hoc: pairwise Mann‚ÄìWhitney + Holm
                        pvals, labs = [], []
                        for i in range(k):
                            for j in range(i+1, k):
                                u = stats.mannwhitneyu(groups[i], groups[j], alternative="two-sided")
                                pvals.append(float(u.pvalue))
                                labs.append(f"{summ['group'].iloc[i]} vs {summ['group'].iloc[j]}")
                        if pvals:
                            adj = holm_bonferroni(np.array(pvals), np.array(labs))
                            st.dataframe(adj.head(50), use_container_width=True, hide_index=True)
                            st.caption("Pairwise Mann‚ÄìWhitney (Holm-adjusted).")

                        strength = ("y·∫øu" if (np.isnan(eps2) or eps2 < 0.06)
                                    else ("v·ª´a" if eps2 < 0.14 else "m·∫°nh"))
                        hi = str(summ.iloc[0]["group"]) if len(summ) else "‚Äî"
                        st.success(f"**K·∫øt lu·∫≠n:** Kh√°c bi·ªát **{strength}** (Œµ¬≤={eps2:.2f}). Nh√≥m median cao nh·∫•t: **{hi}**.")
                    else:
                        st.warning("C·∫ßn √≠t nh·∫•t 2 nh√≥m ƒë·ªÉ so s√°nh.")

        # ---------- Repeated (within) ----------
        else:
            cand_id = [c for c in DF.columns if is_cat(c)]
            cand_factor = [c for c in CAT_COLS]
            if len(NUM_COLS) == 0 or len(cand_id) == 0 or len(cand_factor) == 0:
                st.info("C·∫ßn: 1 numeric (Y), 1 ID (subject), 1 categorical (condition).")
            else:
                box_top_r = st.container(border=True)
                with box_top_r:
                    st.markdown("### Nonparametric ‚Äî Repeated (within)")
                    with st.popover("‚ìò H∆∞·ªõng d·∫´n"):
                        st.markdown(
                            "**Repeated (within):**\n\n"
                            "So s√°nh *c√πng m·ªôt ƒë·ªëi t∆∞·ª£ng* qua nhi·ªÅu ƒëi·ªÅu ki·ªán (d√πng Median).\n\n"
                            "*V√≠ d·ª•: So s√°nh doanh s·ªë (b·ªã l·ªách) c·ªßa t·ª´ng C·ª≠a h√†ng v√†o Th√°ng 1, 2, 3.*\n\n"
                            "‚Ä¢ 2 ƒëi·ªÅu ki·ªán: **Wilcoxon**.\n\n"
                            "‚Ä¢ 3+ ƒëi·ªÅu ki·ªán: **Friedman**."
                        )
                    y_col = st.selectbox("üéØ Y (numeric)", NUM_COLS, key="rep_y")
                    id_col = st.selectbox("üßë‚Äçü§ù‚Äçüßë ID (subject)", cand_id, key="rep_id")
                    
                    # *** (THAY ƒê·ªîI) L·ªçc th√¥ng minh: Condition kh√¥ng th·ªÉ l√† ID ***
                    cond_choices = [c for c in cand_factor if c != id_col]
                    if not cond_choices:
                        st.warning("Kh√¥ng ƒë·ªß c·ªôt categorical kh√°c ƒë·ªÉ l√†m Condition.")
                        st.stop()
                    else:
                        cond_col = st.selectbox("üè∑Ô∏è Condition (within)", cond_choices, key="rep_cond")

                    _type_hint("Y", y_col, "numeric")
                    _type_hint("ID", id_col, "categorical")
                    _type_hint("Condition", cond_col, "categorical")

                box_ctl_r = st.container(border=True)
                with box_ctl_r:
                    # *** (THAY ƒê·ªîI) ·∫®n c√†i ƒë·∫∑t n√¢ng cao ***
                    max_subj_fit = 5_000
                    plot_subj = 80
                    run = st.button("‚ñ∂Ô∏è Run", use_container_width=True, key="rep_run")

                if run:
                    d0 = DF[[y_col, id_col, cond_col]].dropna().copy()
                    count = d0.groupby([id_col, cond_col]).size().unstack(cond_col).dropna()
                    subj_keep = count.index
                    d = d0[d0[id_col].isin(subj_keep)]

                    uniq_ids = d[id_col].unique()
                    if len(uniq_ids) > max_subj_fit:
                        keep = pd.Index(uniq_ids).sample(max_subj_fit, random_state=42)
                        d = d[d[id_col].isin(keep)]

                    pivot = d.pivot_table(index=id_col, columns=cond_col, values=y_col, aggfunc="mean")
                    pivot = pivot.dropna(axis=0)
                    levels = list(pivot.columns)
                    m = len(levels); n = pivot.shape[0]

                    if m == 2:
                        a = pivot[levels[0]].values
                        b = pivot[levels[1]].values
                        res = stats.wilcoxon(a, b, zero_method="wilcox", correction=False, alternative="two-sided", mode="auto")
                        p = float(res.pvalue); Wstat = float(res.statistic)
                        z = float(stats.norm.isf(p/2.0)) if p > 0 else np.inf
                        r_eff = z / np.sqrt(n) if n > 0 and np.isfinite(z) else np.nan
                        st.markdown(f"**Wilcoxon signed-rank**: W = {Wstat:.3f}, p = {p:.4g}, r ‚âà {r_eff:.3f} (effect size)")

                        means = pivot.mean().reset_index()
                        means.columns = ["cond","mean"]
                        fig = px.line(means, x="cond", y="mean", markers=True)
                        
                        if plot_subj > 0:
                            samp = pivot.sample(min(plot_subj, pivot.shape[0]), random_state=42)
                            for _, row in samp.iterrows():
                                fig.add_trace(go.Scatter(x=levels, y=row.values, mode="lines", opacity=0.25, showlegend=False))
                        st.plotly_chart(fig, use_container_width=True)

                        level = ("m·∫°nh" if (not np.isnan(r_eff) and r_eff >= 0.5)
                                 else "v·ª´a" if (not np.isnan(r_eff) and r_eff >= 0.3) else "y·∫øu")
                        st.success(f"**K·∫øt lu·∫≠n:** Kh√°c bi·ªát **{level}** (r‚âà{r_eff:.2f}).")

                    elif m > 2:
                        fr = stats.friedmanchisquare(*[pivot[c].values for c in levels])
                        chi2 = float(fr.statistic); p = float(fr.pvalue)
                        W = chi2 / (n * m * (m + 1) / 12.0) if n > 0 else np.nan
                        st.markdown(f"**Friedman**: œá¬≤ = {chi2:.3f}, p = {p:.4g}, W = {W:.3f} (Kendall's W effect size)")

                        means = pivot.mean().reset_index()
                        means.columns = ["cond","mean"]
                        fig = px.line(means, x="cond", y="mean", markers=True)

                        if plot_subj > 0:
                            samp = pivot.sample(min(plot_subj, pivot.shape[0]), random_state=42)
                            for _, row in samp.iterrows():
                                fig.add_trace(go.Scatter(x=levels, y=row.values, mode="lines", opacity=0.25, showlegend=False))
                        st.plotly_chart(fig, use_container_width=True)

                        # Post-hoc: pairwise Wilcoxon + Holm
                        pvals, labs = [], []
                        for i in range(m):
                            for j in range(i+1, m):
                                wi = stats.wilcoxon(pivot[levels[i]], pivot[levels[j]],
                                                    zero_method="wilcox", correction=False,
                                                    alternative="two-sided", mode="auto")
                                pvals.append(float(wi.pvalue))
                                labs.append(f"{levels[i]} vs {levels[j]}")
                        if pvals:
                            adj = holm_bonferroni(np.array(pvals), np.array(labs))
                            st.dataframe(adj.head(50), use_container_width=True, hide_index=True)
                            st.caption("Pairwise Wilcoxon (Holm-adjusted).")

                        strength = ("y·∫øu" if (np.isnan(W) or W < 0.1) else ("v·ª´a" if W < 0.3 else "m·∫°nh"))
                        best = str(means.sort_values("mean", ascending=False).iloc[0]["cond"])
                        st.success(f"**K·∫øt lu·∫≠n:** Kh√°c bi·ªát **{strength}** (W={W:.2f}). ƒêi·ªÅu ki·ªán cao nh·∫•t: **{best}**.")
                    else:
                        st.warning("C·∫ßn √≠t nh·∫•t 2 ƒëi·ªÅu ki·ªán ƒë·ªÉ so s√°nh.")
# ================= TAB 6 ‚Äî REGRESSION (drilldown like screenshot + size-safe) =================
with TAB6:
    import numpy as np, pandas as pd, plotly.graph_objects as go, streamlit as st
    try:
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler
        from sklearn.linear_model import LinearRegression, LogisticRegression
        from sklearn.metrics import (
            r2_score, mean_squared_error, mean_absolute_error,
            accuracy_score, precision_score, recall_score, f1_score,
            roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve, auc
        )
        _HAS_SK = True
    except Exception:
        _HAS_SK = False

    st.subheader("üìà Regression ‚Äî Linear & Logistic")
    df_full = st.session_state.get("df")
    if not _HAS_SK or df_full is None or df_full.empty:
        st.info("H√£y n·∫°p d·ªØ li·ªáu (v√† c√†i scikit-learn) ƒë·ªÉ ch·∫°y Regression."); st.stop()

    # ---------- hard limits to avoid MessageSizeError ----------
    MAX_SCATTER_POINTS = 20_000
    MAX_CURVE_POINTS   = 4_000
    MAX_COEF_ROWS      = 800
    MAX_TIME_OPTIONS   = {"M":240, "Q":80, "Y":40}  # tr√°nh multiselect qu√° l·ªõn

    # ---------- helpers ----------
    def _rmse(y_true, y_pred):
        try: return mean_squared_error(y_true, y_pred, squared=False)
        except TypeError: return float(np.sqrt(mean_squared_error(y_true, y_pred)))

    def _downsample_xy(x, y, nmax=MAX_SCATTER_POINTS):
        n = len(x); 
        if n<=nmax: return x, y
        idx = np.linspace(0, n-1, nmax, dtype=int)
        return x[idx], y[idx]

    def _downsample_series(x, nmax=MAX_CURVE_POINTS):
        n=len(x); 
        if n<=nmax: return x
        idx=np.linspace(0, n-1, nmax, dtype=int)
        return x[idx]

    def _cap_df(d, n=MAX_COEF_ROWS):
        return d.head(n).copy() if (d is not None and not d.empty) else d

    def _fmt(x, n=4):
        try:
            fx=float(x)
            if abs(fx)>=1e7: return f"{fx:,.{n}f}"
            if abs(fx)>=1000: return f"{fx:,.{max(0,n-2)}f}"
            return f"{fx:.{n}f}"
        except Exception: return str(x)

    def _choose_task():
        try: return st.segmented_control("Task", ["Linear (numeric Y)","Logistic (binary Y)"], default="Linear (numeric Y)")
        except Exception: return st.radio("Task", ["Linear (numeric Y)","Logistic (binary Y)"], horizontal=True)

    def _build_dummies(df_in, cat_cols, ref_levels):
        out=df_in.copy()
        for c in (cat_cols or []):
            if c not in out.columns: continue
            s=out[c].astype(str).fillna("(Missing)")
            ref=ref_levels.get(c) or s.value_counts().idxmax()
            cats=[ref]+[v for v in s.unique() if v!=ref]
            s=pd.Categorical(s, categories=cats, ordered=True)
            out=pd.concat([out.drop(columns=[c]), pd.get_dummies(s, prefix=c, drop_first=True, dtype=float)], axis=1)
        return out

    def _equation_linear(b0, coefs: pd.Series):
        return "y = " + " + ".join([f"{_fmt(b0,6)}"] + [f"{_fmt(b,6)}¬∑{n}" for n,b in coefs.items()])

    def _equation_logit(b0, coefs: pd.Series):
        return "logit(p) = " + " + ".join([f"{_fmt(b0,6)}"] + [f"{_fmt(b,6)}¬∑{n}" for n,b in coefs.items()]) + "   ‚áí   p = 1/(1 + e^(‚àílogit))"

    def _grade(v, bins, labels):
        try: v=float(v)
        except: return labels[-1]
        for (lo,hi),lab in zip(bins,labels):
            if lo<=v<hi: return lab
        return labels[-1]

    def _clean_time(ts):
        t=pd.to_datetime(ts, errors="coerce")
        bad=t.notna() & ((t.dt.year<1900) | (t.dt.year>2100))
        return t.mask(bad)

    def _top_k_values(df, col, k=200):
        if not col or col not in df.columns: return []
        return df[col].astype(str).value_counts(dropna=False).head(k).index.tolist()

    # ---------- DRILL-DOWN PANEL (UI nh∆∞ screenshot) ----------
    def drilldown_panel(df: pd.DataFrame, prefix="rg"):
        st.markdown("### üîé Drill-down filter ‚Äî Khoanh v√πng d·ªØ li·ªáu")
        ckR, ckC, ckP, ckU, ckT = st.columns([1,1,1,1,1])
        useR = ckR.checkbox("Region",  key=f"{prefix}_useR")
        useC = ckC.checkbox("Channel", key=f"{prefix}_useC")
        useP = ckP.checkbox("Product", key=f"{prefix}_useP")
        useU = ckU.checkbox("Customer",key=f"{prefix}_useU")
        useT = ckT.checkbox("Time",    key=f"{prefix}_useT", value=True)

        # Time block (gi·ªëng h√¨nh)
        time_col = None; per_rule="M"; sel_periods=[]
        if useT:
            st.write("")  # spacing
            st.caption("C·ªôt th·ªùi gian")
            time_col = st.selectbox(" ", ["‚Äî"]+list(df.columns), index=0, key=f"{prefix}_timecol", label_visibility="collapsed")
            st.caption("Granularity")
            per_txt = st.radio(" ", ["Month","Quarter","Year"], horizontal=True, key=f"{prefix}_gran", label_visibility="collapsed")
            per_rule = {"Month":"M","Quarter":"Q","Year":"Y"}[per_txt]
            if time_col and time_col!="‚Äî":
                t=_clean_time(df[time_col])
                periods = t.dt.to_period(per_rule).astype(str).dropna()
                uniq = sorted(periods.unique().tolist())
                # gi·ªõi h·∫°n option ƒë·ªÉ payload nh·∫π
                cap = MAX_TIME_OPTIONS[per_rule]
                if len(uniq)>cap: uniq = uniq[-cap:]
                st.caption("Kho·∫£ng th·ªùi gian")
                sel_periods = st.multiselect(" ", uniq, default=uniq[-1:] if uniq else [], key=f"{prefix}_selT", label_visibility="collapsed")
        
        # Others
        region_col=channel_col=prod_col=cust_col=None
        selR=selC=selP=selU=[]
        if useR:
            region_col = st.selectbox("C·ªôt Region", ["‚Äî"]+list(df.columns), index=0, key=f"{prefix}_colR")
            if region_col and region_col!="‚Äî":
                selR = st.multiselect("Region", _top_k_values(df, region_col), key=f"{prefix}_valR")
        if useC:
            channel_col = st.selectbox("C·ªôt Channel", ["‚Äî"]+list(df.columns), index=0, key=f"{prefix}_colC")
            if channel_col and channel_col!="‚Äî":
                selC = st.multiselect("Channel", _top_k_values(df, channel_col), key=f"{prefix}_valC")
        if useP:
            prod_col = st.selectbox("C·ªôt Product", ["‚Äî"]+list(df.columns), index=0, key=f"{prefix}_colP")
            if prod_col and prod_col!="‚Äî":
                selP = st.multiselect("Product", _top_k_values(df, prod_col), key=f"{prefix}_valP")
        if useU:
            cust_col = st.selectbox("C·ªôt Customer", ["‚Äî"]+list(df.columns), index=0, key=f"{prefix}_colU")
            if cust_col and cust_col!="‚Äî":
                selU = st.multiselect("Customer", _top_k_values(df, cust_col), key=f"{prefix}_valU")

        # build mask
        mask = pd.Series(True, index=df.index)
        if useT and time_col and time_col!="‚Äî" and sel_periods:
            t=_clean_time(df[time_col])
            cur=t.dt.to_period(per_rule).astype(str)
            mask &= cur.isin(set(sel_periods))
        if useR and region_col and region_col!="‚Äî" and selR:
            mask &= df[region_col].astype(str).isin(selR)
        if useC and channel_col and channel_col!="‚Äî" and selC:
            mask &= df[channel_col].astype(str).isin(selC)
        if useP and prod_col and prod_col!="‚Äî" and selP:
            mask &= df[prod_col].astype(str).isin(selP)
        if useU and cust_col and cust_col!="‚Äî" and selU:
            mask &= df[cust_col].astype(str).isin(selU)

        return time_col if time_col!="‚Äî" else None, region_col, channel_col, prod_col, cust_col, mask

    # ---------------- use drilldown panel ----------------
    time_col, region_col, channel_col, prod_col, cust_col, dd_mask = drilldown_panel(df_full, "rg")
    df = df_full.loc[dd_mask].copy()
    if df.empty:
        st.warning("Kh√¥ng c√≤n d·ªØ li·ªáu sau khi khoanh v√πng."); st.stop()

    # ---------------- choose task & variables ----------------
    task = _choose_task()
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols_local = df.select_dtypes(include=["object","category","bool"]).columns.tolist()

    top_row = st.columns([1.2, 0.7, 2.3])  # target | dummy | predictors
    if task.startswith("Linear"):
        if not num_cols: st.warning("Kh√¥ng c√≥ c·ªôt numeric cho Y."); st.stop()
        y_col = top_row[0].selectbox("üéØ Target (numeric Y)", num_cols, key="rg_y_lin")
    else:
        cand = [c for c in df.columns if df[c].dropna().nunique()<=20]
        y_col = top_row[0].selectbox("üéØ Target (binary Y)", cand or list(df.columns), key="rg_y_log")

    dummy_cols, ref_levels = [], {}
    try:
        with top_row[1].popover("Dummy"):
            dummy_cols = st.multiselect("One-hot columns", [c for c in cat_cols_local if c != y_col], key="rg_dummy_cols")
            for c in dummy_cols:
                lv = df[c].astype(str).fillna("(Missing)").value_counts().index.tolist()
                ref_levels[c] = st.selectbox(f"Ref `{c}`", lv, index=0, key=f"rg_ref_{c}")
    except Exception:
        with top_row[1].expander("Dummy", expanded=False):
            dummy_cols = st.multiselect("One-hot columns", [c for c in cat_cols_local if c != y_col], key="rg_dummy_cols")
            for c in dummy_cols:
                lv = df[c].astype(str).fillna("(Missing)").value_counts().index.tolist()
                ref_levels[c] = st.selectbox(f"Ref `{c}`", lv, index=0, key=f"rg_ref_{c}")

    X_cols = top_row[2].multiselect(
        "üß© Predictors", [c for c in df.columns if c != y_col],
        default=[c for c in num_cols if c != y_col][:5], key="rg_X_cols"
    )
    if not y_col or not X_cols:
        st.info("H√£y ch·ªçn **Target** v√† ‚â•1 **Predictor**."); st.stop()

    # ---------------- prepare data & auto settings ----------------
    base = df[[y_col]+X_cols].copy()
    for c in X_cols:
        if c in num_cols: base[c]=pd.to_numeric(base[c], errors="coerce")
        else: base[c]=base[c].astype(str)

    if task.startswith("Linear"):
        y_series = pd.to_numeric(base[y_col], errors="coerce"); y_is_binary=False
    else:
        y_tmp = df[y_col].astype(str).str.strip()
        classes = sorted(y_tmp.dropna().unique().tolist())
        pos_auto = (y_tmp.value_counts(normalize=True).idxmin() if len(classes)==2 else classes[0]) if classes else None
        pos_label = st.selectbox("Positive class (label=1)", classes or ["‚Äî"], index= (classes.index(pos_auto) if classes and pos_auto in classes else 0), key="rg_pos")
        y_series = (y_tmp == str(pos_label)).astype(int); y_is_binary=True

    X = base[X_cols].copy()
    for c in X.columns:
        if c in num_cols: X[c]=pd.to_numeric(X[c], errors="coerce").fillna(X[c].median())
        else: X[c]=X[c].astype(str).fillna("(Missing)")
    X = _build_dummies(X, dummy_cols, ref_levels)

    n_samples=len(X)
    if n_samples<400:   test_size_auto=0.35
    elif n_samples<4000:test_size_auto=0.25
    else:               test_size_auto=0.20
    if y_is_binary:
        prev=float(np.mean(y_series)) if len(y_series) else np.nan
        if (prev==prev and prev<0.10) or (int(y_series.sum())<50):
            test_size_auto=max(test_size_auto,0.30)
    random_state_auto=int((len(df_full)+len(df)+sum(len(str(c)) for c in X_cols)+len(str(y_col)))%10000)
    # ---- D√ÅN TO√ÄN B·ªò KH·ªêI CODE M·ªöI N√ÄY V√ÄO ----

    XY = pd.concat([y_series.rename(y_col), X], axis=1).dropna()
    if XY.empty: st.warning("D·ªØ li·ªáu r·ªóng sau l√†m s·∫°ch."); st.stop()

    y = XY[y_col].values
    X_df = XY.drop(columns=[y_col]) # <-- Gi·ªØ ·ªü d·∫°ng DataFrame
    feat_names = X_df.columns.tolist()
    nums_now = X_df.select_dtypes(include=[np.number]).columns.tolist()

    # ... (Code tr∆∞·ªõc ƒë√≥ c·ªßa b·∫°n trong Tab 6) ...

    # 1. Chu·∫©n b·ªã X v√† y (d√≤ng 2088-2101)
    XY = pd.concat([y_series.rename(y_col), X], axis=1).dropna()
    if XY.empty: st.warning("D·ªØ li·ªáu r·ªóng sau l√†m s·∫°ch."); st.stop()

    y = XY[y_col].values
    X_df = XY.drop(columns=[y_col]) # Gi·ªØ X d·∫°ng DataFrame ƒë·ªÉ ti·ªán truy c·∫≠p t√™n c·ªôt
    feat_names = X_df.columns.tolist()
    nums_now = X_df.select_dtypes(include=[np.number]).columns.tolist()

    # 2. Chia Train/Test TR∆Ø·ªöC (d√πng DataFrame X_df)
    # Thay ƒë·ªïi: Chia d·ªØ li·ªáu tr∆∞·ªõc khi chu·∫©n h√≥a
    Xtr_df, Xte_df, ytr, yte = train_test_split(
        X_df, y, 
        test_size=test_size_auto, 
        random_state=random_state_auto, 
        stratify=(y if y_is_binary else None)
    )

    # 3. Chu·∫©n h√≥a d·ªØ li·ªáu (Scaling) ƒê√öNG C√ÅCH
    # Thay ƒë·ªïi: Fit scaler ch·ªâ tr√™n t·∫≠p train, sau ƒë√≥ transform c·∫£ train v√† test
    scaler = StandardScaler()
    
    # T·∫°o b·∫£n copy ƒë·ªÉ tr√°nh c·∫£nh b√°o SettingWithCopyWarning
    Xtr = Xtr_df.copy()
    Xte = Xte_df.copy()

    if nums_now: # Ch·ªâ ch·∫°y n·∫øu c√≥ c·ªôt numeric
        # Fit V√Ä transform t·∫≠p train
        Xtr[nums_now] = scaler.fit_transform(Xtr_df[nums_now])
        # Ch·ªâ transform t·∫≠p test (d√πng mean/std c·ªßa t·∫≠p train)
        Xte[nums_now] = scaler.transform(Xte_df[nums_now])

    # 4. Chuy·ªÉn ƒë·ªïi sang values ƒë·ªÉ fit m√¥ h√¨nh (nh∆∞ code c≈© c·ªßa b·∫°n)
    Xtr_values = Xtr.values
    Xte_values = Xte.values

    st.caption(f"**Auto** ‚ûú test_size={test_size_auto:.2f} ¬∑ random_state={random_state_auto} ¬∑ Scaling=StandardScaler ¬∑ ClassWeight(Logistic)='balanced'")

    # ---------------- train & evaluate ----------------
    if task.startswith("Linear"):
        st.markdown("### üìå Linear Regression")
        # S·ª≠a: D√πng Xtr_values, Xte_values
        model = LinearRegression().fit(Xtr_values, ytr)
        ypred = model.predict(Xte_values)

        R2=r2_score(yte, ypred); RMSE=_rmse(yte, ypred); MAE=mean_absolute_error(yte, ypred)
        msk=np.where(yte==0, False, True)
        MAPE=float(np.mean(np.abs((yte[msk]-ypred[msk])/yte[msk]))*100) if msk.any() else np.nan
        y_std=float(np.std(yte, ddof=1)) if len(yte)>1 else np.nan
        rel_rmse=(RMSE/y_std*100) if (y_std and y_std==y_std and y_std!=0) else np.nan
        pearson=np.corrcoef(yte, ypred)[0,1] if len(yte)>1 else np.nan
        resid=yte-ypred; bias=float(np.mean(ypred-yte))
        corr_rp=np.corrcoef(ypred, resid)[0,1] if len(yte)>1 else np.nan
        within10=float(np.mean(np.abs(ypred-yte)<=0.10*np.maximum(np.abs(yte),1e-12))*100)

        c1,c2,c3,c4 = st.columns(4)
        c1.metric("R¬≤", _fmt(R2,4)); c2.metric("RMSE", _fmt(RMSE))
        c3.metric("MAE", _fmt(MAE)); c4.metric("MAPE (%)", _fmt(MAPE,2))

        # Nh·∫≠n ƒë·ªãnh ƒë·∫∑t ngay d∆∞·ªõi KPI (gi·ªØ nguy√™n n·ªôi dung chi ti·∫øt)
        r2_grade=_grade(R2,[(0,0.3),(0.3,0.6),(0.6,0.9),(0.9,1.01)],["y·∫øu","trung b√¨nh","kh√°/t·ªët","r·∫•t cao (c·∫ßn c·∫£nh gi√°c overfit)"])
        resid_msg="kh√¥ng th·∫•y pattern m·∫°nh" if (np.isnan(corr_rp) or abs(corr_rp)<0.15) else "c√≥ d·∫•u hi·ªáu pattern/h·ªá s·ªë ph∆∞∆°ng sai kh√¥ng ƒë·ªìng nh·∫•t"
        st.markdown("\n".join([
            f"- **R¬≤ = {_fmt(R2,4)}** ‚Üí m·ª©c gi·∫£i th√≠ch **{r2_grade}**.",
            f"- **RMSE = {_fmt(RMSE)}** (‚âà **{_fmt(rel_rmse,1)}%** œÉ(Y)); **MAE = {_fmt(MAE)}**; **MAPE = {_fmt(MAPE,2)}%**.",
            f"- **T∆∞∆°ng quan Pred‚ÄìActual = {_fmt(pearson,3)}**; **Bias = {_fmt(bias)}**; **¬±10% ƒë√∫ng ‚âà {_fmt(within10,1)}%**.",
            f"- **Residuals vs Fitted**: |corr| ‚âà {_fmt(corr_rp,3)} ‚Üí {resid_msg}."
        ]))

        # Ph∆∞∆°ng tr√¨nh + h·ªá s·ªë (gi·ªõi h·∫°n top |Œ≤|)
        coefs=pd.Series(model.coef_, index=feat_names); b0=float(model.intercept_)
        st.markdown("#### üìê Ph∆∞∆°ng tr√¨nh (Linear)")
        st.code(_equation_linear(b0, coefs), language="text")

        with st.expander("Gi·∫£i th√≠ch ph∆∞∆°ng tr√¨nh (theo d·ªØ li·ªáu hi·ªán t·∫°i)"):
            top3=coefs.reindex(coefs.abs().sort_values(ascending=False).head(3).index)
            st.write(f"- **Intercept Œ≤‚ÇÄ = {_fmt(b0,6)}**: Y khi numeric ·ªü m·ª©c trung b√¨nh & ph√¢n lo·∫°i ·ªü m·ª©c tham chi·∫øu.")
            if not top3.empty:
                st.write("**3 bi·∫øn t√°c ƒë·ªông m·∫°nh nh·∫•t:**")
                for name,b in top3.items():
                    msg = f"tƒÉng 1œÉ l√†m Y ƒë·ªïi ‚âà {_fmt(b,6)}" if name in XY.columns else f"b·∫≠t bi·∫øn so v·ªõi ref l√†m Y ƒë·ªïi ‚âà {_fmt(b,6)}"
                    st.write(f"  ‚Ä¢ `{name}`: Œ≤={_fmt(b,6)} ‚Üí {msg}.")

        coef_tbl = pd.DataFrame({"Feature":feat_names,"Œ≤ (coef)":coefs.values}).sort_values("Œ≤ (coef)", key=np.abs, ascending=False)
        coef_show=_cap_df(coef_tbl)
        st.dataframe(coef_show, use_container_width=True, hide_index=True, height=min(360,48*(len(coef_show)+1)))

        # Charts (size-safe)
        st.markdown("#### üìä Bi·ªÉu ƒë·ªì h·ªó tr·ª£")
        g1,g2=st.columns(2)
        with g1:
            N=len(yte)
            if N>MAX_SCATTER_POINTS*3:
                fig=go.Figure()
                fig.add_trace(go.Histogram2d(x=yte, y=ypred, nbinsx=80, nbinsy=80, colorscale="Blues", showscale=True))
                lim=[float(min(yte.min(),ypred.min())), float(max(yte.max(),ypred.max()))]
                fig.add_scatter(x=lim, y=lim, mode="lines", name="y=x", line=dict(color="#e67e22"))
                title="Predicted vs Actual ‚Äî 2D Density"
            else:
                xa,ya=_downsample_xy(yte, ypred)
                fig=go.Figure()
                fig.add_scatter(x=xa, y=ya, mode="markers", name="Pred vs Actual")
                lim=[float(min(yte.min(),ypred.min())), float(max(yte.max(),ypred.max()))]
                fig.add_scatter(x=lim, y=lim, mode="lines", name="y=x", line=dict(color="#e67e22"))
                title="Predicted vs Actual"
            fig.update_layout(title=title, xaxis_title="Actual", yaxis_title="Predicted", height=420, margin=dict(l=10,r=10,t=50,b=10))
            st.plotly_chart(fig, use_container_width=True, config={"displayModeBar": False})
            st.caption(f"**Gi·∫£i th√≠ch:** r={_fmt(pearson,3)}; bias={_fmt(bias)}; {_fmt(within10,1)}% ƒëi·ªÉm n·∫±m trong ¬±10% so v·ªõi th·ª±c t·∫ø.")

        with g2:
            if len(yte)>MAX_SCATTER_POINTS*3:
                fig2=go.Figure(); fig2.add_trace(go.Histogram2d(x=ypred, y=resid, nbinsx=80, nbinsy=80, colorscale="Blues", showscale=True))
                fig2.add_hline(y=0, line_dash="dot"); title2="Residuals vs Fitted ‚Äî 2D Density"
            else:
                xp,rp=_downsample_xy(ypred, resid)
                fig2=go.Figure(); fig2.add_scatter(x=xp, y=rp, mode="markers", name="Residuals")
                fig2.add_hline(y=0, line_dash="dot"); title2="Residuals vs Fitted"
            fig2.update_layout(title=title2, xaxis_title="Predicted", yaxis_title="Residual", height=420, margin=dict(l=10,r=10,t=50,b=10))
            st.plotly_chart(fig2, use_container_width=True, config={"displayModeBar": False})

    else:
        st.markdown("### üìå Logistic Regression")
        model=LogisticRegression(max_iter=1000, class_weight='balanced', solver="liblinear").fit(Xtr_values,ytr)
        p_pred=model.predict_proba(Xte_values)[:,1]
        
        fpr,tpr,thr_roc=roc_curve(yte, p_pred); youden=tpr-fpr
        thr_youden=float(thr_roc[np.argmax(youden)]) if len(thr_roc)>0 else 0.5
        f1_vals=[(t, f1_score(yte,(p_pred>=t).astype(int), zero_division=0)) for t in np.linspace(0.1,0.9,33)]
        thr_f1=max(f1_vals, key=lambda z:z[1])[0] if f1_vals else 0.5
        thr=st.slider("Ng∆∞·ª°ng ph√¢n lo·∫°i (threshold)", 0.10,0.90, float(np.round(thr_f1,2)), 0.05, key="rg_thr")
        yhat=(p_pred>=thr).astype(int)

        prevalence=float(np.mean(yte)) if len(yte)>0 else np.nan
        baseline=max(prevalence,1-prevalence) if prevalence==prevalence else np.nan
        acc=accuracy_score(yte,yhat); prec=precision_score(yte,yhat, zero_division=0)
        rec=recall_score(yte,yhat, zero_division=0); f1v=f1_score(yte,yhat, zero_division=0)
        auc_roc=roc_auc_score(yte,p_pred); pr_prec, pr_recall,_=precision_recall_curve(yte,p_pred); auc_pr=auc(pr_recall, pr_prec)

        c1,c2,c3,c4,c5=st.columns(5)
        c1.metric("Accuracy", _fmt(acc,4)); c2.metric("Precision", _fmt(prec,4))
        c3.metric("Recall", _fmt(rec,4));   c4.metric("F1", _fmt(f1v,4)); c5.metric("ROC-AUC", _fmt(auc_roc,4))

        roc_grade=_grade(auc_roc,[(0.5,0.6),(0.6,0.7),(0.7,0.8),(0.8,0.9)],["y·∫øu","trung b√¨nh","kh√°","t·ªët"])
        impr=(acc-baseline)*100 if baseline==baseline else np.nan
        st.markdown("\n".join([
            f"- **Prevalence l·ªõp 1** ‚âà {_fmt(prevalence*100,2)}%; **Baseline acc** ‚âà {_fmt(baseline*100,2)}%"+
            ("" if np.isnan(impr) else f" ‚Üí c·∫£i thi·ªán ‚âà {_fmt(impr,2)} ƒëi·ªÉm %."),
            f"- Threshold = {np.round(thr,2)} ‚Üí Precision={_fmt(prec,3)}, Recall={_fmt(rec,3)}, F1={_fmt(f1v,3)}.",
            f"- **ROC-AUC = {_fmt(auc_roc,3)}** ‚Üí nƒÉng l·ª±c ph√¢n bi·ªát **{roc_grade}**; **PR-AUC = {_fmt(auc_pr,3)}** so v·ªõi prevalence {_fmt(prevalence,3)}.",
            f"- G·ª£i √Ω threshold: F1-opt={np.round(thr_f1,2)}; Youden={np.round(thr_youden,2)}."
        ]))

        coefs=pd.Series(model.coef_[0], index=feat_names); b0=float(model.intercept_[0])
        st.markdown("#### üìê Ph∆∞∆°ng tr√¨nh (Logistic)")
        st.code(_equation_logit(b0, coefs), language="text")
        with st.expander("Gi·∫£i th√≠ch ph∆∞∆°ng tr√¨nh (theo d·ªØ li·ªáu hi·ªán t·∫°i)"):
            p0=1/(1+np.exp(-b0))
            st.write(f"- **Intercept Œ≤‚ÇÄ = {_fmt(b0,6)}** ‚Üí x√°c su·∫•t n·ªÅn p‚ÇÄ ‚âà {_fmt(p0,3)} (numeric ·ªü m·ª©c trung b√¨nh, ph√¢n lo·∫°i ·ªü ref).")
            top3=coefs.reindex(coefs.abs().sort_values(ascending=False).head(3).index)
            for name,b in top3.items():
                st.write(f"  ‚Ä¢ `{name}`: Œ≤={_fmt(b,6)} ‚Üí Odds Ratio‚âà{_fmt(np.exp(b),3)}.")
        coef_tbl=pd.DataFrame({"Feature":feat_names,"Œ≤ (log-odds)":coefs.values,"Odds Ratio":np.exp(coefs.values)}).sort_values("Odds Ratio", ascending=False, key=np.abs)
        coef_show=_cap_df(coef_tbl)
        st.dataframe(coef_show, use_container_width=True, hide_index=True, height=min(380,48*(len(coef_show)+1)))

        # charts (size-safe)
        st.markdown("#### üìä Bi·ªÉu ƒë·ªì h·ªó tr·ª£")
        h1,h2=st.columns(2)
        with h1:
            fig=go.Figure()
            fig.add_scatter(x=_downsample_series(fpr), y=_downsample_series(tpr), mode="lines", name="ROC")
            fig.add_scatter(x=[0,1], y=[0,1], mode="lines", name="Chance", line=dict(color="#e67e22", dash="dot"))
            fig.update_layout(title="ROC Curve (Test set)", xaxis_title="FPR (1‚àíSpecificity)", yaxis_title="TPR (Recall)", height=420, margin=dict(l=10,r=10,t=50,b=10))
            st.plotly_chart(fig, use_container_width=True, config={"displayModeBar": False})
            st.caption(f"**Gi·∫£i th√≠ch:** AUC={_fmt(auc_roc,4)}; threshold Youden‚âà{_fmt(thr_youden,2)} gi·ªØ TPR cao v√† FPR th·∫•p.")

        with h2:
            cm=confusion_matrix(yte,yhat,labels=[0,1])
            fig2=go.Figure(data=go.Heatmap(z=cm, x=["Pred 0","Pred 1"], y=["Actual 0","Actual 1"], colorscale="Blues", showscale=False, text=cm, texttemplate="%{text}"))
            fig2.update_layout(title=f"Confusion Matrix (Threshold={np.round(thr,2)})", height=420, margin=dict(l=10,r=10,t=50,b=10))
            st.plotly_chart(fig2, use_container_width=True, config={"displayModeBar": False})
            tn,fp,fn,tp=cm.ravel()
            tpr_now=tp/(tp+fn) if (tp+fn)>0 else np.nan
            fpr_now=fp/(fp+tn) if (fp+tn)>0 else np.nan
            st.caption(f"**Gi·∫£i th√≠ch:** TPR={_fmt(tpr_now,3)}, FPR={_fmt(fpr_now,3)} ¬∑ Precision={_fmt(prec,3)} ¬∑ Recall={_fmt(rec,3)} ¬∑ F1={_fmt(f1v,3)}.")

        with st.expander("Precision‚ÄìRecall Curve", expanded=False):
            fig3=go.Figure(); fig3.add_scatter(x=_downsample_series(pr_recall), y=_downsample_series(pr_prec), mode="lines", name="PR")
            fig3.update_layout(title="Precision‚ÄìRecall Curve (Test set)", xaxis_title="Recall", yaxis_title="Precision", height=360, margin=dict(l=10,r=10,t=50,b=10))
            st.plotly_chart(fig3, use_container_width=True, config={"displayModeBar": False})
